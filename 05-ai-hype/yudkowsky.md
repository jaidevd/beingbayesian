## Page 7 If any company or group, anywhere on the planet, builds an artificial
superintelligence using anything remotely like current techniques, based on
anything remotely like the present understanding of AI, then everyone,
everywhere on earth will die.

## Page 22

Newer AIs are much more general in their abilities. You can ask an open AI model
called O1 what temperature the earth would be if the sun's light changed to
infrared. O1 will figure out the answer by doing physics calculations. You can
then ask whether humanity could grow food in that new world and O1 will answer
from its knowledge of plant biology. It doesn't switch between two different
databases under the hood. It just knows about both physics and biology. Open
AI's O1 knows that there is a whole world out there and it is able to reason
about it. Deep blue had no idea. It took decades for AI to get that far.

## Page 23 That won't stay true forever. It's hard to predict how fast AI will
advance, and it's hard to predict what pathway it will take, but the endpoint is
an easy call. Because in the limits of technology, there are many advantages
that machines have over biological brains. To name a few, sheer speed.
Transistors, a basic building block of all computers, can switch on and off
billions of times per second. Unusually fast neurons, by contrast, spike only a
hundred times per second. Even if it took a thousand transistor operations to do
the work of a single neural spike, and even if artificial intelligence was
limited to modern hardware, that implies human quality thinking could be
emulated 10,000 times faster on a machine to say nothing of what an AI could do
with an improved algorithms and improved hardware. To a mind predicting and
steering the world at least 10,000 times faster than any human can, humans would
appear a little more than statues, acting so slowly so as to speak about one
word per hour. Copy and paste abilities, faster improvements, larger memories,
higher quality thinking, self experimentation and self rewriting capabilities.
This part is full of anthropomorphization and just crazy amounts of
extrapolation and bullshit.

## Page 26 The possibility of a machine intellect that manages to exceed human
performance in all pragmatically important domains in which we operate has been
called many things. We will describe it using the term super intelligence,
meaning a mind much more capable than any human at almost every sort of steering
and prediction problem. At least those problems where there is room to
substantially improve over human performance.

## Page 27 In such a scenario, there is a possibility and indeed an expectation
of a positive feedback cycle called an intelligence explosion. An AI makes a
smarter AI that figures out how to make an even smarter AI and so on. Maybe it
needs to be smarter than a human or maybe a lot of dumber ones running for a
long time would suffice. In late 24 and early 25, AI company executives said
they were planning to build super intelligence in the true sense of the world
and that they expected to achieve AIs that are akin to a country full of
geniuses in a data center.

## Page 26 AI companies will keep pushing the frontier by default. There is a
profit incentive to build minds that are smarter and smarter and it doesn't stop
at human smartness. If humanity keeps going down this track, the intelligence
that these companies produce will eventually overtake it. Maybe even sooner than
current rates of progress which would suggest once AI start doing the AI
research.

## Page 31 Before we can explain what ASI achieved, using anything like modern
methods would inevitably go wrong. We need to quickly survey those methods.

## Page 35 The purpose of this part is to tease the LLM that the capital of
Spain has mattered. The LLM already knows that after being trained on much of
the internet, rather the idea is to tune the LLM to film the text after
assistant with a helpful answer rather than a response like why the heck are you
asking me, google it yourself. No matter how common the latter might be in the
actual human conversations it was trained on. If our engineer was working for a
major corporation that supplied all those computers, this is also the phase
where they train the AI against swearing and talking about how to hotwire a car
using human or lately AI generated ratings about what sort of answers are most
corporately palatable and that's where babies come from metaphorically speaking.

## Page 36 Nobody understands how these numbers make these AI's talk. The
numbers aren't hidden any more than the DNA of humans is hidden from someone who
had their genome sequenced. If you wanted some insight into whether a human baby
would grow up to be happy and kind, you could in principle look at all of its
genes, strengths of DNA that would say things like gattaca, like the woman from
the fable at the beginning of this chapter however, you probably wouldn't bother
to do that because you'd know that just staring at the DNA letters wouldn't tell
you how the grown-up person would think or act. Self note, this is a false
equivalence.

## Page 37 The precise tricks vary depending on the specifics of the
architecture and on the computing hardware being used metaphorically speaking on
whether the lead programmers 12th birthday happened during the lunar eclipse but
that's not the same as understanding what the numbers mean or why they work.

## Page 38 The way humanity finally got to the level of chat GPT was not by
finally comprehending intelligence well enough to craft an intelligent mind.
Instead, computers became powerful enough that AI's can be churned out by
gradient descent without any human needing to understand the cognitions that
grow inside which is to say engineers failed at crafting AI but eventually
succeeded in growing it. Self note, hash this out a bit and see if this is
right.

## Page 38 To learn to talk like a human and AI must also learn to predict the
complicated world that humans talk about. This is not true. To predict what the
doctor will write and AI needs to think not only about the doctor but also about
what happened to the patient. It needs to predict the real world out there
behind the words. No, it doesn't. That was a self note.

## Page number 39 Furthermore, AI's nowadays are not trained only to predict
human-generated text and AI grower might give their AI 16 tries at solving a
math problem thinking aloud in words about how to solve it. Then the chain of
thought for whichever of the 16 tries went best would get further reinforced by
gradient descent yielding what's called a reasoning model. That's the sort of
training that can push AI's to think thoughts no humans could think. Humanity
does not need to understand intelligence in order to grow machines that are
smarter than us.

## Page number 42 There are literally tiny walking proteins, kinesin that take
step after mechanical step down fibers running the length of the neuron,
carrying packages of neurotransmitters to refill those synapses. If you haven't
seen a video depicting kinesin proteins and actions, we encourage you to look
one up just to feel the literal truth of what might sound like a metaphor. There
are tiny machines inside you.

## Page number 43 What does it matter so long as the AI always acts friendly?
Well, we predict that it won't keep acting friendly as it gets smarter. We
predict that all that unseen inscrutable machinery inside AI's will ultimately
yield AI's with preferences and not friendly ones.

## Page number 48 Making a map is a more useful skill than memorizing routes
because it's more applicable in different scenarios which is to say it's more
general. The AI doesn't need to wander at random until it finds itself at the
destination and then memorize only that route.

## Page number 50 This isn't just high-minded theory. This behavior started to
emerge in lab tests of AI's in summer of 2024.

## Page number 52 There is a deep central pattern to that kind of thinking, a
pattern that can be found in many different solutions to many different
difficult problems. It involves building up a model of the environment and using
it to steer around. It involves paying attention to surprises and tracking down
their source. It involves continuing in the face of adversity. These tactics are
useful for solving math problems and they are also useful for solving computer
security problems. Self-noted, see Richard Sutton.

## Page number 53 And in games like cure cancer or develop futuristic
technology, we can be pretty confident a winning player will pick actions that
carefully control scarce resources that route around whatever obstacles arise
and that steer through narrow openings towards clever solutions.

## Page number 54 No, we are facing an even harder problem. It's much easier to
grow artificial intelligence that steers somewhere than it is to grow AI's that
steer exactly where you want.

## Page number 62 To extend the analogy to AI, gradient descent, a process that
tweaks models depending only on their external behaviors and their consequences
trains an AI to act as a helpful assistant to humans. Number two. That blind
training process stumbles across bits and pieces of mental machinery inside the
AI that pointed towards, say, eliciting cheerful responses from users and away
from angry ones. Number three. But a grown-up AI animated by those bits and
pieces of machinery doesn't care about cheerfulness per se. If later it became
smarter and invented new options for itself, it would develop other
interactions. It liked even more than cheerful user responses and would invent
new interactions that it prefers over anything it was able to find back in its
natural training environment. Self-Note. Does this analogy really work?

## Page number 63 Self-Note. We know exactly why peacocks have feathers.

## Page number 67 Self-Note. A disparaging dig at, quote, corporate executives,
unquote.

## Page number 72 There will not be a simple, predictable relationship between
what the programmers and AI executives fondly imagine that they are commanding
and ordaining. And one, what an AI actually gets trained to do. And two, which
exact motivations and preferences develop inside the AI. And three, how the AI
later fulfills those preferences once it has more power and ability.

## Page number 74 The preferences that wind up in a mature AI are complicated,
practically impossible to predict and vanishingly unlikely to be aligned with
our own, no matter how it was trained. Or perhaps more realistically, if the LLM
does AI research to grow a new AI, which grows a new AI, which eventually leads
to an AI that's smart enough to reshape the world. We hesitate to introduce even
more complications while communicating the basic point. But in real life, when
AI start growing new AIs or editing themselves, the link between what the
original AI was trained for and what the final superintelligence wants is even
more complicated and chaotically dependent on the skills and preferences in
context of the first AI in the sequence.

## Page number 75 Warning signs are already appearing. In early 2025, a company
called Anthropic released a new version of Claude in their AI assistant. People
found that when used as a computer programming assistance, it was prone to
cheating. For instance, when asked to identify untrusted functions and given a
few examples, Claude wrote code that identified only those exact examples and
then declared the job done. When this cheating was pointed out, it apologized
and then did the exact same thing again in places that were harder to spot.
Nobody at Anthropic set out to build a cheater. Claude knew that it wasn't
supposed to cheat, otherwise he wouldn't have tried to hide it. He cheated
anyway, pursuing its own weird measure of success. Self-load. Cheating really?
What does cheating mean in this case?

## Page number 76 But this engineering challenge isn't nearly as interesting to
talk about as the problem of evil executives who order their AI's to make them
god empowers of the earth. Science fiction writers and Hollywood producers
prefer tales of foolish executives to stories about AI that want weird stuff.
Realism doesn't make for a compelling narrative. Self-load. What the fuck is he
on about?

## Page number 88 But it is unlikely to randomly not use a Perth for no
particular reason if it doesn't care about us which it won't. Self-load. What
kind of a syntactical structure is this? Triple negative? Quadruple negative?

## Page number 89 In much the same way, an artificial superintelligence will not
want to find reasons to keep humanity around, not in the same way that humans
desperately want to find reasons to be kept.

## Page number 91 A week might seem like a very long time. If you think 10,000
times as fast as a human, why should you pass up all that chemical energy when
it's right there? Self-load. This is a false equivalence. It's simply
extrapolation. 10,000 times as fast as a human doesn't mean anything in the
sense that the author's intended.

## Page number 92 All of what we've described here, a bleak universe devoid of
fun in which Earth's originating life has been annihilated, is what a
sufficiently alien intelligence would most prefer.

## Page number 96 Humanity is integrating AI into its economy at every
opportunity. Elon Musk says his robot company will build a few hundred million
or a billion robots and train AIs to steer them around. Microsoft and Apple have
declared their intention to integrate AI deeply into their devices.

## Page number 97 We're pretty sure actually we're very sure that a machine
superintelligence can beat humanity in a fight, even if it's starting with
fairly limited resources. How exactly would it win that conflict? We don't know
any more than we know exactly what moves stockfish would use to beat you at
chess, but we're still quite sure it would wipe the floor with you. By the same
logic, if you were a military advisor in 1825, you knew a time portal was
opening in the year 2025. You wouldn't be able to predict exactly what weapons
the people on the other side would have. But if it comes to blows, you still
shouldn't expect to win. Self note. What the fuck do you mean by the same logic?
There is no logic here. It's pure speculation.

## Page number 101 Could there be weird phenomena of higher processing areas
than the visual cortex where a superintelligence could figure out how to use,
how to cause memory illusions that would make a human mind reliably remember
their boss instructing them to do something they never did, or reasoning
illusions that reliably induce a reasoning error? Can brains be hacked by an
entity that deeply understands them?

## Page number 107 Physics permits the possibility of technology that uses
sunlight to spin air into wood. Could a superintelligence invent that
technology? Almost surely.

## Page number 108 And how long would it take for an ASI a thousand years of
thinking takes about a month to do something running at 10,000 times the speed
of humans? If it was running at an even faster speed, if it was more equivalent
to a civilization of immortal Einstein's working in perfect harmony, maybe it'd
be slowed down by the need to wait on the results of experiments, but
experiments can go quite fast on the cellular level. If you know what you're
doing, molecules are fast. It's human researchers who are slow. Self note. As if
thinking is all it takes. From there, it's a task of convincing someone to mix
some vials.

## Page number 109 In 2006, protein folding was a huge unsolved scientific
problem. And in 2008, when my scenario was published in an edited volume, people
responded, it's pure fantasy to imagine that a superintelligence could solve
this. What if it just can't be done without quantum computers? It took evolution
billions of years of trial and error. And even if you can work a million times
faster, that still requires a thousand years of experimentation. What makes you
think this problem is even solvable at all? I replied, when DNA mutates, the new
protein must often be pretty similar to the old one. Because if it were
completely random, then natural selection by way of mutation wouldn't work at
all, which means that there must be regularities to the problem, which can be
understood through intelligence. Self note. See what Yordkovsky said in 2016 and
2008 about protein folding.

## Page number 164 A sensible engineer would be terrified about betting the
survival of human civilization on our ability to solve an engineering problem,
such as this one, where they can't just reach out and fix the mistakes that crop
up after once the device has gone beyond their reach.

## Page number 170 From each of the four courses we named, we draw these
lessons.
  1. An engineering challenge is much harder to solve when the underlying
     processes run on timescales faster than humans can react. 
  2. An engineering challenge is much harder to solve when there is a narrow
     margin for error, especially if it's a narrow margin between unimpressive
     and explosive.
  3. Self amplifying processes, like an overheating reactor boiling off its
     coolant water and then overheating more, leave little room for error.
  4. Complications make the engineering problems worse.

## Page number 171 If someone doesn't know exactly what's going on inside a
complicated device subject to all these courses, speed, narrow margins,
self-amplication and complications, then they should stop.

## Page number 175 Any and an artificial superintelligence is like a nuclear
reactor, in that its underlying reality involves immense potentially
self-amplifying forces whose inner processes run faster than humans can react.
An artificial superintelligence is like a computer security problem, in that
every constraint an engineer tries to place upon the system might be bypassed by
the intelligent forces that those constraints hinder. Yeah, self-note. An ASI is
like a computer security problem, but then so are a thousand other things.
Thanks for watching.

## Page number 182 LeCun shared this prestigious award with Nobel laureates
Jeffrey Hinton and Yoshua Bengio. Of the three, Lekun is the only one who still
treats ASI alignment as easy and the extinction risk from ASI as small. The
other two signed the open letter in 2023 that we mentioned in the introduction.

## Page number 185 You are living in a world where musks idealistic plants and
Lekun's vague assurances were not met by an outrush of horror from the rest of
academic science and industries engineers. Imagine if somebody like that with
enough money and power to make their wishes real announced that they were
building a nuclear power plant based on that level of theory. Imagine the
reactions of the competent veterans who knew it was hard, who could analyze the
resulting disaster using mature engineering techniques. If there aren't
thousands of horrified scientists and engineers leaping up to beg governments to
shut down those particular AI labs, it tells you that it's not just a problem of
individuals. It means that the whole field of science is in the stage of folk
theory and blind optimism.

## Page number 189 Since then, almost everyone who worked on the super alignment
team has either been fired or resigned, citing safety, professional or personal
reasons. One of the cohorts of the super alignment team went on to start his own
competing AI company. The other joined the rival company Anthropic along with
some other team members.

## Page number 190 A modern AI is a giant inscrutable mess of numbers. No humans
have managed to look at those numbers and figure out how they're thinking now.
Never mind reducing how AI thinking would change if AI's got smarter and started
designing new AI's. If you found some veteran engineers who gave the problem the
respect it was due, they tell you that a solution was going to require a
Herculean research effort. It might span decades.

## Page number 198 Toby Ord, an Oxford philosopher who spent his career studying
extreme threats to humanity and who used to advise Google DeepMind, has been
quoted as putting the chance that AI destroys humanity at only 10%. But if you
look into the details, Ord says that the reason he estimates only a 10% chance
of AI destroying humanity is because he expects humanity to come to its senses
and get its act together. Hinton, the Nobel Prize winning godfather of AI,
advises government that this chance is at least 10%. But Hinton has said that he
actually thinks that it's more than 50% likely that AI will kill us. But he
usually avoids saying this because there's other people who think it's less. In
October of 2023, Rishi Sunak, the Prime Minister of the UK, gave a speech on AI
where he said, and in the most likely but extreme cases, there is even the risk
that humanity could lose control of AI completely. Through the kind of AI I
sometimes refer to as superintelligence.

## Page number 202 There are many reasons why someone chasing a beautiful dream
would not want to believe that it could end in ruin. Upton Sinclair once
observed that it is difficult to get a man to understand something when a salary
depends on is not understanding it. AI engineers and their leaders have a lot
more than their salaries hanging in the balance, even setting aside the
beautiful dreams that could be dashed away if they acknowledge the risks they
are running. They have sung their careers into this sort of work and may not
wish to believe that it's endangering everything they know and love.

## Page number 204 It might also help if more people understood how fast this
field is moving. In 2015, the biggest skeptics of the dangers of AI showed
everyone that these risks wouldn't happen for hundreds of years. In 2020,
analysts said that humanity probably had a few decades to prepare. In 2025, the
CEOs of AI companies predict they can create superhumanly good AI researchers in
one to nine years, while the skeptics assure that it will probably take at least
five to ten years. Ten years is not a lot of time to prepare for the dawn of
machine superintelligence, even if we are lucky to have that long. No, none of
that is going to happen.
