# Wait But Why

- He admits that standing at the cusp fleels "normal"
- A guy from 1750. time-ported to today would die of shock. True enough. What's
  this gotta do with Al?
- the 1750 guy would have to go back to 1500 and try to shock another guy -
  except with far less success
- Apparently the 1750 would have to go back to 12,000 BC!
- The law of accelerating returns (Kurzweil)
- what woks exponential could also just be more than linear.
- Which metric has actually grown exponentially? Which has grown linearly and
  which in between? (Which functions are linear, nonlinear and exponential?)
- "Rate of advancement" increases. Sure, but does it increase all that much?
- Quotes Kurzweil to establish the staggering nature of progress
- He's not recessarily subscribing to this yet. He's in explanation mode

- If Kuzswal is right; we'd die of shock by only 2030. In 2050, the world would
  be unrecognizable.

> This isn't science fiction. It's what many scientists smarter and more
> knowledgeable than you or I firmly believe - and if you look at history, it’s
> what we should logically predict.

* Should you, though? _What_ (other than population, internet usage, fossil fuel
  consumption) actually shows the law of Acelerating returns?
* Side note about your own thoughts on Moore's Law. It cannot magically expedite
  everything. High IQ != smarter. Faster ALU Ops != smarter machines.

### Urban's 3 reasons on why we're skeptical of an outlandish forecast

1. We think in straight lines.

True, but just as disastrous, if not more, to think exponentially. Tons of room
between linear and exponential. Much in that gap which explains this shit.

> In order to think about the future correctly, you need to imagine things
> moving at a much faster rate than they’re moving now.

Why? Enough can be explained by just _faster_, or _more_.

2. The very recent history is the flat part of an S-curve.

Is it? BTW he does say that a "huge growth spurt" might be brewing right now.

3. Our own experience makes us stubborn old men about the future.

* On this he and I firmly agree.

> What we know doesn't give us the tools to think about the future.

---

All right; so far he's not said anything clearly outrageas. All he's done so far
is establish the law of accelerating returns and explained _why_ it's
mind-boggling; or might seem like BS, but may not be.

> Logic suggests that... at some point, they'll makes a leap so great that it
> completely alters life as we know it.

---

## The Road to Superintelligence

3 reasons why people are confused about AI:

1. Movies & pop culture
2. Broad topic
3. We don't know what AI is.

AI Calibers:
1. ANI: task specific
2. AGI: described here as something that reasons, plans, thinks abstractly,
comprehends complex ideas, learns quickly and from experience (It's interesting
that _this_ is what Urban believes AGI is. Even Sama, since LLMs, thinks of AGI
in much more ambitious terms.)
3. ASI: The Machine, or Samaritan, or Skynet.

Where we currently are: ANI -> bunch of routine examples.

> Each new ANI innovation quietly adds another brick on the road to AGI and ASI.

Does it, really? And what's _innovative_ here?

* ANI to AGI - interesting point about how it's easy to build something that
  plays chess, but hard to build something that recognizes faces.

* Increasing computing power - Moore's Law is a _historically reliable_ rule
  (apparently).

* Making it smart: Plagiarize the brain This is where I can delve into whether
  more parameters = smarter models.

> Optimistic estimates say we can do this (reverse engineer the brain) by 2030.

* Links again to a Kurzweil article :-\
* First mention of ANNs here.
* Nods to a _Pantheon_ like scenario about brain scanning and uploading
  consciousnesses.

> If emulating the human brain seems hopeless, remember the power of exponential
> growth.

What a fucking compounding of fallacies.

An aside: Urban has history with Elon Musk - not one that can be left off the
hook easily.

> Try to do what evolution did, but for us this time. So how can we simulate
> evolution to build AGI?

His answer is _genetic algorithms_, FFS!

Oh, and

> ... we have a lot of advantages over evolution... (check what he says here)

Good lord, the hubris!
>  It’s no doubt we’d be much, much faster than evolution—but it’s still not
>  clear whether we’ll be able to improve upon evolution enough to make this a
>  viable strategy.

Trial & Error != Random

I'm just really mad right now.

> Make this whole thing the computer's problem, not ours.

In other words, bootstrapped intelligence, modifying your own code.

---

An aside. Urban kicked off his Elon Musk series with an invited article titled
"The World's Raddest Man" in May 2015.

---

* Urban's relationship with Musk and Kurzweil might be worth exploring.
* Sheldon Cooper circumvents inventing a time machine.

> All of these things ^ could happen soon... AGI could creep up on us because
> of...

Guess what? Exponential growth!!

---

## Road from AGI to ASI - Hardware & Software

* Bostrom's "village idiot" - that's what ChatGPT and Gemini today are.

> Considering the scale of intelligence (not just human); the village idiot is
> only a few steps away from Einstein.

And what then? An intelligence explosion.

> I want to pause here to remind you that every single thing I’m going to say is
> real—real science and real forecasts of the future from a large array of the
> most respected thinkers and scientists. Just keep remembering that.

* The intelligence explosion is the ultimate example of the law of accelerating
  returns.

Well, in that case, it's very likely that the explosion will be as full of gas
as the law itself.

* Beliefs of beliefs of beliefs - cascading, compounding fallacies. None of
  which have been critically appraised often enough.

* Every chart and graph on this site screams "exponential", "nonlinear" and
  "extreme".

---

# Will it be a nice God?

* Full on scifi territory.
* What's superintelligence? Speed vs quality of intelligence
* Admits that no way to know what ASI will do or what the consequences for us
  will be.
* Nick Bostrom: Extinction and immortality as attractor states.

> Two shocking facts: ASI affords the chance to be immortal, and it also
> threatens to make us extinct.
> Again, a lot of the smartest people on the planet have been trying to answer
> this.

## When comes ASI?

* So Urban wasn't exactly exaggerating or lying when he says that Jeremy Howard
  believes this shit. But Howard wasn't talking about AGI and stuff.
* He acknowledges that criticisms of Kurzweil exist from the {Paul Allen, Gary
  Marcus, Mitch Kapor} camp.
* Also quotes a 3rd camp containing Bostrom which says that neither camp is on
  solid ground. This could happen; we don't know when. It may not happen.
* Hubert Dreyfus argues it will never happen.
* It's fascinating that Urban brings this all up so late in the while story.

Names Dropped: Vernor Vinge, Ben Goertzel, Bill Joy, Ray Kurzweil, Jeremy
Howard.

Further sources:

1. Jeremy Howard's TED talk and other writings
2. Bostrom and Muller's surveys (there's a nice switcheroo that Urban pulls
   here.)
3. Kurzweil has had many of his predictions come true, apparently. See if this is actually true. Use the superforcasting method.


---
Asides: The Economics of Superintelligence - The Economist: 24th July 2025
