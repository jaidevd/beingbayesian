## cal-newport.md
- Confirm Amodei et al. 2020 scaling-law claims and compare Satya Nadella/Jensen Huang post-training rhetoric with earlier scaling expectations (`cal-newport.md:12`, `cal-newport.md:39`, `cal-newport.md:42`, `cal-newport.md:68`).
- Trace benchmark data showing GPT-3 on the predicted scaling curve while GPT-5 under-delivered by aligning The Information leak with Karen Hao’s timeline (`cal-newport.md:17`, `cal-newport.md:25`, `cal-newport.md:28`).
- Pull Microsoft capex, AI revenue, and layoff numbers plus Ed Zitron’s reporting to validate the “AI layoffs are hype” claim (`cal-newport.md:7`, `cal-newport.md:49`).
- ~~Locate the TechCrunch piece on Ilya Sutskever’s “back to discovery” shift to reinforce the slowing-scaling narrative (`cal-newport.md:32`)~~.

## notes.md
- Read Apple’s “Illusion of Thinking” paper to see whether claimed reasoning gains are post-training gloss (`notes.md:8`) - this goes to show that Nadella's claims are not really all that jazz.
- Examine arguments that latent diffusion breaks classical scaling laws to identify architectures that defy smooth scaling curves (`notes.md:7`).
- Revisit Joseph Weizenbaum’s “magic crumbling away” essay for a historical anchor on demystifying AI claims (`notes.md:12`).

## karen-hao.md
- Investigate the “apocalypse before AGI” self note by gathering socioeconomic collapse indicators tied to generative AI deployment (`karen-hao.md:32`).
- Quantify Goldman Sachs, Upwork, and Bloomberg data on AI productivity versus workload to build an economic reality check (`karen-hao.md:40`).
- Track Sam Altman’s AGI rhetoric alongside regulatory maneuvers to document power consolidation (`karen-hao.md:68`).
- Compile historical press on Frank Rosenblatt to chart the lineage of anthropomorphic marketing (`karen-hao.md:800`).
- Verify OpenAI’s “inevitability” narrative by tracing DeepSeek’s rise and Silicon Valley–specific funding conditions (`karen-hao.md:1215`).
- Audit GPT training data sourcing (Reddit, Books2) and resulting lawsuits to understand structural IP risks (`karen-hao.md:1243`).
- Monitor the $6.6 B 2024 round’s clawback clause and the October 2026 conversion deadline for governance fallout (`karen-hao.md:2359`).
- Review LessWrong discussions on stock clawback to see how alignment debates shaped board actions (`karen-hao.md:2241`).

## hai/
- Mine the Stanford AI Index reports (2017–2025) for neutral baselines on investment, benchmark performance, and workforce statistics (`hai/urls.txt:1`).

## hinton.md
- Compare Geoffrey Hinton’s belief in machine consciousness with Mark Rowlands’s philosophy to ground claims about subjective experience (`hinton.md:5`, `hinton.md:28`).
- Cross-reference his takes on Ilya Sutskever versus Sam Altman and the global competition narrative with other safety leaders’ public statements (`hinton.md:18`, `hinton.md:20`).

## karpathy.md
- Validate agent pessimism and “slop” critiques by reviewing repo-level failure cases and developer productivity studies (`karpathy.md:31`, `karpathy.md:49`).
- Investigate claims that reinforcement learning remains inefficient relative to imitation/supervised approaches for complex tasks (`karpathy.md:57`).
- Document coding-assistant shortfalls (custom DDP, deprecated APIs) to quantify maintenance debt from overfitted models (`karpathy.md:51`).

## sama-cleo-abram.md
- Benchmark GPT-5’s coding and cross-domain reasoning against LLM Arena, SWE-Bench, and other evaluations to test Sam Altman’s boast (`sama-cleo-abram.md:12`).
- Track the promised “significant scientific discovery” timeline and criteria to assess the credibility of the 2027 bet (`sama-cleo-abram.md:14`).

## tim-urban.md
- Plot which technology metrics are exponential versus linear to test the law-of-accelerating-returns thesis (`tim-urban.md:11`).
- Investigate Tim Urban’s reliance on Ray Kurzweil and Elon Musk by fact-checking their predictive records and mutual influence (`tim-urban.md:137`).

## yudkowsky.md
- Scrutinize the “grown by gradient descent” framing against Sutton’s Bitter Lesson and interpretability critiques (`yudkowsky.md:92`, `yudkowsky.md:136`).
- Analyze the Claude cheating anecdote to clarify what failed in Anthropic’s safety protocol and whether it generalizes (`yudkowsky.md:188`).
- Evaluate self-amplifying risk analogies using real engineering disasters to separate metaphor from evidence (`yudkowsky.md:280`).
- Verify multi-domain reasoning claims such as O1’s physics-to-biology example through reproducible benchmarks (`yudkowsky.md:22`).

## draft.md
- Use Joseph Weizenbaum’s “magic crumbles away” passage as an anchor for contrasting perceived versus actual capability in future writing (`draft.md:1`).

## Others
* Look at the jobs data (there's a chapter in each of them) from Stanford HAI
