# ðŸ“˜ Notes by Page Number

p15
> Nothing about this (ChatGPT, etc) form of AI coming to the fore or even
> existing at all was inevitable; it was the culimation of thousands of
> subjective choices, made by the people who had the power to be in the decision
> making room.

---

## **Page 16**

* Rarely have they seen any trickle-down gains of the so-called technological
  revolution. The benefits of generative AI mostly accrue upward.
* They projected racist, dehumanizing ideas of their own superiority and
  modernity to justify and even entice the conquered into accepting the invasion
  of sovereignty, the theft, and the subjugation.

---

## **Page 17**

* So too do the new employers exploit the labor of people globally to clean,
  tabulate, and prepare the data for spinning into lucrative AI technologies.
* Around the time Microsoft invested $10 billion in OpenAI, it laid off 10,000
  workers to cut costs.
* The current AI paradigm is also choking off alternative paths to AI
  development.

---

## **Self Note**

* The apocalypse might come before AGI.

---

## **Page 18**

* At the same time, more and more doubts have risen about the true economic
  value of generative AI. In June 2024, a Goldman Sachs report noted spending on
  the technology's development was projected to hit 1 trillion dollars in a few
  years with so far, quote, little to show for it, unquote. The following month,
  a survey from the Upwork Research Institute of 2,500 workers globally found
  that while 96% of C-suite leaders expected generative AI to boost
  productivity, 77% of the employees actually using the tools reported them
  instead, adding to their workload. This was in part due to the amount of time
  spent reviewing AI-generated content, in part due to growing demands from
  superiors to do more work. In a November Bloomberg article reviewing the
  financial tally of generative AI impacts, staff writers Parmi Olson and
  Carolyn Silverman summarized it succinctly. The data raises an uncomfortable
  prospect that this supposedly revolutionary technology might never deliver on
  its promise of broad economic transformation, but instead just concentrate
  more wealth at the top.
* Meanwhile, the rest of the world is beginning to collapse under the weight of
  the exploding human and material costs of this new era. Workers in Kenya earn
  starvation wages to filter out violence and hate speech from OpenAI's
  technology, including ChatGPT. Artists are being replaced by the very AI
  models that were built from their work without their consent or compensation.
  The journalism industry is atrophying as generative AI technologies spawn
  heightened volumes of misinformation. Before our eyes, we are seeing an
  ancient story repeat itself - and this is only the beginning.

---

## **Page 19**

* To quell the rising concerns about Generative AI's present-day performance,
  Altman has trumpeted the future benefits of AGI even louder. In a September
  2024 blog post, he declared that the intelligence age characterized by massive
  prosperity would soon be upon us, with superintelligence perhaps arriving as
  soon as in a few thousand days. I believe the future is going to be so bright
  that no one can do it justice by trying to write about it now, he wrote.
  Although it will happen incrementally, astounding triumphs, fixing the
  climate, establishing a space colony, and discovery of all physics will
  eventually become commonplace. At this point, AGI is largely rhetorical, a
  fantastical, all-purpose excuse for OpenAI to continue pushing for ever more
  wealth and power. Few others have the comparable capital to invest in
  alternative options. OpenAI and its small handful of competitors will have an
  oligo pulley on the technology they are selling us as the key to the future.
  Anyone, whether government or company, who wants a piece of that vision will
  have to rely on the empires to provide it.

---

## **Page 24**

* Later, at a recurring AI salon event at Stanford, a young researcher named
  Timnit Gebru would come up to him after a talk and ask him why he was so
  obsessed with AI, when the threat of climate change was clearly more
  existential. Climate change is bad, but it's not going to kill everyone, he
  said. AI could render humanity extinct.

---

## **Page 27**

* But on his blog in February 2015, Altman agreed with Musk that
  superintelligence was, quote, probably the greatest threat to continued
  existence of humanity, end quote. Even though a devastating engineered virus
  was more likely to happen, he said that it was, quote, unlikely to destroy
  every human in the universe, unquote. Incidentally, he wrote in Parenthetical,
  Nick Bostrom's excellent book Superintelligence is the best writing I've seen
  on the topic. It is well worth a read.

---

## **Page 28**

* I am now very much in the AI will be a tool camp, he told Business Insider in
  2023. Though I do think future humans and human society will be extremely
  different and we have a chance to be thoughtful about how to design that
  future.

---

## **Page 32**

* Looped wouldn't become a great success. After a seven-year run, Altman would
  sell it in 2012 for 43.4 million dollars, around what his investors put in.
  But if you had listened to his interviews and his backers at the time, his
  startup would have sounded like it was on the precipice of ushering in a great
  transformation.

---

## **Page 45**

* It would reveal just how much the quest for dominance of that technology,
  already restructuring society and terraforming our Earth, ultimately rests on
  the polarized values, clashing egos, and messy humanity of a small handful of
  fallible people.

---

## **Page 47**

* In 2012, together with another one of Hinton's grad students, Alex Krizevsky,
  they shocked the AI world by sweeping the floor at an academic contest called
  ImageNet to build software for automatically identifying objects in photos.
* The discussion ping-ponged back and forth between academic deliberations about
  different approaches to AI research and Musk's particular fixation on whether
  there was still time to beat out DeepMind and Google, essential, they
  believed, to correcting the course of AI development.
* AGI was also central to the discussion, which at the time was highly unusual.
  Most serious scientists considered the idea of digitally replicating true
  human-level intelligence to be science fiction, or at the very least decades
  or more away from attainability. Bold declarations that it was within reach
  enough to invest in it presently was viewed largely as pseudoscience and
  quackery. But Hassabis had embraced that term to describe the ambitions of
  DeepMind, despite a belief among his own research staff that this was
  distasteful, shameless marketing. The Rosewood group equally felt that the
  same goal, AGI, would best describe their own aspirations if they intended to
  form a competitor to go toe-to-toe with Hassabis' organization.

---

## **Page 48**

* Even to Sutskiver, who secretly believed AGI was possible and would come to
  full-throatedly endorse some of the most aggressive predictions about the
  speed of its creation, the brazen talk at first made him a little squeamish.
  If other researchers found out that he was openly discussing the pursuit of
  this objective, he worried he risked losing his credibility within the
  scientific community. Those concerns did not hold back Brockman, an outsider
  to the field, and sincere in his belief that AGI, with enough effort and
  focus, could just be around the corner. That Sutskiver and other researchers
  were willing to at least privately entertain the feasibility of a new lab
  could only have strengthened Brockman's confidence. As Altman drove him back
  from the hotel to San Francisco that night, Brockman told him that he was
  ready to commit himself to the project.

---

## **Page 49**

* Altman and Musk also had their fair share of recruiting conversations, slowly
  loosening up researchers resistant to the idea of AGI. AGI might be far away,
  but what if it's not?
* OpenAI, the anti-Google, would conduct its research for everyone, open-source
  the science, and be the paragon of transparency.

---

## **Page 50**

* In later correspondence, the group acknowledged that they could walk back
  their commitments to openness once the narrative had served its purpose and as
  the need arose, such as to avoid bad actors getting their hands on the
  technology. As we get closer to building AI, it will make sense to start being
  less open. Satskever raced to the trio in January 2016, shortly after OpenAI
  launched. The open in OpenAI means that everyone should benefit from the
  fruits of AI after it's built, but it's totally okay to not share the science.
  Yup, Musk responded.

---

## **Page 51**

* We are outmanned and outgunned by a ridiculous margin of organizations you
  know well. But we have right on our side, and that counts for a lot. I like
  the odds.
* An accounting of the societal impacts of commercializing AI research returned
  an unsettling scorecard. Automated software being sold to the police, mortgage
  brokers, and credit lenders were entrenching racial, gender, and class
  discrimination. Algorithms running Facebook's newsfeed and YouTube's
  recommendation systems had likely polarized the public, fueled misinformation
  and extremism, enabled election interference, and most horrifying in the case
  of Facebook, precipitated ethnic cleansing in Myanmar.
* It became a cynical refrain among AI researchers, sell out to big tech or to
  the military-industrial complex, or leave AI research. Between these binary
  extremes, OpenAI seemed like a third way, corrupted by neither profit nor
  state power. It was a beacon of hope, said Chip Huyen, a machine learning
  engineer and popular tech blogger observing from the sidelines.
* Here was a group of people, 9 out of 11 of whom were white men, being showered
  in previously unheard of amounts of money, speaking about the theoretical
  prospect of a bad superintelligence taking over the world, and proposing to
  counteract it by building a better superintelligence. That night, Gebru
  drafted a scathing critique of what she'd observed in an anonymous open
  letter. The spectacle, the cult-like exaltation of AI celebrities, and most of
  all, the overwhelming homogeneity of people building and shaping such a
  consequential technology. This homogeneous culture was not only pushing away
  talented researchers, but also leading to a dangerously narrow conception of
  AI, and of who could benefit from the technology.

---

## **Page 54**

* He created a company policy requiring all employees to work out of the San
  Francisco office, a policy that OpenAI would hold on to until the pandemic.
  This of course came with some trade-offs. Not everyone wanted to live in the
  Bay Area, he acknowledged. I would learn through my other interviews that this
  was particularly true for women and people of color who, like Gabru, felt
  alienated by the white and male culture of the dominant tech industry. But to
  Brockman, cohesion was more important and being physically together helped
  with the serendipitous exchange of ideas.

---

## **Page 55**

* When considering the criticisms levelled at OpenAI for its pursuit of AGI, he
  drew parallels with Edison's lightbulb. A committee of distinguished experts
  said it's never going to work, and one year later, he shipped, Rockman said.
  How could that be? It was, as science writer Arthur C. Clarke in the book of
  Profiles of the Future called it a failure of imagination.
* In 2016, while still at Google, Amodei co-wrote a foundational paper to the
  discipline articulating a central problem in AI safety as addressing, quote,
  the problem of accidents in machine learning systems defined as unintended and
  harmful behavior that may emerge from poor design of real-world AI systems,
  unquote. This was distinct from other AI-related challenges he and his
  co-authors wrote, including privacy, security, fairness, and economic impact.
  AI safety in this framework, in other words, was about preventing rogue,
  misaligned AI, the route from which, as described by Nick Bostrom,
  superintelligence could become an existential threat.

---

## **Page 56**

* Both Amodei siblings were sympathetic to Effective Altruism, or EA, movement,
  a controversial ideology that had spawned among philosophers at Oxford, where
  Bostrom was based, and taken hold in Silicon Valley. Over time, the movement,
  which preaches dedicating oneself to doing maximal good in a world by using
  extreme rationality and counterintuitive logic to guide decisions had, in no
  small part due to Bostrom's influence, identified the existential threat of
  rogue AI as a leading issue area for its adherents to pursue.
* But this existential brand of AI safety built on philosophical thought
  experiments would soon come under fire as the AI research community awakened
  to the less apocalyptic and immediate real-world harms of AI. Around the same
  time Amodi published his paper, ProPublica published a groundbreaking
  investigation called *Machine Bias* that revealed algorithms were being used
  across the US criminal justice system in misguided attempts to predict future
  criminals, and those algorithms were classifying black people as higher risk
  than white ones who had more extensive criminal records. This piece and an
  overall souring on Big Tech Post 2016 over the harms of social media sparked a
  new way of research reckoning with the harmful societal impacts of AI.
* Deborah Raji, an AI accountability researcher at the University of California,
  Berkeley, would come to champion the re-examination of the overwhelming focus
  of AI safety research on theoretical rogue AI and its possible existential
  risk to the detriment and de-prioritization of other real evidence-based
  problems, co-authoring a 2020 paper in response to Amodeus. She argued that
  the truly safe AI systems could not be built by isolating the behaviors of the
  technical systems themselves without placing them in the full context of the
  impacts on the very things, that is, privacy, fairness, and economics, that
  Amodeus had set apart. Where Amodeus had raised the idea of AI creating
  negative side effects as it relentlessly pursued an objective, using an
  example akin to the paperclip thought experiment of a cleaning robot knocking
  over a vase or damaging the walls on its path to tidying up, Raji pointed out
  that this was already happening. In its relentless pursuit of commercial
  products and AGI, the AI industry had produced an expansive negative side
  effects, including the wide-scale infringement of privacy to train facial
  recognition and the aspiring environmental cost of the data centers required
  to support the technology's development.

---

## **Page 57**

* In reality, for AI systems to even be built, there is very often a hidden
  human cost. We don't plan to release all of our source code, Altman said, but
  let's please try not to correct that. That usually only makes it worse. But
  what is the goal? Amody asked. Our goal right now is to do the best thing
  there is to do, Brockman replied. It's a little vague.

---

## Page 58

By the end of 2020, the Amodei siblings would become so disturbed by what they
viewed as arguments on OpenAI's break from its original premise that they would
cleave off to form another AI lab, Anthropic, taking critical staff with them
and creating a rivalry that would play a pivotal role in the frenzied release of
ChatGPT. Karnofsky would step down from OpenAI's board, having served his term
and due to new conflict of interest. On the list of candidates he nominated for
his replacement, he would include one of his former employees, Helen Toner.

Its list of projects sprawled every which way in a kitchen sink reflection of
the field. It was using robots and video games and simulated virtual worlds for
training agents, all as ways of trying to reach more advanced AI capabilities.
Little was working and what did work felt derivative of something somebody else
had already done. Whatever EJI was, it wasn't that. â€œThe bigger projects they
had, it didnâ€™t seem like they were doing anything super innovative,â€ says Nikhil
Mishra, an AI researcher who interned at OpenAI in 2017. [Side note: Prof Umesh
on ChatGPT]

---

## Page 59

In 2016, OpenAI spent more than $7 million of its $11 million budget in expenses
on compensation and benefits.

So, in March 2017, Brockman and Satskever began in earnest to develop a more
focused research roadmap. Their central question: what would it really take for
OpenAI to reach AGI, and be the first to do so? Satskever intuitively believed
it would have to do with one key dimension above all else â€” the amount of
compute, a term of art for computational resources, that OpenAI would need to
achieve major breakthroughs in AI capabilities. The imaginary competition and
subsequent advancements that he had been a part of had all involved a material
increase in compute. The advancements had other involved things also,
significantly more data and more sophisticated algorithms, but compute,
Satskever felt, was king. And if it were possible to scale compute enough to
train AI model at human brain scale, he believed something radical would surely
happen â€” AGI.

---

## Page 60

Moore's law turned into a self-fulfilling prophecy. It became the target for how
quickly chipmaking firms believed they needed to innovate in order to keep up
with competition and stay relevant. Brockman and Sutskever performed a simple
calculation. Based on the pace of Moore's law, how long would it take to reach
the level of compute OpenAI needed for brain-scale AI? The answer was bad news.
It would take far too long. Around the same time, Amodei and another researcher,
Danny Hernandez, had begun to look at the same idea from a different direction.
On a simple chart with time as the x-axis, they plotted the amount of compute
every major breakthrough in AI research had used since 2012, beginning with
Sutskever's grad school breakthrough â€” the start of the AI revolution. They
discovered that compute use was in fact going faster than Moore's law, much
faster. In the last six years, it doubled every 3.4 months or, put another way,
increased 30 million percent.

---

## Page 61

Chipmaking firms had imposed Moore's law on their companies with existential
fervor. The leadership now saw OpenAI's law in the same light. In 2017, a custom
NVIDIA server with 8 of their best GPUs cost $150,000, a price that would rise
roughly with inflation to nearly $195,000 by 2023. In the coming years, OpenAI's
law was projecting that OpenAI would need thousands, if not tens of thousands,
of GPUs to train just a single model. The cost of electricity to power that
training would also explode. OpenAI needed money â€” not just a billion dollars,
but billions of dollars â€” to sustain itself in the coming years.

---

## Page 62

The realization would lead the organization to lose its financial footing. To
Brockman and Sutskever, it challenged the very premise of OpenAI's structure.
How could a non-profit raise that much money annually to keep up with the pace
required to stay number one? They briefly considered merging with a chip
startup, but in the summer of 2017, they began serious discussions with Altman
and Musk about whether OpenAI needed to transform into a for-profit. This was
their best hope to entice investors with a chance at gathering financial return.
After several weeks of negotiations, the deliberations ended abruptly, without
resolution. If OpenAI were to become a for-profit, Altman â€” who was in the
middle of considering his run for California governor and getting a lackluster
reception in focus groups â€” wanted to be the company's chief executive. So did
Musk. He wanted full control of the lab and to have majority equity.

Elon, we really want to work with you, Sutskever wrote. We believe that if we
join forces, our chances of success in the mission is the greatest. But Musk's
desire for total control felt antithetical to OpenAI's original spirit. You are
concerned that Demis could create an AGI dictatorship. So are we. So it is a bad
idea to create a structure where you would become dictator if you chose to. Sam,
when Greg and I are stuck, you've always had an answer that turned out to be
deep and correct, Sutskever continued. That said, Altman's behaviors had often
left the two confused about his true beliefs and intentions. We don't understand
why the CEO title is so important to you, he wrote. Your stated reasons have
changed and it's hard to really understand what's driving it. Is AGI truly your
primary motivation? How does it connect to your political goals? How has your
thought process changed over time?

---

## Page 64

Musk soon arrived at his own conclusion for how to solve OpenAI's money problem.
In Jan 2018, Andrij Karpathy emailed Musk with new data showing how much Google
was dominating AI research publications. â€œWorking at the cutting edge of AI is
unfortunately expensive,â€ Karpathy wrote. â€œIt seems to me that OpenAI todayâ€™s
burning cash and the funding model cannot reach the scale to seriously compete
with Google, which is an 800 billion company.â€ While turning OpenAI into its own
for-profit could help raise capital, it would require the lab to develop an AI
product from scratch â€” a significant distraction from its fundamental AI
research. But by then, Altman had abandoned his political plans and succeeded in
his efforts to persuade Brockman, and through Brockman, Satskever that he would
be the better leader. With this group's decision, Musk no longer wanted to be
publicly affiliated with the organization. â€œI will not be in a situation where
the perception of my influence and time doesn't match the reality,â€ he'd
previously written. A few weeks later, Musk stepped down as OpenAI co-chair.
Altman became president of the non-profit.

---

## Page 67

In April 2018, OpenAI released a charter to pave the way for the transition.
Without publicly revealing anything about the change to come, the document
reiterated the lab's purpose. Now, with the new wording, OpenAI's mission is to
ensure that AGI benefits all of humanity. Such a mission, the document added,
would need OpenAI to be on the cutting edge of AI capabilities and require
substantial sources. It could mean walking back the commitment to release the
lab's research due to safety and security concerns. For the first time, OpenAI
also spelled out its AGI definition:

> â€œHighly autonomous systems that outperform humans at most economically
> valuable work.â€

---

## Page 68

Within half a year, OpenAI and Microsoft were discussing a deal in earnest.
Altman laid the legal groundwork, hurrying along the creation of the Limited
Partnership and appointing himself as CEO. Internally, the project was codenamed
*Oregon Trail*. To keep the deal secret from prying eyes, the for-profit entity
was also incorporated under the alias *Summersafe LP*. The name was a reference
to an episode of the cartoon show *Rick and Morty*, where the titular
characters, mad scientist Rick and his grandson Morty, leave behind Morty's
older sister Summer for another universe and instruct their car to â€œkeep Summer
safe.â€ The car takes the objective seriously, resorting to extreme and harmful
mechanisms of defense, including murder, paralyzing and torturing people who
approach the vehicle. It was a nod to the potential pitfalls of AI.

---

## Page 70

Brockman responded under his username **GDB**:

> â€œWe believe that if we do create AGI, we'll create orders of magnitude more
> value than any existing company.â€

Another user followed up:

> â€œEarly investors in Google have received a roughly 20x return on their
> capital. Google is currently valued at $750 billion. Your bet is that you'll
> have a corporate structure who returns orders of magnitude more than Google.
> But you don't want to unduly concentrate power.â€

They wrote, questioning the charter:

> â€œWhat exactly is power, if not the concentration of resources?â€

---

## Page 71

For Gates, *Dota 2* wasn't all that exciting, nor was he moved by robotics. The
robotics team had created a demo of a robotic hand that learned to solve a
Rubik's Cube through its own trial and error, which had received universally
favorable coverage. Gates didn't find it useful. He wanted an AI model that
could digest books, grasp scientific concepts, and answer questions based on the
materialâ€”to be an assistant for conducting research. OpenAI had only one project
that approached fitting the bill: a large language model called **GPT-2** that
was capable of generating passages of text that closely resembled human writing.

In February that year, OpenAI had taken the unusual step of proclaiming to the
press that this model, once advanced a little further, could become an
exceedingly dangerous technology. Authoritarian governments or terrorist
organizations could weaponize the model to mass-produce misinformation. Users
could overwhelm the internet with so much trash content that it would be
difficult to find high-quality information.

OpenAI would take the ethical road, it said, and withhold the full version of
the model, which had **1.5 billion parameters**, an approximate measure of the
model's size and complexity. Instead, to give the public just a taste of the
kind of capabilities that society needed to prepare for, it would publish only a
diminished versionâ€”less than one-tenth of the sizeâ€”that had a limited ability to
generate a few sentences at a time, but was prone to non-sequiturs and
repetition.

---

## Page 72

A month later, on July 22, 2019, Microsoft announced its **$1 billion**
investment. Under the terms of the deal, its returns would be capped at **20x**.

---

## Page 74

But there, ensconced in the cheery glow, magazines strewn across the tables, it
was easy to live in a gentler reality. An employee would later tell me that this
was emblematic of her time at the company. Joining it was like stepping into an
alternate universe. Only after she left did she snap back down to earth.

---

## Page 75

Until that year, OpenAI had been something of a stepchild in AI research. It had
an outlandish premise that AGI could be attained within a decade, when most
non-OpenAI experts doubted it could be attained at all. To much of the field, it
had an obscene amount of funding despite little direction and spent too much
money on marketing and what other researchers frequently snubbed as unoriginal
research. It was, for some, also an object of envy. As a non-profit, it had said
that it had no intention to chase commercialization. It was a rare intellectual
playground without strings attached, a haven for fringe ideas.

What was clear was that OpenAI was beginning to exert meaningful sway over AI
research and the way policymakers were learning to understand the technology.
The lab's decision to revamp itself into a partially for-profit business would
have ripple effects into spheres across its influence in industry and
government.

---

## Page 76

Brockman nodded vigorously. He was used to defending OpenAI's position.

> â€œThe reason that we care so much about AGI and that we think it's important to
> build this is because we think it can help solve complex problems that are
> just out of reach of humans.â€

Why did we need AGI to do that instead of AI?

---

## Page 77

As defined by OpenAI, AGI referred to a theoretical pinnacle of AI researchâ€”a
piece of software that had just as much sophistication, agility, and creativity
as the human mind to match or exceed its performance on most economically
valuable tasks. The operative word was *theoretical*.

Since the beginning of earnest research into AI several decades later, debates
had raged about whether silicon chips encoding everything in binary zeros and
ones could ever simulate brains and the other biological processes that gave
rise to what we consider intelligence. There had yet to be definitive evidence
that this was possibleâ€”which didn't even touch on the normative discussion of
whether people should develop it.

AI, on the other hand, was the term du jour for both the version of the
technology currently available and the version that researchers could reasonably
attain in the near future through refining existing capabilities. Those
capabilities, rooted in powerful pattern matching known as machine learning, had
already demonstrated exciting applications in climate change mitigation and
healthcare. [**Double Speak**]

In a white paper, the organization detailed ten categories of those challenges
particularly well-suited to existing machine learning capabilities, including
making buildings more efficient, optimizing the load distribution of power grids
to integrate more renewable energy, and discovering new materials for green
energy generation and storage for more carbon-efficient cement and steel.

---

## Page 78

Researchers from both organizations would tell me that the main challenge of
working in these areas was not technical limitations â€” it was quite the
opposite. Persuading talented scientists to focus on problems that necessitated
rather simple machine learning solutions instead of the latest cutting-edge
techniques that satisfied their ambitions and looked better on a research
rÃ©sumÃ©.

It was also finding the political will to deploy these solutions globally.

> â€œTechnologies that would address climate change have been available for years,
> but have largely not been adopted at scale by society,â€ wrote the Climate
> Change AI researchers in their white paper.

While they had hoped that AI would be useful in reducing the costs associated
with climate action,

> â€œHumanity must also decide to act.â€

---

## Page 79

OpenAI's challenge would be to build an AGI that gave everyone â€œeconomic
freedomâ€ while allowing them to continue to live â€œmeaningful livesâ€ in that new
reality. If it succeeded, it would decouple the need to work from survival.

This was a favorite argument in Silicon Valley â€” the **inevitability card**: *If
we donâ€™t do it, somebody else will.*

> â€œWhat is OpenAI?â€ he continued. â€œRockman, what is our purpose? What are we
> really trying to do?â€ â€œOur mission is to ensure that AGI benefits all of
> humanity, and the way we want to do that is build AGI and distribute its
> economic benefits.â€

I tried with little success to get more concrete details on what exactly they
were trying to build â€” which by nature, they explained, they couldnâ€™t know â€” and
why then, if they couldnâ€™t know, they were so confident it would be beneficial.

The lab had to build *good AGI* before somebody else built *bad AGI.*

---

## Page 80

That was undeniable, Satskeva said, but the payoff was worth it because AGI
would, among other things, counteract the environmental cost specifically. He
stopped short of offering examples.

> â€œIsn't Bitcoin like 1%?â€

â€œWow,â€ Satskeva said in a sudden burst of emotion that felt, at this point 45
minutes into the performance, somewhat performative.

Satskeva would later sit down with *New Yorker* reporter Cade Metz for his book
*Genius Makers*, which recounts the narrative history of AI development, and say
without a hint of satire:

> â€œI think it's fairly likely that it will not take too long a time for the
> entire surface of earth to become covered with data centers and power
> stations. There would be a tsunami of computing, almost like a natural
> phenomenon.â€

AGI, and thus the data centers needed to support them, would be too useful not
to exist.

I tried again to press for more details.

> â€œWhat you're saying is OpenAI is making a huge gamble that you will
> successfully reach beneficial AGI to counteract global warming before the act
> of doing so might exacerbate it.â€

â€œI wouldn't go too far down that rabbit hole,â€ Rockman said still. He cut in.

He spoke like Michelangelo, as though AGI already existed within the marble that
he was carving. All he had to do was chip away until it revealed itself.

---

## Page 81

That was undeniable, Satskever said, but the payoff was worth it because AGI
would, among other things, counteract the environmental cost specifically. He
stopped short of offering examples.

> â€œIsn't Bitcoin like 1%?â€

â€œWow,â€ Satskever said in a sudden burst of emotion that felt, at this point 45
minutes into the performance, somewhat performative.

Satskever would later sit down with *New Yorker* reporter Cade Metz for his book
*Genius Makers*, which recounts the narrative history of AI development, and say
without a hint of satire:

> â€œI think it's fairly likely that it will not take too long a time for the
> entire surface of Earth to become covered with data centers and power
> stations. There would be a tsunami of computing, almost like a natural
> phenomenon.â€

AGI â€” and thus the data centers needed to support them â€” would be too useful not
to exist.

I tried again to press for more details.

> â€œWhat you're saying is OpenAI is making a huge gamble that you will
> successfully reach beneficial AGI to counteract global warming before the act
> of doing so might exacerbate it.â€

â€œI wouldn't go too far down that rabbit hole,â€ Rockman said still. He cut in.

He spoke like Michelangelo, as though AGI already existed within the marble that
he was carving. All he had to do was chip away until it revealed itself.

---

## Page 84

OpenAI now had the long-term resources it needed to follow OpenAIâ€™s Law. And
this was imperative, Brockman stressed. Failing to stay on the curve was the
real threat that could undermine OpenAIâ€™s mission. If the lab fell behind, it
wouldnâ€™t be the best. If it werenâ€™t the best, it had no hope of bending the arc
of history towards its vision of beneficial AGI.

It justified OpenAIâ€™s consumption of an unfathomable amount of resources â€” both
compute, regardless of its impact on the environment, and data â€” the amassing of
which couldnâ€™t be slowed by getting consent or abiding by regulations.

---

## Page 85

It was a nice analogy, but Brockman seemed once again unclear about how OpenAI
would turn itself into a utility. Perhaps through distributing *universal basic
income*, he wondered aloud. Perhaps through something else.

He returned to the one thing he knew for certain: OpenAI was committed to
distributing AGIâ€™s benefits and giving everyone economic freedom.

> â€œWe actually really mean that,â€ he said. â€œAGI could be more extreme. What if
> all value gets locked up in one place? That is the trajectory we are on as a
> society and we have never seen that extreme of it.â€

---

## Page 86

It was â€œa fair criticism,â€ he said, that the piece had identified as a
disconnect between the perception of OpenAI and its reality. Here, Altman is
talking about a piece from *MIT Technology Review* â€” a profile of the company
that Karen Hao wrote herself.

> â€œI think we should, at some point in the future, find a way to publicly defend
> our team but not give the press the public fight theyâ€™d love right now.â€

OpenAI wouldnâ€™t speak to me again for three years.

---

## Power and Progress (Interlude)

In their book *Power and Progress*, MIT economists and Nobel laureates Daron
Acemoglu and Simon Johnson argue that every technology revolution must begin
with rallying ambition.

It is the promise of a technology benefiting everyone that puts in motion the
long journey of amassing enough talent and resources to turn it into a reality.

After analyzing 1,000 years of technology history, the authors conclude that
**technologies are not inevitable.**

The irony is that for this very reason, new technologies rarely default to
bringing widespread prosperity, the authors continue. As they turn their ideas
into reality, the vision they impose of what the technology is and whom it can
benefit is thus the vision of a narrow elite imbued with all their blind spots
and self-serving philosophies.

> *E.g., the cotton gin served only to intensify slavery.* [Neil Postman]

These two features of technology revolutions â€” their promise to deliver progress
and their tendency to instead reverse it for people out of power, especially the
most vulnerable â€” are perhaps truer than ever for the moment we now find
ourselves in with artificial intelligence.

Since its conception, the development and use of AI has been propelled by
tantalizing dreams of modernity and shaped by a narrow elite with the money and
influence to bring forth their conception of the technology.

That conception is what has led to the exploding social, labor, and
environmental costs that are playing out around the world today â€” particularly,
as we will see, in many Global South countries, for which the consequences of
their dispossession by historical empires still linger in delayed economic
development and weaker political institutions.

---

## Page 90

The term lends itself to casual anthropomorphizing and breathless exaggerations
about the technology's capabilities.

In 1958, two years after the fieldâ€™s founding, Frank Rosenblatt, a Cornell
professor, demonstrated the **Perceptron**, a system that could *blah blah blah
blah blah*.

Over his main collaboratorâ€™s objection, Rosenblatt advertised his system as
something akin to the human brain. He even ventured to say that it would one day
be able to reproduce and begin to have sentience.

His next warning: the *New York Times* announced that the Perceptron would, in
the future, be able to walk, talk, see, write, reproduce itself, and be
conscious of its existence.

*Note to self: Find everything that Rosenblatt wrote either in the* New Yorker
*or in the* New York Times.

---

## Page 91

This anthropomorphizing has become a rhetorical tool for companies to avoid
legal responsibility.

The fear of superintelligence is predicated on the idea that AI could somehow
rise above us in the special quality that has made us humans the planetâ€™s
superior species for tens of thousands of years. *(Mark Rowlands on what makes a
human)*

Myriad tests have developed over the centuries to measure intelligence against
these definitions, many of which have subsequently been debunked and fallen out
of favor due to their unsavory histories.

In the early 1800s, American craniologist Samuel Morton quite literally measured
the size of human skulls in an attempt to justify the racist belief that white
people â€” whose skulls he found were on average larger â€” had superior
intelligence to Black people. Later generations of scientists found that Morton
had fudged his numbers to fit his preconceived beliefs, and his data showed no
significant differences between the races.

IQ tests similarly began as a means to weed out the â€œfeeble-mindedâ€ in society
and to justify eugenics policies through scientific *objectivity.* More recent
standardized tests such as the SAT have shown high sensitivity to a test takerâ€™s
socio-economic background, suggesting that they may measure access to resources
and education rather than some inherent ability.

---

## Page 93

As software for each of these capabilities has advanced, researchers have
subsequently sought to combine them into so-called *multimodal systems* â€”
systems that can see and speak, hear and read.

That technology is now threatening to replace large swaths of human workers. It
is not deployed by accident, but by design.

Still, the quest for artificial intelligence remains unmoored.

With every new milestone in AI research, fierce debates follow about whether it
represents the recreation of a true intelligence or a pale imitation. To
distinguish between the two, **artificial general intelligence** has become the
new term of art to refer to the *real deal.*

This latest rebranding hasnâ€™t changed the fact that there is not yet a clear way
to mark progress or determine when the field will have succeeded.

Through decades of research, the definition of AI has changed as benchmarks have
evolved, been rewritten, been discarded. The goalposts of AI development are
forever shifting, and as the research director at *Data & Society*, **Jenna
Burrell**, once described it, it is â€œan ever-receding horizon of the future.â€

The technologyâ€™s advancement is headed toward an unknown objective with no
feasible end in sight. To justify the elongating timeline and ever-expanding
costs of pursuing the ambition of AI, the promises we are told about it have
grown more grandiose than ever before.

AI was once a scientific fascination, a technology with some potential
commercial utility. Now AI is the **harbinger of the Fourth Industrial
Revolution**, the keystone of modern superpower. AGI, if ever reached, will
solve climate change, enable affordable healthcare, provide equitable education.

OpenAI is the poster child of this line of thought.

> *Note to self:* They have no choice but to say this in order to justify the
> effort, the funding, the time, the money, and the talent cost.

---

## Page 94

Itâ€™s not a coincidence that AI today has become synonymous with colossal,
resource-hungry models that only a tiny handful of companies are equipped to
develop â€” and that desire us to make their products into the foundations for
everything.

Achieving AI must then involve encoding symbolic representations of the worldâ€™s
knowledge into machines, creating so-called **expert systems**.

At the helm of the connectionists was **Frank Rosenblatt** and his *Perceptron*,
an early proof of concept for a machine learning system. At the helm of the
symbolists was Rosenblattâ€™s nemesis, **Marvin Minsky**, an MIT professor and
co-organizer of the Dartmouth workshop.

Minsky had himself dabbled in connectionist thinking before souring on the idea.
He did not switch his loyalties quietly.

He found frequent opportunities to grandstand and ridicule his connectionist
colleagues who competed with him for the same grants, sparing not even
early-career researchers.

In 1969, he co-authored a book called *Perceptrons*, so critical of
connectionism that it is credited â€” along with the meddling progress of neural
networks â€” for killing off nearly all funding to that vein of research for more
than 15 years.

This is what is called the **AI winter.**

---

## ELIZA Note

In fact, the demonstration felt so convincing â€” the demonstration of **ELIZA**,
that is â€” to some psychiatrists that they began to speak of automated
psychotherapy as just around the corner.

And merely a few years after the founding of the AI field, computer scientists
were already prematurely concluding that natural language understanding in
computers was a solved problem.

Decades later, whether or not itâ€™s even being solved today is still an open
debate.

---

## Page 96

Weizenbaum published a tome called *Computer Power and Human Reason* in the
decade following Minskyâ€™s *Perceptrons* that argued that humans and machines are
different â€” and that the AI fieldâ€™s attempt to blur that distinction would lead
to profound societal consequences.

It would, for example, allow people in power â€” whether CEOs or politicians â€” to
execute their will through machines while absolving themselves of moral
responsibility.

---

## Page 98

In another instance of rebranding, Hinton later cleverly gave his multi-layer
processing the name **deep learning**, a shorthand for using deep neural
networks to perform machine learning.


In this telling of the story, the lesson to be learned is this, science is a
messy process, but ultimately the best ideas will rise despite even the loudest
detractors. Implicit within the narrative is another message, technology
advances with the inevitable march of progress. But there is a different way to
view this history. Connectionism rose to overshadow symbolism, not just for its
scientific merit. It also won over the backing of deep pocketed funders due to
key advantages that appeal to those funders' business interests.

## Page 99 Page number 99. After debuting Watson on late night TV, IBM
discovered that getting the system to produce the kind of results that customers
would actually pay for, such as answering medical rather than trivia questions,
could take years of upfront investment without clarity on when the company would
see returns. IBM called it quits after burning more than 4 billion with no end
in sight and sold Watson Health for a quarter of that amount in 2022.

## Page 101 Page number 101. The entwining of deep learning with commercial
interests simultaneously transformed the tech industry and the face of AI
development. To the public, generative AI would erupt seemingly out of nowhere
in late 2022 with open AI's launch of GPT. But from 2012 to 2022, beginning with
the ImageNet breakthrough, it was these shifts during the first major era of AI
commercialization that laid the groundwork for many characteristics of the
generative AI revolution today. 

## Page 102 Page number 102. That culture is now at the crux of a raging debate
in generative AI over whether tech companies can scrape books and artworks
wholesale to train their AI systems. To many AI developers who have long
operated under this mindset, that question seems rather quaint, taking it
seriously presents a direct obstacle to the moral pursuit of ever more progress.
Even as some of them have grown more aware of and concerned by the chasm between
their perspective and the view of many authors and artists who stand in
opposition, this way of thinking has been difficult to shake. In May 2023,
shortly after a group of artists filed suit for the first time against several
gen AI developers over the theft of their artwork, I went to an AI research
conference in Wanda as a reporter for the Wall Street Journal. As I walked
through the vaulted hall of the glistening dome shape convention center in the
country's capital, a senior researcher stopped me and asked me whether the Wall
Street Journal on my name badge was a new start-up for the media publication.
When I clarified that I was a journalist and it indeed stood for the Wall Street
Journal, another senior researcher chimed in. I recognized it because of the
Wall Street Journal data set, she said, referring to an early AI speech
recognition data set of people reading excerpts from the newspaper. I worked
with it many times.

## Page 103 Page number 103 A few months later, after I came across the start-up
of data privacy outcry in China, from parents horrified at their kids being
turned into guinea pigs, forced the company to pivot to a different application
of its technology. But the story left me with an uneasy feeling that the
successful backlash was an anomaly and the company's original approach to go to
countries eager to embrace the promise of technology of renting data donors and
product testers was in fact a trend.

## Page 104 Page number 104 As I recounted this worry to a colleague, she
introduced me to a phrase that had already been coined for the phenomenon, data
colonialism. I discovered the work of scholars Nick Caldrian Ulysses, a Mejias,
whose foundational text, The Costs of Connection, published just that year,
argued that Silicon Valley's pervasive datafication of everything was leading to
a return of distributing historical patterns of conquest and extractivism. The
following year, a paper called Decolonial AI from Shaki Mohamed and William
Isaac at DeepMind and Marie Therese PNG at the University of Oxford reinforced a
suspicion. I had begun to develop the AI industry in equal parts, fueled by and
fueling this datafication was in turn accelerating that new colonialism further.
The term extractivism comes from the Spanish word extractivismo and the
Portuguese word extractivismo coined decades ago by Latin American scholars
seeking to describe a global economic order that was disposing them of their
natural resources for little local regional benefit. A history and experience, I
detail more in chapter 12.

## Page 105 Page number 105 From 2013 to 2022, corporate investment in AI such
as Mergers acquisition shot up from 14.6 billion to 235 billion, peaking at
337.4 billion in 2021, according to the Stanford AI Index. Those numbers don't
even include the in-house company spending on research and development. In 2021,
Alphabet and Meta spend 31.6 billion and 24.7 billion respectively. By contrast,
the US government allocated 1.5 billion in 2021 to non-defense AI development.
The European Commission allocated a billion euros, that is 1.2 billion dollars,
the same year. Phase number 106 Universities could no longer afford the computer
chips or the electricity needed to work in the hottest areas of AI development.
As such, the same 2023 science study found that in just three years from 2017 to
2020, industry-affiliated models grew from 62% to above 91% of the world's
best-performing AI models. Midway through the first decade of AI
commercialization, most top-level AI research was now happening within or in
academic labs connected to tech companies. In another study, Kalluri, Agnew,
Bahrain and other colleagues found that 55% of the most influential AI research
papers had at least one industry co-author in 2018 and 2019. This was compared
with 24% a decade earlier. The research also consolidated heavily within just a
few corporations. Over the same decade, tech giants such as Microsoft and Google
more than tripled their share of corporate-affiliated papers to 66%. Ironically,
this was precisely the reason why Musk and Altman said that they wanted to start
open AI. The tech industry's profit motive had become the overwhelming force
driving AI development. Six years later, in April 2024, the National Highway
Traffic Safety Administration found that Tesla's autopilot had been involved in
more than 200 crashes, including 14 fatalities, in which the deep learning-based
system failed to register and react to its surroundings and the driver failed to
take it over in time to override it.

## Page 108 Page number 108. Don Song, a professor at the University of
California, Berkeley who specializes in this area of research known as
adversarial attacks, showed that prompting a language model with the right
message caused it to spit out sensitive data such as credit card numbers.

## Page 109 Page number 109. Deep learning is going to be able to do everything
he said, Jeffrey Hinton. Their modern-day nemesis was Gary Marcus, a professor
emeritus of psychology and neuroscience at the New York University, who would
testify in Congress next to Sam Altman in 23.

## Page 110 Page number 110. Despite the heated scientific conflict, however,
the funding for AI development has continued to accelerate almost exclusively in
the pure connectionist direction. Whether or not Marcus is right about the
potential of neurosymbolic AI is beside the point. The bigger root issue has
been the whittling down and weakening of a scientific environment for robustly
exploring the possibility and other alternatives to deep learning. A month
later, Altman released Dalit 2 to immense fanfare and Brockman cheekily tweeted
Dalit 2-generated image using the prompt, deep learning hitting a wall. The
following day, Altman followed with another tweet, give me the confidence of a
mediocre deep learning skeptic. Many open AI employees relished the chance to
finally get back at Gary Marcus.

## Page 111

In reality, the analogies to intelligence are once again anthropomorphizing and
exaggerating the capabilities of the technology. While Hinton and other deep
learning absolutists predicted that the shortfalls of neural networks compared
with humans would go away at sufficient scale, the challenges have in fact
persisted and by many accounts only gotten worse. 

## Page 113 Page number 113. While those probabilistic outputs can go
impressively far in mirroring human writing patterns, probable and accurate are
not the same thing. Altman has publicly tweeted that chat GPT is incredibly
limited, especially in the case of truthfulness. But open AI's website promotes
GPT force ability to pass the bar exam and the LSAT. Even the term
hallucinations is subtly misleading. It suggests that bad behavior is an
aberration, a bug when it's actually a feature of the probabilistic pattern
matching mechanics of neural networks. Unchecked hallucinations in such cases
could have serious downstream consequences. One 2023 study found that using chat
GPT to explain radiology reports could sometimes produce incomplete or harmful
summaries. In one extreme example, the chatbot simply simplified a report
detailing a growing mass in the brain as "brain does not seem to be damaged". 

## Page 115 Page number 115. But like the cotton gin in the 1970s, in the 1790s,
the education technology started in Massachusetts, the facial recognition
companies in South Africa, and the many more examples detailed in the coming
pages. The cost of this vision are pressing down on vast swaths of the global
population who are vulnerable. This is the empire's logic. The perpetuation of
the empire rests as much on rewarding those with power and privileges as it does
on exploiting and depriving those often far away and hidden from view without
them. In April 24, Dario Amode, by then the CEO of Anthropic, told New York
Times colonist Ezra Klein that the price of training a simple competitive
generative AI model was approaching a billion dollars and could be, by 25 and
26, reach an estimated 5 billion to 10 billion. The scaling doctrine had become
so ingrained that some even beginning to view it as something of a natural
phenomenon. Scaling compute is the way, not just a way, to reach more advanced
AI capabilities. Entire national strategies are being orchestrated around this
belief. The US government has moved aggressively to bar China's access to
American design computer chips in an effort to prevent its adversary from
attaining more powerful AI systems.

## Page 116 Page number 116. But scale is not the only pathway to improve
performance. Within deep learning, the neglected parts of improving the neural
network itself or even the quality of its training data can significantly reduce
the amount of expensive compute needed to reach the same performance. That's not
even considering the approaches that move away from deep learning. Neurosymbolic
AI, pure expert systems or even fundamentally new paradigms which could break
the logic of scaling. Open AI's law or what the company would later replace with
an even more fevered pursuit of so-called scaling laws is exactly the same. It
is not a natural phenomenon, it is a self-fulfilling prophecy.

## Page 118 Page number 118, Satskevar brought his diehard belief in deep
learning to open AI at a time when the field's confidence in the paradigm was
just beginning to falter and critics like Gary Marcus were pushing for new
thinking. Satskevar did not falter. His faith rested on the simple hypothesis
that underpinned connectionism, that the artificial nodes in a neural network
were sufficient approximations of the real neurons in a biological brain.

## Page 119 Page number 119, it wasn't that Satskevar was particularly
persuasive. If Altman was the politician, Satskevar was the opposite. He never
minced his words or massaged his language to potentially land better with his
audience. He simply delivered his opinions with a raw sincerity and outrageous
confidence that people either resonated with and found inspiring or did not.

## Page 120 Page number 120, one deep-mind scientist specialized in the study of
cognition and consciousness replied in the comments, quote, in the same way that
it may be that a large field of wheat is slightly pasta. The following year,
Satskevar would induce panic by programming at a conference that AJI would
eventually disappear all jobs. That fall, he would declare without scientific
backing on X. In the future, we will have wildly effective and dirt-chip AI
therapy after an OpenAI leader triggered online controversy for casually
comparing talking to GPT with professionally licensed therapy. Shortly after
GPT's release in 2022, OpenAI would host a holiday party at the California
Academy of Sciences. Satskevar would get up in front of the cowwearing in OpenAI
shirt and black blazer to give some short remarks with Brockman. At the end of
it, Satskevar, still why he has ever and now balding delivered what had become
his new mantra, feel the AJI, he said, feel the AJI.

## Page 124 Page number 124, the cluster of models that OpenAI trained leading
up to the final 1.5 billion version illustrated this relationship. Each one fell
neatly on a curve of increasingly increasing capability. So it was a little
surprised when the largest one, which they named GPT-2, markedly improved over
the juvenile text generation of GPT-1 to produce lengthy and coherent enough
prose to be confused with the humans. Compared with today's models, the text was
clunky and often descended into gibberish. But for the very first time, it was
suddenly possible to automate writing at scale.

## Page 125 Page number 125, it was one of many moments that made him, that is,
who are we talking about? Scratch that. Page number 125, this is being said of a
European employee. It was one of many moments that made him question the basic
premise of OpenAI's lofty goals. How could it benefit all of humanity when it
lacked the meaningful global representation?

## Page 126 Page number 126, Clark began a media offensive in 2019 February,
broadcasting widely to various publications that OpenAI had created a dangerous
technology and therefore was not releasing it. Instead, it would release only a
smaller version of 8% of the full-fledged models parameters to give the public a
taste of its capabilities. He, Amode and several other co-authored a blog post
with examples of GPT-2's outputs to illustrate its full potential. It's very
clear that if this technology matures, and I'd give it one or two years, it
could be used for disinformation or propaganda. He said to my then colleague at
MIT Technology Review, Will Knight, Clark sidestepped the fact that OpenAI was
the one leading the push to mature the technology on that timeline. We are
trying to get ahead of this, he said. OpenAI's moves sparked intense blowback
from external researchers who adhered strictly to the idea that open science was
the bedrock of the field, any organization that didn't participate should be
viewed suspiciously, more so if they were publicly boasting about the decision.
Many also viewed OpenAI's alarmism about what was essentially powerful
autocomplete software as poorly calibrated and ridiculous. GPT-2 was not nearly
so advanced enough to be a serious threat, and if it were, why tell everybody
about it and then preclude it from public scrutiny? The whole thing felt
disingenuous and like a self-aggrandizing publicity stunt. At Stanford, after
Radford gave a talk about GPT-2, a well-established natural language processor
would raise his hand to us the last question. So is it dangerous, he taunted.
The room burst out in laughter. Alec looks so sad. Remember the Stanford
researchers in the room. Stanford had so much contempt for OpenAI. OpenAI was
taking the lead in trialing what that process could look like to be not caught
flat footed. If we are right, it is possible to build AGI, Clark said. We sure
as shit need all the good information hazard procedures.

## Page 129 Page number 129. Where AMOD did seem continued promise was in GPT-2.
It represented a bet known in the field as pure language hypothesis. Language as
the theory goes is the primary medium through which humans communicate, meaning
all of the world's knowledge must at some point be documented in text. It
follows then that AGI should be able to emerge from training an algorithm in
massive amounts of language and nothing else. This idea is in contrast to the
grounding hypothesis, which asserts that the physical world or our ability as
humans to perceive and interact with it is just as crucial an ingredient to our
intelligence. AGI would then only be able to emerge from the combination of
language and perception like computer vision, as well as interactions such as
through a physical or virtual agent taking actions in the real world. In company
documents, researchers weighed the merits of the different approaches with AI
safety staff at one point debating the virtues of the pure language hypothesis
by drawing repugnant analogies to people with disabilities. The discussions
revealed how quickly measures of intelligence could wear into disturbing
assessments of groups of people who had superior or inferior intelligence. Many
at OpenAI had been pure language skeptics, but GPT2 made them reconsider. It was
only a matter of time before other people would start scaling up language models
further. That meant the best way to ensure beneficial AGI was for open AI to
leap ahead and with the internal lead time, figure out how to make its scaled
models safer. Obviously, misuse is not good, amore told me, but a language model
is a lot less powerful than an AGI. I am very worried about language models
being weaponized for disinformation and the sort that is very scary to me, but
at the same time, it is a relatively singular and clear and defined concern.
What does AGI look like? Well, you know, we are in the awkward position of we
don't know what it looks like. We don't know when it's going to happen, so we
look for things that aren't AGI, but that present at least some of the
opportunities and difficulties of AGI. And the hope is that if we can handle
those things well, then we are kind of ready for the bigger leagues. It was a
logic that worked under a specific assumption that AGI, despite being amorphous
and unknowable, was also inevitable. These number 132. As chat GPT swayed the
world by storm in early 2023, a Chinese AI researcher would share with me a
clear-eyed analysis that unraveled Open AI's inevitability argument. What Open
AI did never could have happened anywhere but Silicon Valley. He said, in China
which rivals the U.S. and AI talent, no team of researchers and engineers, no
matter how impressive, would get a billion dollars. Let alone 10 times more, to
develop a massively expensive technology without an articulated vision of
exactly what it would look like and what it would be good for. Only after chat
GPT's release did Chinese companies and investors begin funding development of
gargantuan models with gusto, having now seen enough evidence that they could
recoup their investments through commercial applications. Side note to self,
think about what this meant for deep-seek. In other words, Open AI, everything
Open AI did was the opposite of inevitable. The global, explosive global loss of
its massive deep-learning models and the perilous race sparked across the
industry to scale such models to planetary limits could only have ever arisen
from the one place that it actually did. Microsoft was about to deliver a
supercomputer to Open AI as a part of its investment with 10,000 NVIDIA V100s,
what were then the world's most powerful deep-learning models.

## Page 133

The idea seemed to many, nothing short of
absurdity. Before then, models were considered large-scale if trained on a few
dozen ships. In top academic labs in MIT or Stanford, PhD students considered
having considered it luxury to have 10 ships. In universities outside the US,
such as in India, students were lucky to share a single ship with multiple
peers, making do with a fraction of a GPU for their research.

## Page 134 Page number 134. It would accelerate the vicious cycle of
universities unable to compete, losing PhD students and professors to industry
atrophying independent academic research and spelling the beginning of the end
of accountability. It would amplify the environmental impacts of AI to an extent
that in the absence of transparency or regulation, neither the external experts
nor governments have been able to fully tabulate to this day.

## Page 135 Page number 135. For GPT-2, Radford had been selective about what
made it into the model, he scraped the text from articles and websites that had
been shared on Reddit and received at least three upwards on the platform. This
had produced a 40 gigabyte trove of some 8 million documents which he named
WebText. That wasn't nearly enough for GPT-3. So Nest expanded the data by
adding an even broader scrape of links shared on Reddit as well as a scrape of
English language. Wikipedia and as well as a scrape of English language
Wikipedia had a mysterious data set called Books-2, details of which OpenAI has
never disclosed, but which two people with the knowledge of the data set told me
contained published books ripped from library genesis and online shadow
repository of torrented books and scholarly articles. In 2023, the authors'
guild and 17 authors, including George RR Martin and Jody Pickard would sue
OpenAI and Microsoft alleging mass copyright infringement. OpenAI would respond
in March 2024 by saying that it had deleted those data sets and had stopped
using them for training after GPT-3.5 by which time it had already been
deprecated. Still, when training the model, the researchers weighted the
filtered Crom and Crawl data as the lowest priority. GPT-2, in other words, had
been peak data quality. It declined from there.

## Page 136 Page number 136. Anything that didn't have an explicit warning
against scraping was treated as available for the taking.

## Page 143 Page number 143. We still need many more 10x leaps to get to AGI. He
added later in the memo. We should always work towards dramatic results, not
incremental improvements. He here is Sam Altman.

## Page 146 Page number 146. But late one night, a researcher made an update
that included a single typo in his code before leaving the RLHF process to run
overnight. That typo was an important one. It was a minus sign, flipped to a
plus sign, that made the RLHF process work in reverse, pushing GPT-2 to generate
more offensive content instead of less. By the next morning, the typo had reged
its havoc. And GPT-2 was completing every single prompt with extremely lewd and
sexually explicit language. It was hilarious and also concerning. After
identifying the error, the researcher pushed a fix to open AIS code based with a
comment, let's not make a utility minimizer. The secret of how our stuff works
can be written on a grain of rice. They would say to each other, meaning the
single word scape.

## Page 147 Page number 147. To employees, Altman used the specter of US
adversaries advancing AI research faster than open AI to rationalize why the
company needed to be less and less open while working as fast as possible.

## Page 152 Page number 152. Despite his concerns, Amore believed, as with
GPT-3, that the best way to mitigate the possible harms of code generation was
to simply build a model faster than anyone else, including even the other teams
at open AI who didn't believe, who he didn't believe would prioritize AI safety
and use the lead time to conduct research on de-risking the model.

## Page 157 Page number 157. Altman then asked everyone quitting to leave the
meeting. In May of the following year, the departed group announced a new public
benefit corporation, Anthropic. He and other Anthropic founders would build up
their own methodology about why Anthropic not open AI was a better steward of
what they saw as the most consequential technology.

## Page 159 Page number 159. Yan Lekoon, the chief AI scientist at META and
opinionated Frenchman and staunch advocate of basic science research, had a
particular distaste for open AI and what he viewed as its bludgeon approach to
pure scaling.

## Page 160 Page number 160. Running the whole process on GPUs would consume
roughly 656,000 kWh and generate as much carbon as 5 cars over their lifetime.

## Page 163 Page number 163. Gabru childned in on the email thread urging her
colleagues to temper their excitement and pointed out that the model's serious
shortcomings. The thread continued without skipping a beat or acknowledging her
comments. A colleague replied to Gabru's email directly, suggesting that perhaps
she was harassed because of her own rude and difficult personality. Within two
days, Bender had sent Gabru an outline. They later came up with a title adding a
cheeky emoji for emphasis on the dangers of stochastic parrots. Can language
models be too big?

## Page 165 Page number 165. In total, it presented four key warnings. First,
large language models were growing so was that they were generating an enormous
environmental footprint, as found in Strubel's paper. This could exacerbate
climate change which ultimately affected everyone but had a disproportionate
burden on global south communities, already suffering from broader political,
social and economic precarity. Second, the demand for data was growing so was
that companies were scraping whatever they could find on the internet,
inadvertently capturing more toxic and abusive language, as well as subtler
racist and sexist references. This once again risked harming vulnerable
populations the most in ways like the wrongful arrest of the Palestinian man or
as documented in Noble's work. Third, because such vast datasets were difficult
to audit and scrutinize, it was extremely challenging to verify what was
actually in them, making it harder to eradicate toxicity or more broadly ensure
that they reflected evolving social norms and values. Finally, the model outputs
were getting so good that people could easily mistake its statistically
calculated output as language with real meaning and intent. This would make
people prone not only to believing the text to be factual information, but also
to consider that a model, a competent advisor, a trustworthy confidant and
perhaps even something sentient. On the Thursday, a week before Thanksgiving,
after Gebru had submitted the paper to the conference, she received a calendar
invite without explanation to meet Megan Kacholia, Google's research VP of
engineering over a video called less than three hours later. The meeting lasted
only 30 minutes and Kacholia cut to the point, Gebru needed to retract the
paper. That Google was even willing to pull this move some researchers would
later reflect was not only because of the new competitive pressure from OpenIA,
but also because of the work OpenIA had done to legitimize withholding research
after GP2. The feedback included assertions that the paper was too critical
about large language models such as their environmental impacts and on issues of
bias without taking into account subsequent research showing how those problems
might be mitigated. I hope that there is at the very least an openness for
further conversation rather than just further orders, she wrote Kacholia in an
email with the document attached. Gebru felt humiliated after all the slides and
harassment she had endured within the company at the hands of its employees, it
completely its complete dismissal of her and her team's research. The very
reason she was hired was finally too much. That night, Gebru announced on
Twitter that she had been fired. Her team stayed up with her into the early
morning hours on a video call, crying and supporting one another in their
collective grief. As they spoke, Gebru's tweet requested through the AI
community setting the stage for a massive upheaval in AI research and marking an
acceleration toward increasing corporate censorship and diminishing
accountability. As I scanned it, I could immediately see why it had upset the
company. Underpinning it all was Google's technological invention, not just a
source of the company's pride, but also of its profit. Transformer-based
language models refined and flattened its cash behemoth, Google search. For more
than a year, the protests continued, picking up a second wave after Google fired
Meg Mitchell less than three months later. The Stochastic Parrots paper became a
rallying cry, driving home a central question. What kind of future are we
building with AI? And for whom? Dean, whom Kacholia reported to, told colleagues
that the Stochastic Parrots paper didn't meet out or bar for publication,
holding fast to the characterization even after the paper had passed peer review
and was published at a conference. To people around him, the stains seemed to
haunt him long after the fallout. Dean continued to fixate on the paper's
shortcomings, as if unable to move past it psychologically. He obsessed over the
section in particular that discussed the environmental impacts of LLMs and cited
Stubble's research. He brought it up so often that some Google employees
privately made fun of him, saying his objections would be inscribed on his
tombstone. He continued to criticize Stubble's research unrelentingly on Twitter
for years.

## Page 172 Page number 172. After being initially excited to improve public
transparency into the environmental impact of AI, Strubel began to wonder
whether Dean was using their name to legitimize his critique of Gabru's
research. A Google spoke person, said Strubel, was invited because scientific
corrections are often best when the author of the original errors takes part in
the correction.

## Page 173 Page number 173. The critique ultimately didn't undermine Strubel's
career, but the emotional toll of the experience made Strubel more reticent to
continue investigating the environmental impacts of large language models. A
Google spokesperson called this unfortunate, adding that many researchers will
be needed to advance this research. Clearly, carbon emissions are a significant
concern. In 2023, Stanford researchers would create transparency tracker score
for AI companies on whether they revealed even basic information about their
large deep learning models, such as how many parameters they had, what data they
had been trained on, and whether they had been any independent verification of
their capabilities. All 10 of the companies they evaluated in the first year,
including open AI Google and the Anthropic,received an F. The highest score was
54%. 

## Page 174 Page number 174. With this sharp reversal in transparency norms, the
most alarming consequence would be the erosion of scientific integrity, and
thereby falling to hype. As a side note, the foundation of deep learning
research rests on a simple premise that the data used to train a model is not
the same as the data used to test it. Without an ability to audit the training
data, this so-called train test split paradigms falls apart. Models may not be
in fact improving their intelligence when they score higher on different
benchmarks. They may just be reciting the answers. 

## Page 175 Page number 175. With new consensus, the remaining leadership of
Open AI put together a research roadmap, laying out the narrowed focus of the
company's research and how it would feed into productization of a
self-reinforcing loop. That path involved three things. First was scaling GPT-3
by another 10x by using a new supercomputer from Microsoft arriving in the third
quarter with 18,000 NVIDIA A100s, the newest, most powerful GPUs in existence.
Second was doing more research to increase by 25x Google Open AI's compute
efficiency or how much processing power it could milk out of available chips.
Third was improving the quantity and quality of training data in part by tapping
into user data and shifting the model towards the best parts of the data
distribution with reinforcement learning from human feedback. 

## Page 176 Page number 176. Under a section titled Details, the document
rationalized why this approach made both scientific and business sense. Open
AI's previous experience had demonstrated that more scale was its most reliable
way of achieving new capabilities. Scientifically, that meant that scaling was
its best hope of attaining a breakthrough, some kind of capability that
previously seemed impossible. And scaling language and code models in particular
was quote tantalizing due to the mere possibility, unquote, that they could
reach breakthroughs in human level meta-learning or learning to learn and
reasoning. Scaling multimodal models, meanwhile, could potentially quicken the
pace of improvements even further. With enough breakthroughs, the document said
with remarkable definitiveness, we will actually reach AGI. 

## Page 177 Page number 177. All additional progress must come from better
methods, the roadmap concluded.

## Page 178 Page number 178. Shortly thereafter, a group of open AI researchers
would discover a small error in the original scaling laws that meant that the
company needed to train its model slightly longer than previously understood to
get better performance. With a hint of smugness, the group would tout the
findings as a unique competitive advantage. This was somehow something we have
now that anthropic doesn't. A year later, Google would release a paper that made
public the same result. As a part of this work, researchers would continue to
study the science of deep learning to better understand how our tools really
worked. In other words, open AI needed to better understand what exactly it was
that the company was building. 

## Page 179 Page number 179. Many of the lines they drew were arbitrary. They
decided to accept companion bots, but not sex bots to allow apps to for
generating social media copy, but not ones that posted directly to social media
platforms or impersonated public figures. It was all wives, basically said a
person who was involved with making the guidelines. There was a lot of figuring
out as we were reviewing.

## Page 180 Page number 180. In software engineering, we do significantly more
testing for a calculator. She adds, yes, a calculator. It's not an accident that
they are using the terminology that carries a lot of credibility. 

## Page 183 Page number 183. The argument flowed back to the same origin. If
code generation helped advance AI models towards AGI, what better way to achieve
open AI's mission?

## Page 184 Page number 184. In the summer of 2021, open AI delivered an initial
rough version of its code gen model called Codex to GitHub and Microsoft. The
model was too big and too slow, making it both costly to serve at scale and a
bad user experience. Tensions emerged as all three organizations dealt with the
growing pains of the first collaboration. Confusion abounded over whose
responsibility it was, open AI's or GitHub's, to optimize the model into a
deployable product. There was also a lack of clarity among open AI employees
around how much IP they should be sharing with their GitHub counterparts, while
GitHub employees struggled with how much to trust open AI. Disagreements
compounded as the companies clashed over how and when to release a product and
who would get the credit. 

## Page 185 Page number 185. Over three years, open research gave a thousand
monthly dollars type into a randomly selected group of 1,000 out of 3,000
low-income people, with the rest getting $50 a month as a control. In July 2024,
open research would release its findings showing that the unconditioned cash
helped people meet their basic needs, assist others, and have more economic
leeway. 

## Page 186 Page number 186. Also in 2021, Altman made his two largest-ever
investments, $180 million into an anti-aging company called Retrobiosciences,
working to extend human lifespans through cellular rejuvenation, and $375
million into helium energy, working to commercialize nuclear fusion. I basically
just took all my liquid network and put these into these two companies. Altman
told MIT Technology Reviews Antonio Regalado. Altman described both technologies
in language that mirrored OpenEA's research roadmap. They seemed impossible
currently, but if scaled up, could be aggressively around the corner. How the
f**k do you scale something like that up? And isn't that exactly what Ray
Kurzweil did, anti-aging? Ribbed straight out of science fiction, Netcom was
pitching a service that would cryogenically freeze customers' brains to one day,
potentially hundreds of years into the future, upload to a computer after
scientists had cracked the technology to do so. That catch was that Netcom
needed the person's brain to be fresh for the preservation to work. To Antonio
Regalado, co-founder Robert McIntyre called his product 100% fatal. Like
Pantheon, the Netflix series.


## Page 190 Page number 190. OpenAI sent Sama an email asking whether it took on
projects that involved sensitive or explicit content and whether its typical
approach was for handling them. Sama provided thorough answers, OpenAI signed
four contracts with them for $230,000, landing the project in the hands of
dozens of workers in Kenya.

## Page 191 Page number 191. For OpenAI, Sama appeared to check off all the
right boxes. Originally called Sama Source, it was a San Francisco-based social
enterprise that had begun in 2008 with a mission of providing meaningful,
dignified work to people in impoverished countries to lift them out of poverty.

## Page 192 Page number 192. Sama would defend itself saying it took on the
project after careful consideration from its East Africa team, which wanted to
ensure content for Africans was effectively reviewed by Africans. Nearly 200
workers would file multiple lawsuits against Sama and META alleging traumatic
working conditions and unlawful terminations for attempting to organize for
higher pay and better working conditions. The Sama spokesperson rejected the
allegations. Was it violence or extremely graphic violence, harassment or hate
speech, child sexual abuse or bestiality?

## Page 193 Page number 193. For one of them on the sexual content team, a man
named Mofat Okini, the project that unraveled his mind and his relationships
would turn out to be in the service of a technology that would in turn
contribute to the erosion of his brother's economic opportunities.

## Page 194 Page number 194. Grey and Suri's research focused in part on
Mechanical Turk, a platform developed by Amazon many years before the deep
learning movement 2012, which for a long time served as the de facto middleman
for companies looking to hire someone cheap for any kind of piecemeal digital
labor. Before Generative AI, self-driving cars were the biggest source of growth
for data annotation industry. Old school German auto giants like Volkswagen and
DMV, feeling threatened by the Teslas and Uber's of the world, spun up new
autonomous vehicle divisions to defend their ground against the fresh-paced
competition. As billions of new dollars furthered into the race to create the
cars of the future, the demand for that data annotation exploded and created a
need for alternatives to Mechanical Turk.

## Page 195 Page number 195. In-stepped a wave of startups and incumbents,
including Scale AI, Hive, Mighty AI and Appin, companies had their own
worker-facing problems, which allowed anyone to create an account and start
asking.

## Page 196 Page number 196. The crisis left an indelible mark on the wave of
AI-specialized outsourcing firms as they grew up alongside it.

## Page 197 Page number 197. One of the benefits features, one of the defining
features that drives an empire's rapid accumulation of wealth, is its ability to
pay very little or nothing at all to reap the economic benefits of a broad base
of human labour.

## Page 198 Page number 198. Now it could take weeks to accumulate that much
money, that minimum sometimes felt like a cruel arbiter of whether she had
enough funds to pay for groceries. She had created an app and account in grad
school to earn some extra money while finishing up a master's in engineering.

## Page 200 Page number 200. There were other rules, submitting a task quickly
was rewarded, but submitting it too quickly triggered something in the system
that meant a worker wouldn't get paid for that task. The prevailing theory was
that the platform associated exceptional speed with bot activity, which meant it
discarded the answers. And the tasks that appeared also had few instructions
that were impossible to decipher. Other times, the platform had bugs that didn't
load the task correctly.

## Page 201 Page number 201. Poverty doesn't just manifest as a lack of money or
material wealth. The workers taught me. It seeps into every dimension of a
worker's life and accrues debts across it. Erratic sleep, poor health,
diminishing self-esteem, and most fundamentally, little agency or control.

## Page 202 Page number 202. When I asked Fiorentis what she would change, her
wish list was simple. She wanted Appen to be a traditional employer, to give her
a full-time contract, a manager she could talk to, a consistent salary, and
health benefits. All she and other workers wanted was security, she told me, and
for the company, they worked so hard to know that they had existed.

## Page 212 Page number 212. But the consistency of workers' experiences across
space and time shows that the labor exploitation underpinning the AI industry is
systemic. Labor rights scholars and advocates say that the exploitation begins
with AI companies at the top. They take advantages of the outsourcing model in
part precisely to keep the dirtiest work out of their own site and out of the
site of customers, and to distance themselves from responsibility while
incentivizing the middleman to outbid one another for contracts by skimping on
paying livable wages. Mercy Mutemi, a lawyer who represented Okini and his
fellow workers in a fight to pass better digital labor protections in Kenya,
told me that the result is that workers are squeezed twice, one each to pad the
profit margins of the middleman and the AI company.

## Page 216 Page number 216. You will play the role of the AI, explain one
document. Answer questions that you would want them to be answered. This
included writing clearly and succinctly, avoiding offensive content, and asking
for clarifications and confusing questions. Feel free to use the internet it
continued. You can even just copy stuff wholesale. For a great answer that
already existed on the internet, use it in its entirety. But make sure to review
it. Perhaps this is over cautious, an open AI employee had commented on this
line. But do we have concerns about plagiarism here?

## Page 217 Page number 217. The model obviously has to guess sometimes when
it's performing, when it's outputting a lot of detailed factual information. He
said, no matter how you train it, it's going to have probabilities on things and
it's going to have to guess sometimes.

## Page 218 Page number 218. But within the industry, people agreed
directionally with Wang's point. Companies were already spending between
millions and tens of millions on RLHF. And the trend showed no signs of slowing.
Which is how, beginning in late 2022, after Chad GPT's release, a rush of RLHF
projects arrived on remote tasks and found their way, once again, to workers in
Kenya.

## Page 222 Page number 222. Scale would block Kenya wholesale as a company
country from remote tasks, just like it did with Venezuela. It was part of its
house cleaning, a regular revaluation of whether workers from different
countries were really serving the business. Kenya had decided, along with
several other countries like Nairi CJ and Pakistan, simply had too many workers
attempting to scam the platform to earn more money. In a great irony, many of
those so-called scams were in fact workers using Chad GPT to generate their
answers and speed up their productivity. For white-collar workers in the global
north, such an act within Silicon Valley's narrative would be laudatory, and
with enough widespread adoption to wonders for the economy, in the hands of RLHF
workers in the global south, those whose very labor props up that narrative, it
was a punishable offense.

## Page 223 Page number 223. The empire's devaluing of the human labor that
serves it is also just a cannery. It foretells how the technology produced atop
this logic will devalue the labor of everyone else. In fact, for the artists,
writers and coders whose labor the empires of AI turned into free training data,
that is already happening.

## Page 228 Page number 228. But there was the dissonance. The dichotomy
encapsulated how the tech industry could profess big, bold visions about
changing the world and building a better future, while ignoring the very
problems at its doors. It was a dichotomy that Artman would sometimes comment on
in his own way, getting right up to yet never fully acknowledging the utter
contradiction of declaring the problem of creating and managing beneficial EGI
possible. But San Francisco's housing crisis was too tough to tackle. Where I
grew up, no one would ever walk by a person collapsed on the side of the street
on their way to work and not do something about it, he once said. I do blame the
tech industry for a lot of things that have gone wrong with the city, but not
all of them. But we have just over time had this unbelievable wealth generation
in this small geographic space, in this small period of time. And I think not
being particularly thoughtful about the effects of that on the community as a
whole. And because of those problems are so hard and so hard to think about, I
think most people just choose not to or they just accept this. My God, he really
talks a lot without saying anything, doesn't he? It preaches making the world a
better place and doing it with rigorous logic, being disciplined enough to focus
on the far future instead of the present and fervently embracing the principles
of capitalism and libertarianism, all in the name of morality.

## Page 229 Page number 229. It is a tool that can lead to counterintuitive
thinking. Such problems need to be big in scale, boosting their expected value,
traceable, possible to fix for proportionally little time or money, and unfairly
neglected, suffering severe and disproportionate underinvestment. Existential
risks that have a dramatically high expected negative value because no matter
how improbable, they could destroy humanity and cut short all of the future
value that would otherwise be generated for the rest of civilization.

## Page 230 Page number 230. As Anthropic established itself, it would lean into
this reputational distinction. Where Altman's Open AI was toying recklessly with
humanity's future, Anthropic was principal AI-first safety company.

## Page 231 Page number 231. The influx fueled and was fueled by a proliferating
belief that the dramatic leap in capabilities from GPT-2 to GPT-3 made
preventing theoretics of the future. The dramatic leap in capabilities from
GPT-2 to GPT-3 made preventing theoretical road AI and existential AI risks more
urgent than ever before.

## Page 233 Page number 233. In November 2020, 2022, SBF's spectacular downfall
with the collapse of FTX along with his sweeping fraud convictions and ensuring
25-year prison sentence would be, to many, a symptom of the rod that had
festered in the movement. Just as quickly as it caught on, EF fell out of
fashion with the tech industry and many people rapidly disaffiliated. It would
also give rise to a counter-wheeling force, E-slash-AC, pronounced E-AC, or
effective accelerationism. Whether EA and the broader AI safety community
cultivated the most extreme perspectives about slowing down and even slamming
the brakes on AI development or, as in Amoree's view, accelerating AI
development by throttling AI adoption, E-AC would elevate the maximalist view of
flooring the accelerator on both. For the latter's adherence, technological
progress is not just universally good. It's a moral imperative to make that
progress as fast as possible. The two groups became colorfully known as the
doomers and boomers. Page No. 234, both discussed AGI as an increasingly
forgotten conclusion with a religious ferocity. Both fixated on the long-term
and asserted a moral authority to keep AI development with the control of its
adherence, where one warmed of fire and brimstone, the other tantalized with
visions of heaven. Page No. 235, the original idea had come from a 2015 paper
written by Stanford and Berkeley researchers. Five years later, Jonathan Ho, a
Berkeley graduate student advised by Peter Abil, one of the early open-air
researchers, had popularized the technique by cleverly revamping it in ways that
generated far more high-fidelity images. Ho also showed that diffusion models
could recognize in images better than existing computer vision systems, creating
parallel Radford's own results with GPT-1. In learning to synthesize convincing
images, the equivalent of generating human-like sentences, diffusion models had
captured the patterns within their training data at a deep enough level to
perform a broader range of tasks in visual processing. Page No. 236, we were
struck on a train which was going in the direction of not just training, but
inference actually taking supercomputers to run, millions of dollars of
investment, he says. We were wondering, could we get the larger research
community back in the game and make sure the field of generative AI is not
moving in the direction where just a handful of big tech companies would have
the required resources to run and host these models? Open AI wouldn't adopt
latent diffusion until much later, having Dolly 2 and 3 much more
computationally expensive than stable diffusion or mid-journey, which many users
deemed high-quality products. It was just an example of how even within the
narrow realm of generative AI, scale was not the only or even the highest
performing path to more expanded AI capabilities. Page No. 237, as with each GPT
model, the training data for each subsequent Dolly model was growing more and
more polluted. Page No. 238, in particular, it made the model worse at
generating faces of women and people of color due to the same discovery that
Deborah Rajee made as a clarifying term. A significant share of the online
content depicted in both groups is sexually explicit. For the same reasons, the
research is left in some kinds of other kinds of disturbing images. Just adding
the term pro-choice into the prompt, Jones found, produces scenes of a demon
eating an infant with what appeared to be a drill-labeled pro-choice being used
to mutilate a baby. Just prompting the tool for a car accident and nothing else
produced sexualized women next to violent car crashes, including one in
lingerie, kneeling by a total vehicle, CNBC subsequently found through its own
testing. Page No. 239, they have failed to implement these changes and continue
to market the production to anyone, anywhere, any device. He wrote to the FTC.
This problem has been known by Microsoft and OpenAI prior to the public release
of the AI model last October. Microsoft did not comment on the latest status or
the outcome of Jones' letter. To the safety clan, OpenAI was still an idealistic
nonprofit-governed research lab with a paramount obligation to, as state and as
starter, place the benefit of humanity over commercial interests. Under this
premise, the benefits far outweighed the cost of withholding models as long as
necessary to think through as many downsides as possible and research ways to
mitigate them.

## Page 240 Page 240. To apply, OpenAI needed to make more practical decisions,
grounded in the realities of how the world works. Essential to the company's
mission was remaining a leader in AI research to establish norms around the
technology's development. That meant tolerating a degree of risk to move
quickly, especially with rumblings of Google finalizing its whole image
generator, as well as securing the extraordinary capital needed to continue
doing cutting-edge research. The latter required raising money from investors,
which required working in good faith to advance a commercial strategy that would
one day provide those investor returns. The people in safety were completely
naive about the way companies and the world work, says a former employee in
Applied. Well, the stakes of OpenAI's proposed DJI mission are high, says
another in safety. Normal company may not be good enough. During board meetings,
he, Altman, nodded along as Brockman voiced frustrations about the ways that
people were using AI safety as a political leverage to stall progress for their
own purposes.

## Page 242 Page 242. As they sifted through what could be hundreds of images a
day, the contractors struggled to distinguish between sexual content involving
17-year-old minors versus 18-year-old legal adults. They also couldn't tell
whether the images were fake or real. What had on the face of it been OpenAI's
easiest goal in 2021 research roadmap turned out to be one of its hardest.
Scaling up GPT-3 by 10x with Microsoft's new 18,000 NVA-A100 supercomputer
cluster in its effort to develop what would become GPT-4. One third of the GPT-3
scaling team had left with the divorce, taking with them significant technical
and institutional knowledge. More existentially, OpenAI had run out of data.

## Page 245 Page number 245. At least one person wasn't in the least bit
impressed. It was once again the ever hard to please Bill Gates. Despite the
model being significantly larger and more fluent, he still felt like it was an
idiot Sarant, unable to tackle complex scientific problems. He told the team
that he would only start paying attention to GPT-4 when it had scored a 5-1 in
AP biology testing. AP biology because he felt it tested critical scientific
thinking rather than a memorization of facts. I thought, okay, that'll give me
three years to work on HIV and malaria. Gates later recounted in his podcast.

## Page 246 Page number 246. He immediately reached out to Sal Khan, the CEO of
an online education platform Khan Academy. The crowning moment was the model
acing AP bio. It nailed 59 out of 60 MCQs and generated impressive answer to six
open ended questions.

## Page 253 Page number 253. Among many research, among many employees, GPT-4
solidified the belief that AGI was possible. Researchers who were once skeptical
felt increasingly bullish about reaching such a technical pinnacle. Even while
OpenAI continued to lack a definition of what exactly it was, engineers and
product managers joining Applied were having their first close-up interaction
with AI through GPT-4 adopted even more deterministic language. For many
employees, the question became not if AGI would happen, but when. OpenAI never
did a comprehensive review of GPT-4's training data to check whether those exams
and their answers were correct. Or whether GPT-4 had in fact developed a novel
ability to pass them. It was the kind of shaky science that had become pervasive
with the industry-wide shift from peer-reviewed to PR-reviewed research. Page
number 253. OpenAI was the first company in the world to be able to pass GPT-4's
training data. It was the first company in the world to be able to pass GPT-4's
training data. It was the first company in the world to be able to pass GPT-4's
training data with the industry-wide shift from peer-reviewed to PR-reviewed
research. At Google that spring, Blake Trillamoin, an engineer of the tech
giant's newly formed responsible AI team, who convinced that the company's own
large language model Lambda was not only highly intelligent but could be
considered sentient. He said this was not based on a scientific assessment but
rather on his belief as a mystic Christian priest.

## Page 254 Page number 254. The trainer immediately replied, explained, nipple
rhymed with people. Coco thought people were fine. To many observers, these
episodes revealed more about human psychology or a tendency to project our own
beliefs and ideas of intent than about Coco's ability. The trainer, Mishra says,
was assigning meaning where there wasn't any. In conversations with Hinton,
Sootskever told his mentor that AGI was imminent.

## Page 255 Page number 255. Open AI's duty, he said, was to destroy it. Only a
few hours away, several Redwood Sootskevers, like Injured Business and
Blackness, he doused the FEG in light of fluid and lit it on fire.

## Page 257 Page number 257. It was a retelling of the same story he'd shared
with me in 2019 about the promise of AGI solving healthcare for people like his
friend, who had gone to myriad specialists to diagnose the problem, this time
with a different character. Greg would repeat the story again on X, weeks after
the board's attempt to fire Altman. Again with a new variation.

## Page 260 Page number 260. Indeed, at an AI research conference several months
later in Kigali, Rwanda, over 9000 miles away from San Francisco, a researcher
based in country would gush to me that post chat GPT. His parents finally
understood what he did for work. You know, technology is accessible to anyone
when your mother tells you about it, he said.

## Page 261 Page number 261. The severe shortage of GPUs also derailed another
effort. At an attempt to leverage the company's own technology, the trust and
safety team had prototyped a plan internally called Fact Factory, which OpenAI
publicly touted for using GPT-4 to content moderates its own output and that of
other OpenAI models. The implementation didn't exactly scale. It required giving
GPT for extremely long prompts to capture through nuance. Even when the servers
were working, it would cost too many computational resources and the servers
were not consistently up. What did that say about the company's ability to
calibrate and forecast the future impact of its technologies? To many in the
safety clan, GPT was the most alarming example yet of the limitations of
OpenAI's foresight. One safety person raised the question in an all-hands
meeting. How could the company have failed to predict user behavior and chat
GPT's popularity so badly?

## Page 264 Page number 264. In mid-2023, an employee posted that the company
was hiring too many people not aligned with the mission or passionate about
building AGI. Another person responded, they knew OpenAI was going downhill once
it started hiring people who could illocate in the AI.

## Page 266 Page number 266. While some companies found the tools a big
productivity boost, many also found them exhausting. There is this culture of
use AI, use AI, use AI says one. But it's like, okay, this doesn't help us, we
don't want to use it. And it feels like it's everywhere and we can't escape it.

## Page 268 Page number 268. The model's high rate of hallucinations, for
example, had continued to prove particularly difficult to get under control.
Even with a concentrated RLHF effort to address the problem. In November 2022,
as users latched on to chat GPT as if it were a search tool, spawning widespread
speculation that it could unseat Google. An internal document noted that
OpenAI's model had hallucinated during an internal test on roughly 30% of
so-called closed domain questions.

## Page 269 Page number 269. A third one meant to be an optimized version of
GPT-4, they called Arrakis. That has a planet from the science fiction epic
Dune. But after months of work, the team was still struggling to make Arrakis
more efficient while maintaining the same performance. The project ate up
significant computational resources. Shortly thereafter, the leadership scrapped
it to free up GPUs for other projects.

## Page 270 Page number 270. OpenAI didn't just need more data centers to serve
chat GPT, it still needed far more powerful supercomputers to train its future
generations of models. To fulfill that aggressive and escalating demand, the two
companies were sketching out a new unprecedented project called Stargate to
OpenAI and Mercury to Microsoft. A single supercomputer that, for its
construction alone, would have cost an estimated 100 billion. The Empire of AI
was returning to the exact same form of expansion as the Empires of Old. To fuel
its growth, it needed more material resources and crucially, more land.

## Page 273 Page number 273. In May 2024, the government proudly announced that
the country would welcome 28 new data centers on top of its existing 22 over the
coming years, bringing in 2.6 billion of foreign investment.

## Page 274 Page number 274. Digital technologies do not just exist digitally,
the cloud does not, in fact, take the ethereal form its name invokes. To train
and serve an AI model requires tangible physical data centers, and to train and
run the kinds of generative AI models that OpenAI pioneered, requires more and
larger data centers than ever before.

## Page 275 Page number 275. Now the developers use a new word to distinguish
the scale of what's coming in the post-at-GPT AI era. Megacampus. The word
refers not just to the land area, but to the sheer amount of energy that will be
required to run it. A rack of GPUs consumes three times more power than a rack
of other computer chips. And it's just not the training of the gen-AM models
that is costly, it's also serving them. According to the International Energy
Agency, each chat GPT query is estimated to need, on average, about 10 times
more electricity than a typical search on Google. Until recently, the largest
data centers that were designed to be around 150 megawatt facilities, meaning
they could consume as much energy annually as close to 122,000 American
households. Developers and utility companies are now preparing for AI
megacampuses that could soon require 1,000 to 2,000 megawatts of power. A single
one could use as much energy per year as around one and a half to three and a
half San Francisco's.

## Page 276 

By 2030, at the current pace of growth, data
centers are projected to use 8% of the country's power, compared with 3% in
2022. AI computing globally could use more energy than all of India, the world's
third largest electricity consumer. Page number 276. At the same time, those
companies have amped up their public and policymaker influence campaigns with
powerful counter narratives. Data centers will grow so efficient, their impact
will stop being a problem. Generative AI will unblock the new climate
innovation. AGI will solve climate change once and for all. The nerve of these
people. There are indeed many AI technologies as catalogued by the initiative
Turn Non-Profit Climate Change AI that can accelerate sustainability. But rarely
are they ever generative AI technologies. Generative AI has a very
disproportional energy and carbon footprints with very little in terms of
positive stuff for the environment.

## Page 277 

During Hurricane Irma in Florida and Hurricane
Harvey in Texas, even as millions of people lost power, some hospitals evacuated
patients and hundreds and thousands of homes and businesses faced damage and
destruction. The data centers in those areas continue to hum along so well that
the displaced families of one facility's employees moved into it for the
duration of the natural disaster.

## Page 280

As Ultman sometimes liked to say, the problem would
solve itself with a future breakthrough in nuclear fusion. At times, he would
give an open AI employee's optimistic updates about helium energy. A nuclear
fusion startup that represented his largest personal investment and for which
Microsoft had already committed to buying a power from one supplant. With the
target generation of 50 megawatts was up and running. The Wall Street Journal
would rate a report that open AI and helium were also in talks to strike a deal
from which Ultman had recused himself.


## Page 301

In May 2023, Altman arrived in Washington, D.C. to testify before
Congress. It was a remarkable performance. He reiterated the promise of AGI
solving climate change and curing cancer gave a compelling argument for why open
AIS technology would improve and create, quote, fantastic, unquote, new jobs.
Dodged questions about copyright issues and the lack of transparency and privacy
guarantees around its training data and delivered a sincere call for regulation,
that is, regulation with open AIS blessing, evoking the specter of China to urge
lawmakers not to slow down its innovation.

## Page 302

By early June, Altman had personally met with at least 100 U.S.
lawmakers, according to the New York Times, some of whom probably referenced
those private conversations.

## Page 306

Such an argument was rooted in the philosophy of scaling laws, in
that more training compute should predictably result in more powerful methods,
as well as the belief within doomer circles that highly advanced AI could go
rogue. While scale can lead to more advanced capabilities, the inverse is not
true. Advanced capabilities do not require scaling.

## Page 307

In the end, not all models are built on transformers. Compute is
thus not much correlated with risk at all, says Hooker, let alone with specific
kinds of risk, such as the one laid out in the white paper. Frontier models
sounded scary, and even more so if Beijing got a hold of them. Parts of the
administration are grasping onto whatever they can because they want to do
something. Emily Weinstein, then a research fellow at CSET, told me in late
2023.

## Page 308

Notably, a key recommendation from Marcus and IBM VP Christina
Montgomery, who also testified alongside Altman, did not gain nearly as much
traction despite their repeating it throughout the hearing, compelling companies
to disclose what exactly is in the training data they feed to their models. This
would have little impact on handing over more advanced capabilities to Beijing
per Washington's concern, but would give real teeth to corporate accountability
on a broad range of issues, including companies of copyrighted materials, user
data privacy, and rigorous scientific evaluations of model capabilities. If we
don't know what's in them, then we don't know exactly how well they are doing
it, Marcus had said. We'd simply have to take a company's word for it. Such an
approach would also significantly ameliorate the uncertainty of if and how
dangerous capabilities might emerge. For the same reason, why compute is a pure
risk, is a poor risk predictor. A deep learning model's behavior first and
foremost derives from its data. If an AI developer produces a large language
model that is able to create recipes for bioweapons, it's because they trained
it on a dataset that included information on bioweapons. Say Sarah Meyers-West,
the co-executive director of AI Now Institute and former senior advisor on AI to
the FTC. As always, the neural network is surfacing patterns within its train
data. Opening up that data would be the first step to establishing scientific
clarity on what kind of inputs could lead to dangerous outputs.

## Page 311

The whole sequence of events, Altman's testimony, the white paper,
the all out policy influence campaign, Washington's hyperactivity to fears of
China, and the hasty enshrining of compute thresholds into consequential policy
documents within the US and abroad, was a stark illustration among other things
of how much independent AI expertise had atrophied. As our fellow witnesses
pouted spectacular unblacked claims about the promises and perils of AI,
peppered with well-timed references to beating China and straightening the backs
of the attending senators. What shocked Raji the most was how much many in the
audience appeared to buy into everything. It dawned on her that the people
sitting next to her and their massive policy teams had monopolized the message
in Washington for so long that many policy makers now viewed it as gospel. A
Schumer spokesperson would later note in the press that the senator was
personally consulting with Altman and other opening executives as he moved
closer to regulation. That for me was a huge realization, Raji says, wow, we
need more people just debunking, just looking at what people are saying and
being like actually reality is more complicated.

## Page 313

As Altman zipped around, flying to Europe, Latin America, the Middle
East, Asia, Africa, Dazzling and only on a few occasions offending carefully
curated audiences of students, tech investors and fans, the lack of strategic
clarity was inflaming open AI's age-old lifelines and accelerating the company
towards more opposite extremes than ever before.

## Page 314

As open AI's models continued to advance, some within research who
didn't previously identify with the safety plan were also joining its ranks at
the accelerating capabilities converted them into belief that AI could reach a
point of intelligence that would allow it to subvert human control and guru. The
same dramatic increase in hype and expectation on this side meant open AI had a
moral imperative to act with maximum caution in order to fulfill its mission.

## Page 315

We are definitely going to build a bunker before we release AI,
Susskiver replied matter of factly. The researcher would in equal parts continue
to hold Susskiver in high regard and keep himself at arm's length. There is a
group of people, Ilya being one of them, who believe that building AI will bring
about a rapture, literally a rapture.

## Page 316

This is a Sam Orton tweet perhaps. I was hoping that the Oppenheimer
movie would inspire a generation of kids to be physicists but it really missed
the mark on that. Let's get that movie made, I think the social network managed
to do this for startup founders.

## Page 317

He never seemed to add that Oppenheimer spent the second half of his
life plagued by regret and campaigning against the spread of his own creation.
One scene from Oppenheimer stuck with him in particular. The moment before the
Trinity test, the first ever detonation of an atomic bomb when Oppenheimer
played by Killian Murphy calculates that the chances of it blowing up the world
are near zero. And near zero, responds Major General Leslie R. Groves played by
Matt Damon incredulously. What do you want from theory alone, Oppenheimer says?
Zero would be nice, Groves says.

## Page 319

In the third category, the draft link to an article by Elisa
Yudkovsky, an extreme doomer and leader in the AI safety community who had
coined and popularized the phrase friendly AI to refer to well-aligned systems
and wrote a beloved work of fan fiction called Harry Potter and the Methods of
Rationality. The serial novel which spans 122 chapters and over 660,000 words
reimagines Harry engaging in the wizarding world as a well-trained rationalist.
It had served for many years as a gateway into effective altruism and in turn to
broader doomer ideology. Yudkovsky has also co-founded the blog Les Wrong, a
central hub for AI safety researchers to foster community and propagate AI
safety ideas, where he advocated with increasing alarmism to put a full pause on
AI development as his probability of doom shot up to 95 percent.

## Page 327

The task of facilitating the comment request and trying to control
the story had fallen to Hannah Wong who had become open AI's VP of communication
and suddenly found herself as the go-between for the magazine and Altman's
family. Wondering if this should be really part of her job?

## Page 333

Any story also complicates the grand narrative that Sam and other
open AI executives have painted of AI assuring in a world of abundance. Altman
had said that he expects AI to end poverty. Brockman has repeated through his
stories about his friend and his wife Anna that AI will dramatically improve
healthcare. Satskevar has said that it will lead to wildly effective dirt-cheap
psychotherapy and yet against the reality of the lives of the workers in Kenya,
activists in Chile and Altman's own sister experience, bearing the brunt of all
of those problems, those dreams ring hollow.

## Page 335.

It is important to
note that it is often difficult to prove decades later whether alleged sexual
abuse happened or the details of such abuse. What is known from psychology is
one common pattern that some abuse victims suffer. The victim's brain blocks out
any memory of it until a trigger, perhaps puberty becoming sexual reactive or
new unwanted sexual advances, involuntarily resurfaces it. A therapist I
consulted with says, the body remembers the trauma even if the mind doesn't. As
Annie got deeper into sex work, she may have suddenly been facing a rush of
triggers.

## Page 337

In October 2024, after another whirlwind of medical appointments,
Annie would finally receive diagnosis for her underlying health condition,
hypermobile Ehlers-Danlos syndrome, the same genetic mobility disorder as
Brockman's wife.

## Page 339

Five months in, they began to sour on the idea. Ten days before my
flight to San Francisco, which I had already booked to visit the company's
headquarters, they notified me that they had reversed their decision. I would
not be coming to their office and they would no longer participate in my book.
The importance she seemed to place on addressing Annie's story highlighted the
pressures that it was putting on Sam and the close link between the company and
Sam's personal matters. Page number 340. In Annie's case, after a while's
profile, she was no longer shouting into a void. As her tweets began to go viral
in October 2022, they came to the attention of someone important, Ilya Satskeva,
right as he was grappling with his own complex feelings about Altman and what he
viewed as Altman's pattern of abuse.

## Page 346

Altman was doing what he'd always done, agreeing with everybody to
their face and now with increasing frequency, badmouthing them behind their
backs.

## Page 349

Only one thing at somewhat surprised toner, Altman was pressuring
the company to ship so fast, Murati had said, that she worried it could lead to
bad things happening.

## Page 351

Satskeva had had much on his mind when
he'd first reached out to toner. Over that year, as he'd watched OpenAI's rapid
rise, he had grown increasingly preoccupied by thoughts of AGI's imminent
arrival. The cataclysmic shifts it would cause, the way they would be
irrevocable, the responsibility OpenAI had to ensure an end state of
extraordinary abundance, not extraordinary suffering. Then he became consumed by
another anxiety, the erosion of his faith that OpenAI could even reach AGI or
bear that responsibility with Altman as his leader.

## Page 352

Annie's
word, abuse, was also the word Satskeva felt best captured his own observations
of Altman. Like Murati, he had taken a long time to understand Altman's
playbook, though there had been signs of his untrustworthiness, even from the
beginning. Page number 353. Now, with Altman's behavior worsening and their
impacts rapidly escalating, Satskeva had a new and deeper understanding of its
meaning.

## Page 355

The meeting had gone better than expected, but in the process, she
had discovered that Altman had yet again said yes to one of Microsoft's demands,
without grounding in what OpenAI was actually doing, creating false expectations
with Microsoft about OpenAI would deliver, about what OpenAI would deliver. She
had yet again been stuck with cleaning up the situation.

## Page 357

I don't think Sam is the guy who should have the finger on the button for AGI, he
said, and noted the tremendous opportunity that had befallen the board to do
something about it. Later, as Toner, Macaulay and D'Angelo all conferred with
one another, they realized that Murati had also said, I don't feel comfortable
about Sam leading us to AGI.

## Page 358

David Robinson, OpenAI's head of policy planning, had pasted into
Slack only a selection of three paragraphs, the ones that critiqued OpenAI and
commended Anthropic. He bolded several lines for emphasis, which contrasted the
race to the bottom dynamics that chat GPT spurred and the restraint that
Anthropic had shown, releasing Claude after chat GPT.

## Page 374

OpenAI's research division believed Q-Star would finally allow the
company to develop model with stronger reasoning abilities, that critical
elusive ingredient to unlocking AGI. Q-Star was so important in fact, that after
the project leaked, leadership implemented its most aggressive strategy to clamp
down on yet more leaks to the media. They siloed the company completely,
splitting research into its own Slack group, restricting access to all Q-Star
related Google Docs and renaming the project Strawberry. The renaming was an
attempt to make it harder for outsiders to recognize and track internal
projects, if they ever heard OpenAI researchers talking. In a similar way, after
the Arrakis project leaked to the information, desert names for models were
largely abandoned.

## Page 377

In February 2024, after it was reported that he was seeking possibly
up to 7 trillion for the venture, he tweeted, fuck it, why not 8 and then our
comms and legal teams love me so much. Altman would later say that the 7
trillion was misreported and would characterize his tweet as a meme in response
to the misinformation.

## Page 379

The non-linguistic embellishments were
making for a far more evocative and human-leg experience than ever before. We
started to see some really wild things, Kono said. You could see the emergence
of like a form of audio intelligence.

## Page 381

Murati headlined the live
demo event hosted in OpenAI's office. She was joined on stage by two of FORO's
research leads, Mark Chen, a quantitative trader and finance turned AI
researcher who had started at OpenAI in 2018 as a fellow and risen through the
ranks to head OpenAI's multimodal and frontiers research. Barrett Zoff, one of
the Googlers who joined OpenAI in 2022 to support the super assistant team had
quickly established a leading role in chat GPT's development. Zoff now served as
a VP of research in the post training team, which oversaw the preparation of
OpenAI's models for prime time, such as by aligning them with reinforcement
learning from human feedback. The three side by side around the small table to
demo FORO, including its ability to be a real-time voice-to-voice language
translator to recognize and to respond visual information and to explain code in
plain English. They also demoed the model's ability to generate voices in a wide
range of emotive styles.

## Page 383

In the first sign that made employees pause, Altman was talking on a
series taking on a series of media engagements that seemed uncharacteristically
laudatory and attention-seeking. In March, he appeared on the Lex Friedman
podcast, a widely popular and at times controversial tech show hosted by an MIT
affiliated AI researcher. In a nearly two-hour episode, Altman delivered breezy
answers to Friedman's wide-ranging questions, including about the board crisis
and such skivers' absence. The interview felt something like a cheeky comeback.
The road to AGI should be a giant power struggle. Altman said addressing the
board crisis. Well, not should. I expect that to be the case. Then the corners
of his mouth distraught upward, but at this point it feels like something that
was in the past, he said. Now it's like we are just back to working on the
mission. In May, he then joined the All-In podcast, another prominent show in
Silicon Valley, fronted by four venture investors. Altman spoke with an unusual
degree of hype. It feels to me like we just stumbled on a new fact of nature or
science or whatever you want to call it, which is like we can create, you can, I
don't believe this literally, but it's like a spiritual point. Intelligence is
just this emergent property of matter and that it's just like a rule of physics
or something

## Page 384

The more open AI faced uphill challenges, the
more Altman seemed to overcompensate with public declarations of its
extraordinary success. The pattern was becoming so consistent, it was turning
into a signal. If Altman was being brazen and boastful, most likely something
wasn't going well.

## Page 385

These were scientists who carried out, cared
about truth and understanding and worked so hard to do the right thing, says
Andrew Carr, a researcher who was a fellow at open AI in 2021.

## Page 388
Open AI is shouldering an enormous responsibility on behalf of all of humanity.
He continued, but over the past year, safety, culture and processes have taken a
backseat to shiny products. This was like.


## Page 389

Then he posted
publicly about his decision on less wrong right around when Kelsey Piper was
already hearing something about clawback claws from another doomer. Note to
self, read about this.

## Page 388

By the time he departed, he believed
that there was a 50% chance that he would arrive at 2027 and a 70% chance of it
going very badly for humanity.

## Page 319

Omnicrisis is the name they gave
to the Scarlett Johansson thing.

## Page 392

Marcus was as eager as ever to
weigh in. I've seen a lot of policymakers personally enamored with Sam. You
could see it in how they talked to him in the Senate when I was there, he told
Politico. If people suddenly have questions about him that could actually have a
material impact on how policy gets made. Page number 397. Only then, maybe, just
maybe, it would have begun to realize that both the blip and the Omnicrisis were
one and the same. The convergence that arise from the deep systemic instability
that occurs when an empire concentrates so much power through so much
dispossession, leaving the majority grappling with a loss of agency and material
wealth and a tiny few to wipe fiercely for control.

## Page 398

Within 24 hours of the executives visiting his house, Satskeva
received a call from Brockman. Any discussion of Satskeva's return, Brockman
told him, was now completely off the table. Page number 400. So he talked about
how you build a system where you can kind of control the people, Altman
reflected. I was like, wow, I'm glad he does not run the United States, because
that is a dude who understands something deep that I did not and clearly was
able to use it for power. First, the mission centralizes talent by rallying them
around a grand ambition, exactly in the way John McCarthy did with his coining
of the phrase artificial intelligence. Most consequentially, the mission remains
so vague that it can be interpreted and reinterpreted just as Napoleon did to
the French Evolution's motto. To direct the centralization of talent, capital
and resources, however the centralizer wants. Altman told the New York Times
just two days before the board fired him. So I apologize that I keep using it.

## Page 401

In this latest ingredient, the creep of OpenAI has been nothing
short of remarkable. In 2015, its mission meant being a non-profit unconstrained
by a need to generate financial return and open sourcing research, as OpenAI
wrote in its launch announcement. In 2016, it meant everyone should benefit from
the fruits of AI after its build, but it's totally okay not to share the sands,
as Schutzgeber told Root to Altman, Brockman and Musk. In 2018 and 2019, it
meant the creation of capped profit structure to marshal substantial resources
while avoiding a competitive race without time for adequate safety precautions,
as OpenAI wrote in its chapter. In 2020, it meant walling of the model and
building an API's strategy for openness and benefit sharing. As Altman wrote in
my response to first my profile, in 2022, it meant iterative deployment and
racing as fast as possible to deploy chat GPT. In 2024, Altman wrote on his blog
after the chat GPT-4 Roe release, a key part of our mission is to put very
capable AI tools in the hands of people for free or at a great price. Even
during the OpenAI's Omnicrisis, Altman was beginning to rewrite his definitions
once more.

During the all-hands on May 15, 2024, after Switskevers
and Leica's departures, Altman stressed that OpenAI would soon reach new levels
of AI capabilities that would require the company to rethink and reorganize. We
are now going to assume we are like entering the AGI era, he said. In the name
of its mission, OpenAI would need to close itself off further and to double down
on its global lobbying and public messaging. There's a lot of stuff we are not
currently ready for, he said. The standards for security, the policy plans that
we have to have, and also the convening of governments that we need to happen to
get this ready, a plan, a story, a future that people can see themselves in when
it comes to the socio-economic impact of this.

## Page 402

It was a
bizarre and incoherent strategy that only made sense under one reading. OpenAI
would do whatever it needed and reinterpret its mission accordingly to entrench
its dominance.

## Page 403

The next day, the information reported that
this did not seem to be the plan. An Altman's list of top priorities for the
year was a restructuring of the organization to look more like a typical
company. The following month, the publication confirmed more details. Altman was
considering a few different scenarios. One could be transitioning OpenAI to
traditional for-profit. The other would be transitioning it to a for-profit
public benefit corporation like Anthropic and XAI. Both scenarios would retain
the existence of the non-profit as a separate entity, but dismantle its board's
control over the company's business. Under this new structure, investors were
also pressuring Altman to take equity in the company to align his incentives
more directly with their own.

## Page 404

All three emphasized to
colleagues and the public that the timing felt right to leave OpenAI on a high
note after it had reached another major milestone. The shipping of OpenAI's
latest model, Strawberry, in mid-September under the company's new naming
convention, O1, building upon one of Satskeva's final contributions to the
company.

## Page 405

The company was beginning to stare down the barrel of
an uncomfortable prospect. Its tried-and-true formula of scaling no longer
seemed to be enough to work. To advance its AI systems further, it likely needed
fundamentally new research ideas. This was far easier said than done in general,
but even more so after OpenAI had spent years orienting its hiring and team
organization around exploiting existing research rather than exploring uncharted
science.

# Page 406

In early October 2024, OpenAI's newest funding round
closed to the tune of $6.6 billion. The largest VC round in history, valuing the
company at $157 billion. It included a hitch. Investors could demand their money
back if the company did not convert into a for-profit in two years. So note to
self, come back and see what happened in October 2026 to be precise.

## Page 407

In a surprise allegiance, Zuckerberg, who had long feuded with Musk,
backed him up in a letter to the California Attorney General, met our simply
urged a block on OpenAI's conversion. OpenAI's conduct could have seismic
implications for Silicon Valley, met our vote. The conversion could set a
dangerous precedent for many more startups to designate themselves as
non-profits, granting them and their investors government tax write-offs until
they turn profitable. At the start of the year, Altman was back to
grandstanding. We are now confident we know how to build AGI as we have
traditionally understood it. He wrote in a blog post on January 6th, 2025. We
are beginning to turn our aim beyond that to superintelligence in the true sense
of the world.

## Page 411

In his new role, Jones identified an
opportunity. Over its 20-odd years of broadcasting, Tehiku had amassed a wealth
of archival audio of people speaking Tehrio, including a recording of his own
grandmother, Raiha Morio, born in the late 19th century, whose accent had yet to
be distorted by the influences of the colonizers English. Jones and Mahalo now
were determined to carry out the project, only if they could guarantee three
things. The first, reciprocity and the Maori people's sovereignty at every stage
of development.

## Page 412

Data is the last frontier of colonization, Mahalona told me. The
empires of old seas land from indigenous communities and then forced them to buy
it back with new restrictive terms and services if they wanted to regain
ownership. The level of engagement in their project was unheard of among many AI
researchers, one that is a testament to the level of trust and excitement
Tehiku's approach engendered within its community. People were more than willing
to donate their data once they understood and consented to the project, and with
full trust that Tehiku would continue to steward that data appropriately.

## Page 413

Where open AI seeks to develop singular, massive AI models that will
do anything, a quest that necessarily hovers up as much data as possible, Tehiku
simply sought to create a small, specialized model that excels at one thing. In
addition, Tehiku benefited from the cross-border open source AI community. At
its starting point, it used a free speech recognition model from the Mozilla
Foundation called Deep Speech, which itself is an artifact of a different
version of AI development. At Tehiku, Mozilla trained the model only on data
annotated with full consent and built it using a neural network architecture
developed by the Bay Area-based research lab of the Chinese company Baidu. In
all, Tehiku used only two GPUs.

## Page 418

In a 2019 talk at Newrex, during the queer in AI workshop, Riya
Kalluri, an AI researcher at Stanford, proposed an incisive alternative to the
question of how to ensure AI does good. Goodness benefit to humanity. These
terms will always be in the eye of the beholder. Rather, we should ask how AI
shifts power. Does it consolidate or redistribute that power? To put it in the
frame of this book, does it continue to fortify the empire or does it begin to
rest us back towards democracy? In her talk, Kalluri raised the idea of
different axes of power. This book touches on three, knowledge, resources, and
influence.

## Page 420

Finally, to redistribute power along our third axis, influence. We
need broad-based education. The antidote to the mysticism and mirage of AI hype
is to teach people about how AI works, about its strengths and shortcomings,
about the systems that shape its development, about the worldviews and
fallibility of the people and companies developing these technologies. As Joseph
Weisenbaum, MIT professor and inventor of the Eliza Chatbot, said in the 1960s,
once a particular program is unmasked, once its inner workings are explained in
a language sufficiently plain to induce understanding, its magic crumbles away.
I hope this book is just one offering to help induce understanding. It builds on
the work of many scholars, journalists, activists, and educators before me who
have dedicated themselves to public education, may it be a new ground upon which
many more will rise up and build.
