# ðŸ“˜ Notes by Page Number

p15
> Nothing about this (ChatGPT, etc) form of AI coming to the fore or even
> existing at all was inevitable; it was the culimation of thousands of
> subjective choices, made by the people who had the power to be in the decision
> making room.




---

## **Page 16**

* Rarely have they seen any trickle-down gains of the so-called technological revolution. The benefits of generative AI mostly accrue upward.
* They projected racist, dehumanizing ideas of their own superiority and modernity to justify and even entice the conquered into accepting the invasion of sovereignty, the theft, and the subjugation.

---

## **Page 17**

* So too do the new employers exploit the labor of people globally to clean, tabulate, and prepare the data for spinning into lucrative AI technologies.
* Around the time Microsoft invested $10 billion in OpenAI, it laid off 10,000 workers to cut costs.
* The current AI paradigm is also choking off alternative paths to AI development.

---

## **Self Note**

* The apocalypse might come before AGI.

---

## **Page 18**

* At the same time, more and more doubts have risen about the true economic value of generative AI. In June 2024, a Goldman Sachs report noted spending on the technology's development was projected to hit 1 trillion dollars in a few years with so far, quote, little to show for it, unquote. The following month, a survey from the Upwork Research Institute of 2,500 workers globally found that while 96% of C-suite leaders expected generative AI to boost productivity, 77% of the employees actually using the tools reported them instead, adding to their workload. This was in part due to the amount of time spent reviewing AI-generated content, in part due to growing demands from superiors to do more work. In a November Bloomberg article reviewing the financial tally of generative AI impacts, staff writers Parmi Olson and Carolyn Silverman summarized it succinctly. The data raises an uncomfortable prospect that this supposedly revolutionary technology might never deliver on its promise of broad economic transformation, but instead just concentrate more wealth at the top.
* Meanwhile, the rest of the world is beginning to collapse under the weight of the exploding human and material costs of this new era. Workers in Kenya earn starvation wages to filter out violence and hate speech from OpenAI's technology, including ChatGPT. Artists are being replaced by the very AI models that were built from their work without their consent or compensation. The journalism industry is atrophying as generative AI technologies spawn heightened volumes of misinformation. Before our eyes, we are seeing an ancient story repeat itself - and this is only the beginning.

---

## **Page 19**

* To quell the rising concerns about Generative AI's present-day performance, Altman has trumpeted the future benefits of AGI even louder. In a September 2024 blog post, he declared that the intelligence age characterized by massive prosperity would soon be upon us, with superintelligence perhaps arriving as soon as in a few thousand days. I believe the future is going to be so bright that no one can do it justice by trying to write about it now, he wrote. Although it will happen incrementally, astounding triumphs, fixing the climate, establishing a space colony, and discovery of all physics will eventually become commonplace. At this point, AGI is largely rhetorical, a fantastical, all-purpose excuse for OpenAI to continue pushing for ever more wealth and power. Few others have the comparable capital to invest in alternative options. OpenAI and its small handful of competitors will have an oligo pulley on the technology they are selling us as the key to the future. Anyone, whether government or company, who wants a piece of that vision will have to rely on the empires to provide it.

---

## **Page 24**

* Later, at a recurring AI salon event at Stanford, a young researcher named Timnit Gebru would come up to him after a talk and ask him why he was so obsessed with AI, when the threat of climate change was clearly more existential. Climate change is bad, but it's not going to kill everyone, he said. AI could render humanity extinct.

---

## **Page 27**

* But on his blog in February 2015, Altman agreed with Musk that superintelligence was, quote, probably the greatest threat to continued existence of humanity, end quote. Even though a devastating engineered virus was more likely to happen, he said that it was, quote, unlikely to destroy every human in the universe, unquote. Incidentally, he wrote in Parenthetical, Nick Bostrom's excellent book Superintelligence is the best writing I've seen on the topic. It is well worth a read.

---

## **Page 28**

* I am now very much in the AI will be a tool camp, he told Business Insider in 2023. Though I do think future humans and human society will be extremely different and we have a chance to be thoughtful about how to design that future.

---

## **Page 32**

* Looped wouldn't become a great success. After a seven-year run, Altman would sell it in 2012 for 43.4 million dollars, around what his investors put in. But if you had listened to his interviews and his backers at the time, his startup would have sounded like it was on the precipice of ushering in a great transformation.

---

## **Page 45**

* It would reveal just how much the quest for dominance of that technology, already restructuring society and terraforming our Earth, ultimately rests on the polarized values, clashing egos, and messy humanity of a small handful of fallible people.

---

## **Page 47**

* In 2012, together with another one of Hinton's grad students, Alex Krizevsky, they shocked the AI world by sweeping the floor at an academic contest called ImageNet to build software for automatically identifying objects in photos.
* The discussion ping-ponged back and forth between academic deliberations about different approaches to AI research and Musk's particular fixation on whether there was still time to beat out DeepMind and Google, essential, they believed, to correcting the course of AI development.
* AGI was also central to the discussion, which at the time was highly unusual. Most serious scientists considered the idea of digitally replicating true human-level intelligence to be science fiction, or at the very least decades or more away from attainability. Bold declarations that it was within reach enough to invest in it presently was viewed largely as pseudoscience and quackery. But Hassabis had embraced that term to describe the ambitions of DeepMind, despite a belief among his own research staff that this was distasteful, shameless marketing. The Rosewood group equally felt that the same goal, AGI, would best describe their own aspirations if they intended to form a competitor to go toe-to-toe with Hassabis' organization.

---

## **Page 48**

* Even to Sutskiver, who secretly believed AGI was possible and would come to full-throatedly endorse some of the most aggressive predictions about the speed of its creation, the brazen talk at first made him a little squeamish. If other researchers found out that he was openly discussing the pursuit of this objective, he worried he risked losing his credibility within the scientific community. Those concerns did not hold back Brockman, an outsider to the field, and sincere in his belief that AGI, with enough effort and focus, could just be around the corner. That Sutskiver and other researchers were willing to at least privately entertain the feasibility of a new lab could only have strengthened Brockman's confidence. As Altman drove him back from the hotel to San Francisco that night, Brockman told him that he was ready to commit himself to the project.

---

## **Page 49**

* Altman and Musk also had their fair share of recruiting conversations, slowly loosening up researchers resistant to the idea of AGI. AGI might be far away, but what if it's not?
* OpenAI, the anti-Google, would conduct its research for everyone, open-source the science, and be the paragon of transparency.

---

## **Page 50**

* In later correspondence, the group acknowledged that they could walk back their commitments to openness once the narrative had served its purpose and as the need arose, such as to avoid bad actors getting their hands on the technology. As we get closer to building AI, it will make sense to start being less open. Satskever raced to the trio in January 2016, shortly after OpenAI launched. The open in OpenAI means that everyone should benefit from the fruits of AI after it's built, but it's totally okay to not share the science. Yup, Musk responded.

---

## **Page 51**

* We are outmanned and outgunned by a ridiculous margin of organizations you know well. But we have right on our side, and that counts for a lot. I like the odds.
* An accounting of the societal impacts of commercializing AI research returned an unsettling scorecard. Automated software being sold to the police, mortgage brokers, and credit lenders were entrenching racial, gender, and class discrimination. Algorithms running Facebook's newsfeed and YouTube's recommendation systems had likely polarized the public, fueled misinformation and extremism, enabled election interference, and most horrifying in the case of Facebook, precipitated ethnic cleansing in Myanmar.
* It became a cynical refrain among AI researchers, sell out to big tech or to the military-industrial complex, or leave AI research. Between these binary extremes, OpenAI seemed like a third way, corrupted by neither profit nor state power. It was a beacon of hope, said Chip Huyen, a machine learning engineer and popular tech blogger observing from the sidelines.
* Here was a group of people, 9 out of 11 of whom were white men, being showered in previously unheard of amounts of money, speaking about the theoretical prospect of a bad superintelligence taking over the world, and proposing to counteract it by building a better superintelligence. That night, Gebru drafted a scathing critique of what she'd observed in an anonymous open letter. The spectacle, the cult-like exaltation of AI celebrities, and most of all, the overwhelming homogeneity of people building and shaping such a consequential technology. This homogeneous culture was not only pushing away talented researchers, but also leading to a dangerously narrow conception of AI, and of who could benefit from the technology.

---

## **Page 54**

* He created a company policy requiring all employees to work out of the San Francisco office, a policy that OpenAI would hold on to until the pandemic. This of course came with some trade-offs. Not everyone wanted to live in the Bay Area, he acknowledged. I would learn through my other interviews that this was particularly true for women and people of color who, like Gabru, felt alienated by the white and male culture of the dominant tech industry. But to Brockman, cohesion was more important and being physically together helped with the serendipitous exchange of ideas.

---

## **Page 55**

* When considering the criticisms levelled at OpenAI for its pursuit of AGI, he drew parallels with Edison's lightbulb. A committee of distinguished experts said it's never going to work, and one year later, he shipped, Rockman said. How could that be? It was, as science writer Arthur C. Clarke in the book of Profiles of the Future called it a failure of imagination.
* In 2016, while still at Google, Amodei co-wrote a foundational paper to the discipline articulating a central problem in AI safety as addressing, quote, the problem of accidents in machine learning systems defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems, unquote. This was distinct from other AI-related challenges he and his co-authors wrote, including privacy, security, fairness, and economic impact. AI safety in this framework, in other words, was about preventing rogue, misaligned AI, the route from which, as described by Nick Bostrom, superintelligence could become an existential threat.

---

## **Page 56**

* Both Amodei siblings were sympathetic to Effective Altruism, or EA, movement, a controversial ideology that had spawned among philosophers at Oxford, where Bostrom was based, and taken hold in Silicon Valley. Over time, the movement, which preaches dedicating oneself to doing maximal good in a world by using extreme rationality and counterintuitive logic to guide decisions had, in no small part due to Bostrom's influence, identified the existential threat of rogue AI as a leading issue area for its adherents to pursue.
* But this existential brand of AI safety built on philosophical thought experiments would soon come under fire as the AI research community awakened to the less apocalyptic and immediate real-world harms of AI. Around the same time Amodi published his paper, ProPublica published a groundbreaking investigation called *Machine Bias* that revealed algorithms were being used across the US criminal justice system in misguided attempts to predict future criminals, and those algorithms were classifying black people as higher risk than white ones who had more extensive criminal records. This piece and an overall souring on Big Tech Post 2016 over the harms of social media sparked a new way of research reckoning with the harmful societal impacts of AI.
* Deborah Raji, an AI accountability researcher at the University of California, Berkeley, would come to champion the re-examination of the overwhelming focus of AI safety research on theoretical rogue AI and its possible existential risk to the detriment and de-prioritization of other real evidence-based problems, co-authoring a 2020 paper in response to Amodeus. She argued that the truly safe AI systems could not be built by isolating the behaviors of the technical systems themselves without placing them in the full context of the impacts on the very things, that is, privacy, fairness, and economics, that Amodeus had set apart. Where Amodeus had raised the idea of AI creating negative side effects as it relentlessly pursued an objective, using an example akin to the paperclip thought experiment of a cleaning robot knocking over a vase or damaging the walls on its path to tidying up, Raji pointed out that this was already happening. In its relentless pursuit of commercial products and AGI, the AI industry had produced an expansive negative side effects, including the wide-scale infringement of privacy to train facial recognition and the aspiring environmental cost of the data centers required to support the technology's development.

---

## **Page 57**

* In reality, for AI systems to even be built, there is very often a hidden human cost. We don't plan to release all of our source code, Altman said, but let's please try not to correct that. That usually only makes it worse. But what is the goal? Amody asked. Our goal right now is to do the best thing there is to do, Brockman replied. It's a little vague.

---

## Page 58

By the end of 2020, the Amodei siblings would become so disturbed by what they viewed as arguments on OpenAI's break from its original premise that they would cleave off to form another AI lab, Anthropic, taking critical staff with them and creating a rivalry that would play a pivotal role in the frenzied release of ChatGPT.
Karnofsky would step down from OpenAI's board, having served his term and due to new conflict of interest. On the list of candidates he nominated for his replacement, he would include one of his former employees, Helen Toner.

Its list of projects sprawled every which way in a kitchen sink reflection of the field. It was using robots and video games and simulated virtual worlds for training agents, all as ways of trying to reach more advanced AI capabilities. Little was working and what did work felt derivative of something somebody else had already done. Whatever EJI was, it wasn't that.
â€œThe bigger projects they had, it didnâ€™t seem like they were doing anything super innovative,â€ says Nikhil Mishra, an AI researcher who interned at OpenAI in 2017.
[Side note: Prof Umesh on ChatGPT]

---

## Page 59

In 2016, OpenAI spent more than $7 million of its $11 million budget in expenses on compensation and benefits.

So, in March 2017, Brockman and Satskever began in earnest to develop a more focused research roadmap. Their central question: what would it really take for OpenAI to reach AGI, and be the first to do so?
Satskever intuitively believed it would have to do with one key dimension above all else â€” the amount of compute, a term of art for computational resources, that OpenAI would need to achieve major breakthroughs in AI capabilities.
The imaginary competition and subsequent advancements that he had been a part of had all involved a material increase in compute.
The advancements had other involved things also, significantly more data and more sophisticated algorithms, but compute, Satskever felt, was king.
And if it were possible to scale compute enough to train AI model at human brain scale, he believed something radical would surely happen â€” AGI.

---

## Page 60

Moore's law turned into a self-fulfilling prophecy. It became the target for how quickly chipmaking firms believed they needed to innovate in order to keep up with competition and stay relevant.
Brockman and Sutskever performed a simple calculation. Based on the pace of Moore's law, how long would it take to reach the level of compute OpenAI needed for brain-scale AI?
The answer was bad news. It would take far too long.
Around the same time, Amodei and another researcher, Danny Hernandez, had begun to look at the same idea from a different direction.
On a simple chart with time as the x-axis, they plotted the amount of compute every major breakthrough in AI research had used since 2012, beginning with Sutskever's grad school breakthrough â€” the start of the AI revolution.
They discovered that compute use was in fact going faster than Moore's law, much faster. In the last six years, it doubled every 3.4 months or, put another way, increased 30 million percent.

---

## Page 61

Chipmaking firms had imposed Moore's law on their companies with existential fervor. The leadership now saw OpenAI's law in the same light.
In 2017, a custom NVIDIA server with 8 of their best GPUs cost $150,000, a price that would rise roughly with inflation to nearly $195,000 by 2023.
In the coming years, OpenAI's law was projecting that OpenAI would need thousands, if not tens of thousands, of GPUs to train just a single model. The cost of electricity to power that training would also explode.
OpenAI needed money â€” not just a billion dollars, but billions of dollars â€” to sustain itself in the coming years.

---

## Page 62

The realization would lead the organization to lose its financial footing. To Brockman and Sutskever, it challenged the very premise of OpenAI's structure.
How could a non-profit raise that much money annually to keep up with the pace required to stay number one?
They briefly considered merging with a chip startup, but in the summer of 2017, they began serious discussions with Altman and Musk about whether OpenAI needed to transform into a for-profit.
This was their best hope to entice investors with a chance at gathering financial return.
After several weeks of negotiations, the deliberations ended abruptly, without resolution.
If OpenAI were to become a for-profit, Altman â€” who was in the middle of considering his run for California governor and getting a lackluster reception in focus groups â€” wanted to be the company's chief executive.
So did Musk. He wanted full control of the lab and to have majority equity.

Elon, we really want to work with you, Sutskever wrote.
We believe that if we join forces, our chances of success in the mission is the greatest.
But Musk's desire for total control felt antithetical to OpenAI's original spirit.
You are concerned that Demis could create an AGI dictatorship. So are we. So it is a bad idea to create a structure where you would become dictator if you chose to.
Sam, when Greg and I are stuck, you've always had an answer that turned out to be deep and correct, Sutskever continued.
That said, Altman's behaviors had often left the two confused about his true beliefs and intentions.
We don't understand why the CEO title is so important to you, he wrote.
Your stated reasons have changed and it's hard to really understand what's driving it.
Is AGI truly your primary motivation? How does it connect to your political goals? How has your thought process changed over time?

---

## Page 64

Musk soon arrived at his own conclusion for how to solve OpenAI's money problem.
In Jan 2018, Andrij Karpathy emailed Musk with new data showing how much Google was dominating AI research publications.
â€œWorking at the cutting edge of AI is unfortunately expensive,â€ Karpathy wrote.
â€œIt seems to me that OpenAI todayâ€™s burning cash and the funding model cannot reach the scale to seriously compete with Google, which is an 800 billion company.â€
While turning OpenAI into its own for-profit could help raise capital, it would require the lab to develop an AI product from scratch â€” a significant distraction from its fundamental AI research.
But by then, Altman had abandoned his political plans and succeeded in his efforts to persuade Brockman, and through Brockman, Satskever that he would be the better leader.
With this group's decision, Musk no longer wanted to be publicly affiliated with the organization.
â€œI will not be in a situation where the perception of my influence and time doesn't match the reality,â€ he'd previously written.
A few weeks later, Musk stepped down as OpenAI co-chair.
Altman became president of the non-profit.

---

## Page 67

In April 2018, OpenAI released a charter to pave the way for the transition.
Without publicly revealing anything about the change to come, the document reiterated the lab's purpose.
Now, with the new wording, OpenAI's mission is to ensure that AGI benefits all of humanity.
Such a mission, the document added, would need OpenAI to be on the cutting edge of AI capabilities and require substantial sources.
It could mean walking back the commitment to release the lab's research due to safety and security concerns.
For the first time, OpenAI also spelled out its AGI definition:

> â€œHighly autonomous systems that outperform humans at most economically valuable work.â€

---

## Page 68

Within half a year, OpenAI and Microsoft were discussing a deal in earnest.
Altman laid the legal groundwork, hurrying along the creation of the Limited Partnership and appointing himself as CEO.
Internally, the project was codenamed *Oregon Trail*.
To keep the deal secret from prying eyes, the for-profit entity was also incorporated under the alias *Summersafe LP*.
The name was a reference to an episode of the cartoon show *Rick and Morty*, where the titular characters, mad scientist Rick and his grandson Morty, leave behind Morty's older sister Summer for another universe and instruct their car to â€œkeep Summer safe.â€
The car takes the objective seriously, resorting to extreme and harmful mechanisms of defense, including murder, paralyzing and torturing people who approach the vehicle.
It was a nod to the potential pitfalls of AI.

---

## Page 70

Brockman responded under his username **GDB**:

> â€œWe believe that if we do create AGI, we'll create orders of magnitude more value than any existing company.â€

Another user followed up:

> â€œEarly investors in Google have received a roughly 20x return on their capital. Google is currently valued at $750 billion. Your bet is that you'll have a corporate structure who returns orders of magnitude more than Google. But you don't want to unduly concentrate power.â€

They wrote, questioning the charter:

> â€œWhat exactly is power, if not the concentration of resources?â€

---

## Page 71

For Gates, *Dota 2* wasn't all that exciting, nor was he moved by robotics.
The robotics team had created a demo of a robotic hand that learned to solve a Rubik's Cube through its own trial and error, which had received universally favorable coverage. Gates didn't find it useful.
He wanted an AI model that could digest books, grasp scientific concepts, and answer questions based on the materialâ€”to be an assistant for conducting research.
OpenAI had only one project that approached fitting the bill: a large language model called **GPT-2** that was capable of generating passages of text that closely resembled human writing.

In February that year, OpenAI had taken the unusual step of proclaiming to the press that this model, once advanced a little further, could become an exceedingly dangerous technology.
Authoritarian governments or terrorist organizations could weaponize the model to mass-produce misinformation.
Users could overwhelm the internet with so much trash content that it would be difficult to find high-quality information.

OpenAI would take the ethical road, it said, and withhold the full version of the model, which had **1.5 billion parameters**, an approximate measure of the model's size and complexity.
Instead, to give the public just a taste of the kind of capabilities that society needed to prepare for, it would publish only a diminished versionâ€”less than one-tenth of the sizeâ€”that had a limited ability to generate a few sentences at a time, but was prone to non-sequiturs and repetition.

---

## Page 72

A month later, on July 22, 2019, Microsoft announced its **$1 billion** investment.
Under the terms of the deal, its returns would be capped at **20x**.

---

## Page 74

But there, ensconced in the cheery glow, magazines strewn across the tables, it was easy to live in a gentler reality.
An employee would later tell me that this was emblematic of her time at the company.
Joining it was like stepping into an alternate universe.
Only after she left did she snap back down to earth.

---

## Page 75

Until that year, OpenAI had been something of a stepchild in AI research.
It had an outlandish premise that AGI could be attained within a decade, when most non-OpenAI experts doubted it could be attained at all.
To much of the field, it had an obscene amount of funding despite little direction and spent too much money on marketing and what other researchers frequently snubbed as unoriginal research.
It was, for some, also an object of envy.
As a non-profit, it had said that it had no intention to chase commercialization.
It was a rare intellectual playground without strings attached, a haven for fringe ideas.

What was clear was that OpenAI was beginning to exert meaningful sway over AI research and the way policymakers were learning to understand the technology.
The lab's decision to revamp itself into a partially for-profit business would have ripple effects into spheres across its influence in industry and government.

---

## Page 76

Brockman nodded vigorously. He was used to defending OpenAI's position.

> â€œThe reason that we care so much about AGI and that we think it's important to build this is because we think it can help solve complex problems that are just out of reach of humans.â€

Why did we need AGI to do that instead of AI?

---

## Page 77

As defined by OpenAI, AGI referred to a theoretical pinnacle of AI researchâ€”a piece of software that had just as much sophistication, agility, and creativity as the human mind to match or exceed its performance on most economically valuable tasks.
The operative word was *theoretical*.

Since the beginning of earnest research into AI several decades later, debates had raged about whether silicon chips encoding everything in binary zeros and ones could ever simulate brains and the other biological processes that gave rise to what we consider intelligence.
There had yet to be definitive evidence that this was possibleâ€”which didn't even touch on the normative discussion of whether people should develop it.

AI, on the other hand, was the term du jour for both the version of the technology currently available and the version that researchers could reasonably attain in the near future through refining existing capabilities.
Those capabilities, rooted in powerful pattern matching known as machine learning, had already demonstrated exciting applications in climate change mitigation and healthcare.
[**Double Speak**]

In a white paper, the organization detailed ten categories of those challenges particularly well-suited to existing machine learning capabilities, including making buildings more efficient, optimizing the load distribution of power grids to integrate more renewable energy, and discovering new materials for green energy generation and storage for more carbon-efficient cement and steel.

---

## Page 78

Researchers from both organizations would tell me that the main challenge of working in these areas was not technical limitations â€” it was quite the opposite.
Persuading talented scientists to focus on problems that necessitated rather simple machine learning solutions instead of the latest cutting-edge techniques that satisfied their ambitions and looked better on a research rÃ©sumÃ©.

It was also finding the political will to deploy these solutions globally.

> â€œTechnologies that would address climate change have been available for years, but have largely not been adopted at scale by society,â€ wrote the Climate Change AI researchers in their white paper.

While they had hoped that AI would be useful in reducing the costs associated with climate action,

> â€œHumanity must also decide to act.â€

---

## Page 79

OpenAI's challenge would be to build an AGI that gave everyone â€œeconomic freedomâ€ while allowing them to continue to live â€œmeaningful livesâ€ in that new reality.
If it succeeded, it would decouple the need to work from survival.

This was a favorite argument in Silicon Valley â€” the **inevitability card**: *If we donâ€™t do it, somebody else will.*

> â€œWhat is OpenAI?â€ he continued.
> â€œRockman, what is our purpose? What are we really trying to do?â€
> â€œOur mission is to ensure that AGI benefits all of humanity, and the way we want to do that is build AGI and distribute its economic benefits.â€

I tried with little success to get more concrete details on what exactly they were trying to build â€” which by nature, they explained, they couldnâ€™t know â€” and why then, if they couldnâ€™t know, they were so confident it would be beneficial.

The lab had to build *good AGI* before somebody else built *bad AGI.*

---

## Page 80

That was undeniable, Satskeva said, but the payoff was worth it because AGI would, among other things, counteract the environmental cost specifically. He stopped short of offering examples.

> â€œIsn't Bitcoin like 1%?â€

â€œWow,â€ Satskeva said in a sudden burst of emotion that felt, at this point 45 minutes into the performance, somewhat performative.

Satskeva would later sit down with *New Yorker* reporter Cade Metz for his book *Genius Makers*, which recounts the narrative history of AI development, and say without a hint of satire:

> â€œI think it's fairly likely that it will not take too long a time for the entire surface of earth to become covered with data centers and power stations. There would be a tsunami of computing, almost like a natural phenomenon.â€

AGI, and thus the data centers needed to support them, would be too useful not to exist.

I tried again to press for more details.

> â€œWhat you're saying is OpenAI is making a huge gamble that you will successfully reach beneficial AGI to counteract global warming before the act of doing so might exacerbate it.â€

â€œI wouldn't go too far down that rabbit hole,â€ Rockman said still.
He cut in.

He spoke like Michelangelo, as though AGI already existed within the marble that he was carving.
All he had to do was chip away until it revealed itself.

---

## Page 81

That was undeniable, Satskever said, but the payoff was worth it because AGI would, among other things, counteract the environmental cost specifically.
He stopped short of offering examples.

> â€œIsn't Bitcoin like 1%?â€

â€œWow,â€ Satskever said in a sudden burst of emotion that felt, at this point 45 minutes into the performance, somewhat performative.

Satskever would later sit down with *New Yorker* reporter Cade Metz for his book *Genius Makers*, which recounts the narrative history of AI development, and say without a hint of satire:

> â€œI think it's fairly likely that it will not take too long a time for the entire surface of Earth to become covered with data centers and power stations. There would be a tsunami of computing, almost like a natural phenomenon.â€

AGI â€” and thus the data centers needed to support them â€” would be too useful not to exist.

I tried again to press for more details.

> â€œWhat you're saying is OpenAI is making a huge gamble that you will successfully reach beneficial AGI to counteract global warming before the act of doing so might exacerbate it.â€

â€œI wouldn't go too far down that rabbit hole,â€ Rockman said still.
He cut in.

He spoke like Michelangelo, as though AGI already existed within the marble that he was carving.
All he had to do was chip away until it revealed itself.

---

## Page 84

OpenAI now had the long-term resources it needed to follow OpenAIâ€™s Law.
And this was imperative, Brockman stressed.
Failing to stay on the curve was the real threat that could undermine OpenAIâ€™s mission.
If the lab fell behind, it wouldnâ€™t be the best.
If it werenâ€™t the best, it had no hope of bending the arc of history towards its vision of beneficial AGI.

It justified OpenAIâ€™s consumption of an unfathomable amount of resources â€” both compute, regardless of its impact on the environment, and data â€” the amassing of which couldnâ€™t be slowed by getting consent or abiding by regulations.

---

## Page 85

It was a nice analogy, but Brockman seemed once again unclear about how OpenAI would turn itself into a utility.
Perhaps through distributing *universal basic income*, he wondered aloud. Perhaps through something else.

He returned to the one thing he knew for certain:
OpenAI was committed to distributing AGIâ€™s benefits and giving everyone economic freedom.

> â€œWe actually really mean that,â€ he said.
> â€œAGI could be more extreme. What if all value gets locked up in one place? That is the trajectory we are on as a society and we have never seen that extreme of it.â€

---

## Page 86

It was â€œa fair criticism,â€ he said, that the piece had identified as a disconnect between the perception of OpenAI and its reality.
Here, Altman is talking about a piece from *MIT Technology Review* â€” a profile of the company that Karen Hao wrote herself.

> â€œI think we should, at some point in the future, find a way to publicly defend our team but not give the press the public fight theyâ€™d love right now.â€

OpenAI wouldnâ€™t speak to me again for three years.

---

## Power and Progress (Interlude)

In their book *Power and Progress*, MIT economists and Nobel laureates Daron Acemoglu and Simon Johnson argue that every technology revolution must begin with rallying ambition.

It is the promise of a technology benefiting everyone that puts in motion the long journey of amassing enough talent and resources to turn it into a reality.

After analyzing 1,000 years of technology history, the authors conclude that **technologies are not inevitable.**

The irony is that for this very reason, new technologies rarely default to bringing widespread prosperity, the authors continue.
As they turn their ideas into reality, the vision they impose of what the technology is and whom it can benefit is thus the vision of a narrow elite imbued with all their blind spots and self-serving philosophies.

> *E.g., the cotton gin served only to intensify slavery.*
> [Neil Postman]

These two features of technology revolutions â€” their promise to deliver progress and their tendency to instead reverse it for people out of power, especially the most vulnerable â€” are perhaps truer than ever for the moment we now find ourselves in with artificial intelligence.

Since its conception, the development and use of AI has been propelled by tantalizing dreams of modernity and shaped by a narrow elite with the money and influence to bring forth their conception of the technology.

That conception is what has led to the exploding social, labor, and environmental costs that are playing out around the world today â€” particularly, as we will see, in many Global South countries, for which the consequences of their dispossession by historical empires still linger in delayed economic development and weaker political institutions.

---

## Page 90

The term lends itself to casual anthropomorphizing and breathless exaggerations about the technology's capabilities.

In 1958, two years after the fieldâ€™s founding, Frank Rosenblatt, a Cornell professor, demonstrated the **Perceptron**, a system that could *blah blah blah blah blah*.

Over his main collaboratorâ€™s objection, Rosenblatt advertised his system as something akin to the human brain.
He even ventured to say that it would one day be able to reproduce and begin to have sentience.

His next warning: the *New York Times* announced that the Perceptron would, in the future, be able to walk, talk, see, write, reproduce itself, and be conscious of its existence.

*Note to self: Find everything that Rosenblatt wrote either in the* New Yorker *or in the* New York Times.

---

## Page 91

This anthropomorphizing has become a rhetorical tool for companies to avoid legal responsibility.

The fear of superintelligence is predicated on the idea that AI could somehow rise above us in the special quality that has made us humans the planetâ€™s superior species for tens of thousands of years.
*(Mark Rowlands on what makes a human)*

Myriad tests have developed over the centuries to measure intelligence against these definitions, many of which have subsequently been debunked and fallen out of favor due to their unsavory histories.

In the early 1800s, American craniologist Samuel Morton quite literally measured the size of human skulls in an attempt to justify the racist belief that white people â€” whose skulls he found were on average larger â€” had superior intelligence to Black people.
Later generations of scientists found that Morton had fudged his numbers to fit his preconceived beliefs, and his data showed no significant differences between the races.

IQ tests similarly began as a means to weed out the â€œfeeble-mindedâ€ in society and to justify eugenics policies through scientific *objectivity.*
More recent standardized tests such as the SAT have shown high sensitivity to a test takerâ€™s socio-economic background, suggesting that they may measure access to resources and education rather than some inherent ability.

---

## Page 93

As software for each of these capabilities has advanced, researchers have subsequently sought to combine them into so-called *multimodal systems* â€” systems that can see and speak, hear and read.

That technology is now threatening to replace large swaths of human workers.
It is not deployed by accident, but by design.

Still, the quest for artificial intelligence remains unmoored.

With every new milestone in AI research, fierce debates follow about whether it represents the recreation of a true intelligence or a pale imitation.
To distinguish between the two, **artificial general intelligence** has become the new term of art to refer to the *real deal.*

This latest rebranding hasnâ€™t changed the fact that there is not yet a clear way to mark progress or determine when the field will have succeeded.

Through decades of research, the definition of AI has changed as benchmarks have evolved, been rewritten, been discarded.
The goalposts of AI development are forever shifting, and as the research director at *Data & Society*, **Jenna Burrell**, once described it, it is â€œan ever-receding horizon of the future.â€

The technologyâ€™s advancement is headed toward an unknown objective with no feasible end in sight.
To justify the elongating timeline and ever-expanding costs of pursuing the ambition of AI, the promises we are told about it have grown more grandiose than ever before.

AI was once a scientific fascination, a technology with some potential commercial utility.
Now AI is the **harbinger of the Fourth Industrial Revolution**, the keystone of modern superpower.
AGI, if ever reached, will solve climate change, enable affordable healthcare, provide equitable education.

OpenAI is the poster child of this line of thought.

> *Note to self:* They have no choice but to say this in order to justify the effort, the funding, the time, the money, and the talent cost.

---

## Page 94

Itâ€™s not a coincidence that AI today has become synonymous with colossal, resource-hungry models that only a tiny handful of companies are equipped to develop â€” and that desire us to make their products into the foundations for everything.

Achieving AI must then involve encoding symbolic representations of the worldâ€™s knowledge into machines, creating so-called **expert systems**.

At the helm of the connectionists was **Frank Rosenblatt** and his *Perceptron*, an early proof of concept for a machine learning system.
At the helm of the symbolists was Rosenblattâ€™s nemesis, **Marvin Minsky**, an MIT professor and co-organizer of the Dartmouth workshop.

Minsky had himself dabbled in connectionist thinking before souring on the idea.
He did not switch his loyalties quietly.

He found frequent opportunities to grandstand and ridicule his connectionist colleagues who competed with him for the same grants, sparing not even early-career researchers.

In 1969, he co-authored a book called *Perceptrons*, so critical of connectionism that it is credited â€” along with the meddling progress of neural networks â€” for killing off nearly all funding to that vein of research for more than 15 years.

This is what is called the **AI winter.**

---

## ELIZA Note

In fact, the demonstration felt so convincing â€” the demonstration of **ELIZA**, that is â€”
to some psychiatrists that they began to speak of automated psychotherapy as just around the corner.

And merely a few years after the founding of the AI field, computer scientists were already prematurely concluding that natural language understanding in computers was a solved problem.

Decades later, whether or not itâ€™s even being solved today is still an open debate.

---

## Page 96

Weizenbaum published a tome called *Computer Power and Human Reason* in the decade following Minskyâ€™s *Perceptrons* that argued that humans and machines are different â€” and that the AI fieldâ€™s attempt to blur that distinction would lead to profound societal consequences.

It would, for example, allow people in power â€” whether CEOs or politicians â€” to execute their will through machines while absolving themselves of moral responsibility.

---

## Page 98

In another instance of rebranding, Hinton later cleverly gave his multi-layer processing the name **deep learning**,
a shorthand for using deep neural networks to perform machine learning.

---
