## Page number 2
Estimates from those in the room ranged from 0 up to 90% according to the reporting
on the event. Probability of doom and hope. Schumer tweeted afterward, if managed properly
AI promises unimaginable potential. If left unchecked, AI poses both immediate and long
term risks. These risks have been deemed existential by those who have a pea doom around the high
end, risks that, if left unchecked, would threaten the whole of humankind.
Doom reminds us of titanic cartoonish frictional battles of good versus evil, and the cartoonish
connotations are apt. Just like such frictional battles, pea doom estimates are based in fantasy
rather than data or empirical fact. But that hasn't prevented this imaginary metric from
becoming a fixation of lawmakers, venture capitalists and Silicon Valley's managerial class. We imagine
part of the appeal is that it allows people in power to imagine themselves as heroes out
to save humanity while actually turning away from the very real threats to actual people.
For example, Robert Williams of Detroit, Porchav Woodruff of February 2023, automated facial
recognition systems, and so on.

## Page number 4
But harms befalling real people are not what pea doom refers to. Instead,
it is the wrong metric and the wrong framing. It serves to obfuscate what's really going
on. Artificial intelligence, if you're being frank, is a con. A bill of goods you're being
sold to line someone's pockets. The language of pea doom is a ruse to keep us focused on
imaginary scenarios filled with awe at modern Robert Barron's allegedly potential world
ending technology and too distracted to see the daily harms being done in its name.
A characteristic of our current hype cycle is that conmen are taking a series of tropes
from science fiction of artificial minds hell bent on turning us into paper clips or terminators
waging wars for the right to exist and injecting them into discussions at the highest echelons
of business and government.

## Page number 5
To put it bluntly, AI is a marketing term.
It doesn't refer to a coherent set of technologies. Sometimes the people selling these tools seem
to believe their own marketing. But what really matters is that they can sell it that way.
The types of AI, decision making, classification, recommendation, transcription or translation
and text and image generation. Lumping all of these different technologies under the
label of AI creates the illusion of intelligent technology. Tech synthesis machines have an
outsized role here. Language is so central to our understanding of each other that when
we encounter language that doesn't actually reflect the thoughts, ideas or communicative
intent of another person, it's difficult not to imagine someone human like behind it. This
is probably why LLMs have blown up.

## Page number 7
This wasn't because of any
magic or quantum leap in technology, but for the most part followed from innovation predicated
on the falling costs of microchips and the abundance of digitized data on the web.

## Page number 9
Like other kinds of hype, AI hype plays on FOMO. It is the repeated message
that a set of technology will change the world and you, the consumer or corporate manager
absolutely must use it, lest you be left in the dust. As a consumer, if you don't get
in on the hype product, you'll be seen as a regressive Luddite lacking in modern skills
and or about to have your job automated away. Self note about printers and guns. That was
the self note. That was the self note.

## Page number 9
AI hype in particular plays a
cultural function as well. It connects a commercial goal with a popular fantasy of sentient machines.
The

## Page 10

. The tale is as old as Mary Shelley's Frankenstein about the monster
that turns on its creator or even older in the Judaic figure of the Golem, which in some
iterations of the story goes rogue after its human handlers forgot to deactivate it.

## Page 12

. In an environment where the battle for American supremacy in the Cold War was
being fought on all fronts, military, technological, engineering and ideological, these men sought
to gain favor and funding in the eyes of a defense apparatus trying to edge out the Soviets.
They relied on huge claims with little to no empirical support, bad citation practices
and moving goalposts to justify the ethics which found purchase in the Cold War America.
Fine steps claims. **Self note. You already know about some outrageous articles that Frank**
Rosenblatt himself wrote, but find some such other claims as well.

## Page 13

. A shocked
Weisenbaum spent the rest of his life as a critic of AI, noting that humans were not
meat machines, while Minsky went on to find MIT's AI laboratory and Reagan funding from
the Pentagon unhindered.

## Page 14

. There are applications of ML that are well
scoped, well tested and involve appropriate training data such that they deserve their
place among the tools we use on a regular basis. These include everything, things such
as spell checkers, radiology models, image processing and so on. But in the cacophony
of marketing and startup pitches, these sensible use cases are swarmed by promises of machines
that can effectively do magic, leading users to rely on them for information, decision
making or cost savings, often to their detriment or to the detriment of others. We are told
that AI will quote democratize, unquote, creative activity by allowing everybody to
become an artist. **Self note. Think back to what Karpati told Lex Friedman about what**
would happen if everybody could generate art and what if there was infinite of it. We are
told that AI is on the verge of doing science for us, finally providing us with answers
to urgent problems from medical breakthroughs like cancer to the climate crisis. And self
driving cars are perpetually just around the corner. **Self note. Think of what Karpati**
said about self driving cars to Dwarkesh Patel. He said they are barely done.

## Page 16

. What all these
stories have in common is that someone oversold an automated system. People used it based
on what they were told it could do. And then they got or others hurt.

## Page 17

. AI
Hype today infests almost every corner of our culture. Claims that we are dealing with
sentient beings and that one day these beings will become super intelligent are unavoidable
from popular entertainment to the course of halls of Congress. Breathless reporting
are uncritically parrots corporate statements that AI is going to free you from work, educate
your kids and provide medical care to all who need it. The Hypers claim that AI will
produce art but also might just kill us all. Should it decide to spare us at the end of
the day, you will be able to kick back in a fully automated paradise once AI has solved
the climate crisis and poverty. We also discuss how the search for quote general intelligence
unquote is not only a futile search but one that is grounded in a deeply racist history.

## Page 19

. We also find false promises in media synthesis machines in the creative
fields including art, science and journalism. And these take up in chapter five. AI artists
have proliferated online and the visual artists who they are ripping off are rightly none
too happy about it. And journalism already suffering from dramatic job losses and fire
sales of respected new organizations is ripe for the infection of AI generated content intended
to maximize eyeballs on ads with as little investment in actual journalism as possible.
In chapter six we return to how we began this chapter with a discussion of how AI doomerism
all that talk of PDOOM makes many of the same that is false assumptions and plays similar
economic functions as AI boosterism. Some doomers are true believers and are motivated by a
set of ideologies including effective altruism and long termism which preach that we are
that we need super intelligent technology to colonize the stars.

## Page 21

. In February 22, Open AI's chief scientist Ilya Sutskeva tweeted it may
be that today's large neural networks are slightly conscious. In an August 2022 blog
post, Google VP fellow Blaise Agira Yarkas responded to the Lemoine story. But rather
than countering Lemoine's claims, he suggested that lambda does indeed quote, understand,
unquote concepts and that the debate over whether or not lambda has feelings is not
resolvable or quote scientifically meaningful unquote.

## Page 23

. This particular variety of AI hype has an extra layer of insidiousness.
When we imbue these systems with fictitious consciousness, we are implicitly devaluing
what it means to be human and endorsing a much longer line of thinking about the nature
of intelligence based in eugenics and race science. There's a reason magicians never
explain their tricks. Knowing how it's done helps dispel the illusion.

## Page 25

. Neural nets first proposed by Warren McCulloch and Walter Pitts in the
1940s and first implemented by Frank Rosenblatt in the late 1950s didn't really take off
until the 2010s when both available computing power and data sets became large enough to
make them practical and profitable.

## Page 29

. Weisenbaum initially believed
that an explanation of how Eliza produced its responses would be enough to dispel the
illusion that it understood user input.

## Page 32

. As
tech writer and co-founder of Logic Magazine when Tarnoff wrote in a recent Guardian profile,
Weisenbaum believed that computers constricted rather than enlarged or humanity, prompting
people to think of themselves as little more than machines. By ceding so many decisions
we had created a world that was more unequal and less rational in which the richness of human
reason had been flattened into the senseless routines of code. **Self note. Correlate this**
to what Neil Postman said.

## Page 33

. But while AI boosters have spent time devaluing
what it means to be human, the sharpest and the clearest critics have come from black,
brown, poor, queer and disabled scholars and activists.
Methods of defining and measuring intelligence have been more than
complicit in this project. Indeed, they were designed specifically to do such a thing.

## Page 37

. Many people associate eugenics with some of the most horrific atrocities of the
20th century, such as the Holocaust and the Nazi regime of raising good Aryan families while aiming
to eliminate Jewish people, disabled people and other people the Third Reich considered undesirable.
However, modern eugenics has its origins in the 19th century French and English colonialism
and the industrial revolution. 19th century economist Thomas Malthus suggested that if
birth rates ran unchecked in Britain, then the lack of food housing and other necessities would
mean dire consequences for everyone. Malthus suggested that this could only be solved by
reducing birth rates and discouraging people from having children. This is what would be called
negative eugenics, spanning certain types of people from getting married and having children
and forcing their sterilization. Positive eugenics is the move to promote a birth of
a certain kind of people. In practice, eugenic policies have most often targeted racial or
religious minorities, migrants from the majority world, disabled people and others hated by classes
with power.

## Page 39

. Oddly enough, they also believe that the development of AGI poorly done could spell the end of humanity development, a belief that is known as existential risk.
You would think that dumping billions into AI research while also believing that AI can bring the end of humanity would be at odds with each other and you would be right.
On

## Page 43

, there is a section called Luddites and Labour Power which presents an alternative history of Luddites.

## Page 47

. We are not kidding. Four months after the release of chat GPT, researchers at OpenAI self-published a report titled GPTs are GPTs.
An early look at the labour market impact potential of large language models. This title needs a little unpacking. The first GPT is Generative Pre-Train Transformer.
The second GPT stands for General Purpose Technology, the type of technologies that the authors, following several labour economists liken to developments such as the wheel, the steam engine and the electricity.
Yet in this paper, the authors make some quite strong claims indeed. Not only will 80% of American jobs have at least 10% of their work affected by the introduction of LLMs, but 19% of workers would see over 50% of their work automated.

## Page 48

. In this report, the two sources, the researchers they asked about labour market exposure were other researchers at OpenAI and, wait for it, GPT4 itself. That's right. Researchers asked GPT4 if it could take workers' jobs from them.
A white paper written by analysts at the investment bank Golden Sacks Estimates based on the data from the United States and the European Union that a quarter of all global work could be replaced by AI tools.
In addition, 300 million jobs worldwide could be exposed to automation, meaning part of the job could be replaced. Their methodology, however, does not inspire confidence.
They rated each job tasks from 1 to 7 in difficulty and simply assumed that if the task had a score of 4 and lower that it could be automated away.
Task difficulties of store 2 include check to see if baking bread is done and interpret a blood pressure reading.
Tasks of difficulty of score 4 include test electrical circuits and complete tax forms for a small business.
In other words, they asked us to appreciate the promise of replacing bakers, nurses, electricians and accountants with tax synthesis machines.
Only one of these jobs centrally involves writing text, but surely we'll all be happy with random errors in our taxes, right?

## Page 51

. Take the example of Google search. In the early 2000s, replacing hand curated indexes like Lycos and Yahoo seemed like a large boon for those struggling to navigate the unstructured web.
Google's novel search algorithm seemed to outpace similar services offered by Altavista and other early search engines.
But now Google search itself structures the web and not in a way that benefits the broader public.
Google is first and foremost in the business of selling ads, not providing helpful access to information.
SEO consultants can extort high fees with promises to get their clients aside to the top of Google's results by selecting keywords and optimizing web pages,
which lends Google search to prioritize some pages over others for generating ad revenue.
Over time, Google search results have vastly degraded in quality.
Institutions that we rely on to provide high quality information, like newsrooms, universities and public health departments,
often come up far below links to sites by entrepreneurial SEO gamers that may be providing little factual information, if any at all.
Google's advertising model has led to an inferior product, what author and technology critic Corey Doctorow called N-Shitification.

## Page 53

. We have also seen that these tools do a lot of whole cloths copying of the training data.
In an early release of Copilot, open source developer Armin Ronacher discovered that, given the prompt to code the fast inverse square root,
a speedy approximation of a mathematical formula used for graphics processing, Copilot regurgitated the exact computer code written by V Petkov
in the popular video game Quake 3 Arena, down to the copyright and swear-laden comments.

## Page 56

. The anger motivating these acts isn't isolated.
The destruction of the Vemo Jaguar symbolizes not only the frustration with filling our streets with things we don't need,
but also with filling the internet with content we don't want and filling our workdays with tools that don't work.

## Page 58

. In November 23, the self-driving car company Cruise admitted that its driverless robot axes were monitored and controlled by remote workers.
The New York Times published a story that reported that these cars frequently had to be assisted by remote human workers.
Affronted by this, misinformation cruise CEO Kyle Wacht took to Hacker News, a forum hosted by venture capital incubator Y Combinator to set the record straight.
These cars didn't need to be remotely driven frequently, but 2-4% of the time in tricky situations.
That itself is quite the admission. These cars should hardly be called autonomous.

## Page 66

. Those who resist the imposition of technology are disparaged as technophobes behind the times or incompetent, sometimes even Luddites.
But in fact, Luddites is exactly the right term, even as those using it as an insult don't realize it.
In the tradition of the original Luddites, writers, actors, hotline workers, visual artists and crowd workers alike show us that automation is not a suitable replacement for their labor.
We don't have to accept a reorganization of the workplace that puts automation at the center with devalued human workers propping it up.

## Page 68

. AI boosters will brag that these machines will make key services more accessible for everybody.
Medical advice will be free and on-demand legal services will be available to anybody who needs them and robot teachers will provide the care and knowledge that was only available in our wildest dreams.
In reality, the parts of these that actually matter, relationships, economies of care and time spent with professionals who want to help and understand your problem will be devalued and replaced with cheap fakes for people who can't afford real professionals.
And of course, this is a path of widening inequality, making life worse for those who already are marginalized, the poor, black and brown communities and the people in the majority world.

## Page 70

. In this way, the function of the automation isn't to support people but rather to provide a false sheen of objectivity over a brutally discriminatory system.
What's needed is more resources and more time for social workers to connect families to these resources. Automation in the name of efficiency here only makes the government more efficient at harming families.
**Self note. Again, Neil Postman.**

## Page 74

. In the United Kingdom in 2023, then Prime Minister Rishi Sunak convened and summit around AI held at Bletchley Park, referring to AI as the greatest breakthrough of our time.
However, there are many users of AI in the UK that are having detrimental effects for those intimately involved with its outcomes. It was after all only three years before that masses of students marched in the streets chanting fuck the algorithm in opposition to algorithmic decisions that scored their A level exams.
The Sunak government well went all in on Generative AI. In early 24, Deputy Prime Minister Oliver Dowden announced plans to use LLMs to draft responses to questions submitted by members of parliaments and answers to freedom of information requests.
We find this a really telling commentary on the government's attitude towards freedom of information requests. If they are willing to use a text extruding machine to provide responses, they clearly don't care about answering accurately.
And in 2023, Sunak announced that the NHS would be implementing chatbots all over the place. The text extruding machines would be used to transcribe doctors' notes, schedule appointments and analyze patient referrals.
The NHS also announced in mid-23 that it has invested £123 million to investigate how to implement AI throughout the system, including brain, heart and other medical imaging, and was making another £23 million available for these technologies.
According to Sunak, these plans will ensure that the NHS is fit for the future. We back to differ. Throwing synthetic text into patient interactions and patient records sounds like a recipe for chaos, dumped on an already overstressed workforce.
**Self note and a very important one. Yudhkovsi said something about Rishi Sunak. Find out why didn't he say a word about this nonsense.**
These government leaders, Adams, Newsom and Sunak have accepted Generative AI into the work and operation of the government with confidence and enthusiasm. All of them use words such as ethical, responsible and the like, but there could be another option.
Just don't use these tools. Government processes that affect people's liberty, health and livelihoods require human attention and accountability. People are far from perfect subject to bias and exhaustion, frustration and limited hours.
However, shunting consequential tasks to black box machine trained on always biased historical data is not a viable solution for any kind of just an actual accountable outcome.

## Page 82

, 83. This example highlights the distance between evaluation practices as applied in research settings and the kind of evaluation that is needed to test how well a system could work in the real world.
With the excitement about large scale image and language models in the mid to late 2010s, however, evaluation in research started to go right off the rails when researchers started creating benchmarks which they claimed tested for things like general purpose natural language understanding.
Once the benchmark is published, it turns into a contest for developers to compete in and brag about their scores on a public leaderboard, which further shifts incentives away from testing models for realistic situations and towards achieving a high score on a fixed evaluation task.

## Page 83

, all they are really good for is hype filled headlines like chat GPT passes bar exam, reinforcing the misconception that reciting the correct form is all that is needed for practicing law, medicine, therapy and the like.

## Page 88

, the slack of serious evaluation and poor accuracy rate hasn't stopped Greg Corrado, the head of health AI at Google Health, from going on a press tour bragging about the tool to Wall Street Journal and announcing it loudly at Google I.
The journal reported, originally, that the tool was being tested in actual hospitals, including the prestigious Mayo Clinic. Corrado himself admits that he wouldn't want the tool to be a part of his family's healthcare journey.
But in the same breath, he says that the large language model will take the places in healthcare where AI can be beneficial and expand them by tenfold.
We note that tenfold increase on zero is still zero, so his statement might actually be technically correct.

## Page 90

, everything about Hippocratic AI is appalling. Shah seems to have missed that empathy and personal interest both require subjective experience and human connection.
Keeping a transcript of every conveniently digitized interaction isn't remembering conversation, but it surely is building up a trove of data for future monetization.

## Page 92

, a recent study found out that widely used LLMs propagate racist misconceptions which have been thoroughly debunked but are still common amongst medical students.
For instance, the study found out that four models, Google's Bard, which is now Germany, Open AI's ChatGPT and ChatGPT4 and Anthropics' Claude, reproduced racist myths about black people around lung capacity, skin thickness and kidney function.
Moreover, like UnitedHealth and their faulty algorithm for estimating post-acute injury care, companies are also trying to use language models as a means to evaluate insurance claims which they argue would reduce the number of workers needed to assess if claims
The rush to implement AI solutions to all the problems of government services, law, healthcare and education is inspired, not to mention funded by Silicon Valley tech executives and their philanthropic arms.
Sam Altman and Bill Gates have promised us cheap services for those who don't have access to social services, healthcare and education.
Altman has clear profit motives in hyping AI in these spaces and Gates has an interest as a major philanthropist in the areas of health and education, and it should be mentioned as the co-founder of Microsoft he encouraged the company's major investment in open AI.
As for the Bill and Melinda Gates Foundation, their position in both education and healthcare means that they can set the agenda for public schools and global health.
How did it come to this? We were promised in science fiction and speculative visions of the future that automation would take over the drudgery of doing repetitive labor like data entry, cleaning dishes, scheduling meetings between people.
Instead, we are told we are supposed to accept and even celebrate machines that are creating art and taking all other creative activities that are uniquely human.
In the future, moustach posits. All content will be interactive and dynamic, and if you want characters from different movie or video game franchises to interact in a bespoke way, you can put appropriate prompts into something like Photoshop to get your desired dimension.

## Page 104

, except that's not at all how it's played out in practice.
The people using that content are infringing on the very markets that those artists sell
their work on to make their living.

## Page 105

.
In February 2023, only four months after the public release of Chad GPT, people trying
to make a quick buck by publishing stories extruded by Chad GPT in the speculative fiction
magazine Clarkswood flooded its submission portal, leading the magazine to temporarily
stop accepting submissions.
Fortunately, they have reopened, with a warning that submitting something written by or with
AI will lead to being banned from the site.
Similarly, Julie Ann Dawson, the founder and creator of Bards and Sages, a small publisher
of speculative fiction and role-playing games, announced in 24 that she was closing up shop
after 20 years.
If not, this is exactly what is happening to Google Summer of Cold.

## Page 108

.
The goal behind alt text was to provide a means for low vision and blind users to have
access to the contents of images, and so in principle it should be a rich source of accurate
textual image descriptions.
One wrinkle of this all, however, is that alt text has been repurposed for search engine
optimization.

## Page 109

.
Lastly, the promotion of AI art betrays a deep misunderstanding of the nature of what
ought to be considered art.
The major functions of art include sharing experiences and providing insight into the
human condition, not to mention the joint fulfillment of artistic expression.
As philosopher and technology scholar Jonathan Flowers has said, the purpose of art is to
signal a particular kind of intention and to convey a particular type of experience,
and that is precisely what AI art lacks.

## Page 111

, AI boosters have done a lot of policy legal and ideological work to claim
that the transformative nature of AI tools fulfills the first factor of the test.
Accordingly, that factor is the most fuzzy and opens up the most room for judicial discretion.
Their argument goes something like this, the model training involves copying the original
work, yes, but then it only focuses on the words for text or on pixels for images, turns
them into numbers and input into model and then output something different altogether.
That argument may hold water if the process we described were the only thing that the
synthetic media extruders were doing.

## Page 112

, venture capital firm Addison Horowitz warned that all of their investments
in AI would be worth a lot less if they had to abide by copyright law.
Imposing the cost of actual or potential copyright liability on the creators of AI models will
likely kill or significantly hamper the development.
That is, if they actually had to pay parties, illustrators and writers what their content
is worth, rather than simply stealing that content from the web, their business model
would fall apart.

## Page 113

, Yan Lekun, chief AI scientist at MetaBragd, type a text and galactica.ai
will generate a paper with relevant references, formulas and everything.

## Page 114

, the galactica demo lasted only three days before effectively getting
ridiculed off the internet.
Throughout, Meta's Lekun beclowned himself, first promoting galactica is able to write
scientific papers and then whining about how people were using it.
He tweeted, follow our text, galactica spits out a prediction of what a scientific author
might type, thereby saving time and effort.
This can be very helpful even without being completely accurate.
The usual disclaimer applies, garbage in, garbage out, prompt it with lunacy, get lunacy.
This completely misses the point that LLMs are simply not suited to the task of synthesizing
and presenting scientific information.

## Page 118

, this view also places science as the source of solutions to social and political
problems and moreover computer science as the field developing the problem solving AI
as the ultimate scientific authority.
In that context, it's ironic and painful to find things like climate change so frequently
cited for instance by the World Economic Forum among the things that AI will solve for us.
The climate crisis is fundamentally a collection of social problems about building political
will to overcome current economic incentives and about how to allocate resources to accommodate
climate refugees.
We can't technology our way out of it and neither could a hypothetical AI scientist.

## Page 119

, Messery and Crockett point out that quite apart from whether any of this
is possible, it is actually harmful to the process of doing science.
The allure prestige of AI raises the risk of narrowing fields of inquiry into those
questions which can be approached with these truths.
At the same time, the imagined tools represent the epitome of a view from nowhere or the
idea that one can have objective truth, objective knowledge of a set of truths uncolored by
their personal experience.
At this historical moment where science is finally starting to grapple with the idea
that the standpoint of the scientist matters, we should rather build diverse communities
of knowers.
Western ecologists for instance have begun to learn something that indigenous communities
have known for a very long time.
To control wildfires and maintain healthy locust plant and animal ecologies, humans need to
conduct controlled burns of forest and areas with overgrowth.
The perspectives and stewardship of tribes matters deeply for the management of the natural
lands.
The last thing we need is shiny tech that promises to obviate the need for the hard work
of building inclusive scientific communities and putting those perspectives in conversation.
When asked where she dropped her keys, she responds about five yards that way but the
street lamp is over here.
AI for science makes us think we can find our keys by limiting our view to only those
sidewalks illuminated by the glow of data centers powering it.
Pays on 120.
Tools like DeepMinds may have potential for doing large scale pattern matching but by
failing to recognize that science is a fundamentally human endeavor, they are working against its
collective promise rather than for it.
When the creative and social work of doing and communicating science is treated as a
simple input-output process that can be modeled algorithmically, the people involved are dehumanized.
And it's not just the AI developers who are implicated in this dehumanization, far too
many senior scientists joke that chat GPT works just like a research assistant, suggesting
that they see the junior scholars they are supposed to be mentoring as mere systems for
producing partly incorrect summaries.
And unfortunately, isn't the only way that the push for AI in science harms science.
Pays number 123.
These tools are useful to have in one's toolkit but it's not a scientific revolution nor
a solution to a whole scientific field and it certainly isn't a reason to abandon other
approaches for science and jump on the AI bandwagon.
Pays number 138.
Meanwhile Yoshua Benjiro, professor of computer science at the University of Montreal and
Malo Bourgon, CEO of an organization called the Machine Intelligence Research Institute,
both elaborated on how such a scenario might play out.
Benjiro stated that there are many reasons AI could end up with undesirable goals, each
of them may put the system in conflict with humanity and give the system a reason to preserve
itself despite human attempts to intervene.
Bourgon, meanwhile, was very alarmist indeed.
The most likely outcome of developing smarter than human AI prior to solving alignment is
human extinction.
Present day AI systems do not pose an existential threat but there is a significant charge
that systems in the near future will, as they become capable of performing increasingly
complex multi-step tasks without humans in the loop.
Pays number 139.
One shows a utopian world of abundance, the other a dystopian hellscape.
Neither depicts the real harms of actually existing automation, at best dismissing them
as less important than imaginary existential threats.
Should you be scared of an autonomous AI agent?
No.
But you should be wary of the alarming ideologies behind both AI doomerism and boosterism.
Doomerism boosterism serves to obscure rather than illuminate what's at stake when it comes
to the real to the current AI boom.
Moreover, these technologies are accelerating the real threat of human-made climate change
cutting into our already too thin margin of time to mitigate it.
Pays number 141.
They have markedly not complained about the ways in which the business practices around
AI have accrued financial, social and political power to a very select few.
They are mostly men, they are almost universally white and western.
With so many signatories on these letters, it is likely that the crowd includes people
who believe in this vision religiously as well as people with financial or social incentives
for signing.
But for some of them, it's not really about trying to save humanity but rather a running
of the con.
The supposed danger of the systems is a splashy way to hype their paths with the goal of scoring
big investments in their own AI ventures like Musk and Altman or funding for their own research
centers like Borgon.
Pays number 143.
Embedded in the idea of alignment is a premise with which we fundamentally disagree.
That AI development is inevitable.
We take Ambaraj with this on multiple grounds.
For one thing, what is currently being developed as AI does not work, nor is it helpful for
an overwhelmingly large portion of people on the earth in the world today, especially
people in the majority world.
Furthermore, as we've said elsewhere, there is no clear precise definition of AI, nor
is there any solid evidence of that work of AI research.
Now is on the path towards that undefined destination.
Lastly, the development of mass automation tools is not socially desirable.
If you've gotten this far in the book, you've seen how this technology serves as a means
of centralizing power, amassing data and generating profit rather than providing technology that
is socially beneficial.
In other words, this is a choice, one being made by powerful interests, but one that rests,
but one that the rest of us do not have to go along with.
Thus we reject this inevitably out of hand.
For instance, in 2002, after 9-11, the US passed the American Service Members Protection
Act, also known as the Hague Invasion Act, which authorizes the use of military force
to liberate an American citizen from the International Criminal Court in the Hague, if they are held
in that court's custody.
In light of this, we ask whose understanding of human values would be encoded in a hypothetically
aligned AI system, and why should the rest of the world just have to accept it?
Pays number 145.
When AI doomers warn against existential risk, what they really mean is existential risk
for well-off, white, western and able-bodied people who are insulated from becoming climate
refugees.
There are people who are right now experiencing awful conditions, losing access to rights and
freedoms due to war, famine and rot.
We don't need to construct a thought experiment like the paperclip maximizer to think of conditions
which no human should be subject to, nor to start working on ameliorating them.
Pays number 146.
There's something perverse in the call for those concerned with social justice to subsume
their goals and research under the umbrella of AI safety.
This reminds us of the idea of colorblind racism.
When people in power don't talk about race, they end up reinforcing racism and white supremacy
precisely because they don't talk about how existing institutions are harmful for people
of color and need to be reformed or abolished.
Pays number 148.
Gary Tan, the president of the influential Y Combinator, has signaled his support for
this movement.
In brief, they think that AI is an unmitigated good and that nothing, not regulation, artists
or even the AI safety people should get in their way of developing it as they see fit.
It's also worth mentioning that Tan has been aggressively acting in San Francisco politics,
ruthlessly opposing policies that might address the city's homelessness epidemic in favor
of those that would criminalize being unhoused, all while drunkenly tweeting death threats
to a sitting San Francisco politicians.
Pays number 149.
Doomerism and boomerism are supposedly diametrically opposed camps, but both see AI as inevitable
and desirable.
Take, for example, the words of doomer Eliza Ryutkovsky, a self-styled AI researcher who
founded the Machine Intelligence Research Institute and is very influential in doomer
circles.
In a Time magazine op-ed, he writes that he did not sign the six-month moratorium letter
because he said it is asking for too little, and it would be like the 11th century trying
to fight the 21st century.
If humans were ever asked to defend themselves against an AI system that had turned against
humanity, he suggests that we need to track all large server farms in which AI systems
are trained and, if needed, destroy a rogue data center in an airstrike.
This vision is just as committed to power and feasibility of sentient and autonomous
AI systems as Andre Stuntz but anticipates a dark term.

## Page 150

, doomer style themselves as akin to nuclear engineers building reactors
for cheap energy while knowing that their work can be used to create weapons of mass
destruction with few modifications.
**Self-note, maybe they have the Oppenheimer complex.**
Maybe not complex, maybe they have the Oppenheimer fantasy.

## Page 152

, the totality of systems sold as AI are these things rolled into one.
The danger is not from some hypothetical extinction level event.
The danger emerges from rampant financial speculation, the degradation of informational
trust and environments, the normalization of data theft and exploitation, and the data
harmonization systems that punish the people who have the least power in our society by
tracking them through pervasive policing systems.
But the doomer's boosters would have us looking the other way from all these real arms to
be tested by the dystopian utopian visions.

## Page 153

, there are very few examples of a more intelligent thing being controlled
by a less intelligent thing.
We wonder when the last time was that Dr. Hinton suffered from any kind of food poisoning
and if he did, then decided that bacteria are more intelligent than humans.

## Page 155

, this has been a persistent stumbling block for both doomers and boosters
alike.
When they talk about general intelligence, they don't actually have a justifiable definition
of the concept.
Meta founder and CEO Zuckerberg, for instance, has said that he doesn't have a one sentence
P3 definition of general intelligence.
OpenAI has a nebulous definition of artificial intelligence in their charter, highly autonomous
systems that outperform humans at most economically valuable work.
What does outperform mean?
What is most economically valuable work?
Oddly enough, the term itself has taken a type of spiritual significance, a sort of
we'll know it when we see it kind of quality.
Sutskaber, when he was at OpenAI, urging employees to feel the AGI, instructing them to chant
it like a mantra and if they can't even define it, there is no way that they are capable
of measuring it.

## Page 160

, it's easy to talk about benefits justifying the cost when you aren't the one
actually paying the costs.
Not only the tech barons, but also most of their highly paid employees are fairly insulated
from the climate crisis compared to climate refugees whose those living in tropical zones
and precarious click workers who even in the US can't afford air conditioning or easily
escape from smoke choke cities during ever extending fire seasons.
For a tech baron to confront these harms would require them to take their own culpability
seriously and contemplate their own privilege.
It's far more comfortable to sit back and pontificate on imagined scenarios where they
would be exposed to automation with 18% of global work outright replaced by it by June
24 was saying that the technology is nowhere near where it needs to be to replace jobs
at that rate.
Labour economist at MIT published an estimate that productivity gains from AI will be less
than 0.53% over the next 10 years.
On an investment call after its 2024 Q2 earnings report, Alphabet CEO Sundar Pichai was grilled
with questions about when its big bet on AI to the tune of $12 billion per quarter would
pay off.
David Kahn at VZ Giant Sequoia Capital wrote in June 24 that AI firms need to turn something
like $600 billion in revenue for the AI bet to pay off.
In 2023, the author and technology critic Corey Doctro asked what kind of bubble is
AI?
Comparing the technology to prior cycles of hype including the dot-com bubble, blockchain,
non-fungible tokens and the metaverse, tech bubbles come into varieties, ones that leave
something behind and the ones that leave nothing behind.
Sometimes it can be hard to guess what kind of bubble you are living through until it
pops and you find out the hard way.

## Page No. 195
After the AI bubble bursts, where do these careers go?
Managers and execs aren't likely to hire back career workers to do the skill labour
they once did.
They'll hire more contingent labourers, doing more with less.
The residue of the bubble will be sticky, coating creative industries in a thick, sooty grime
of limitless tech expansionism.
This is the fallout of venture capitalists and tech entrepreneurs not pausing to think
about who would be caught in the blast radius.
