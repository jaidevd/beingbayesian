## Lex Fridman (2 yrs ago)
- There's no magic in neural networks, but they do display emergent behaviour when really large.
- Much more hesitant with the analogies to the brain.
- ANNs are complicated alien artifacts. (not like brains).
- ANNs do compression. Our brains do a lot more.
- Transformers are probably one of the most important developments in AI. A general purpose trainable computer.
- The zeitgeist today is "don't touch the transformer, touch everything else"; scale up data; scale up evaluations.
- Unchanged architecture: - Last 5 years have been about this.
- What's interesting is not the LM itself, but the fact that _scaling_ leads to emergence.
- like Hinton, we accept that LLMs "understand" stuff.
- Text is not enough. There's enough we don't put down in text.
- In 2015, the zeitgeist was about RL (the Atari paper, etc).
- RL is a very inefficient way to train a neural net. Very sparse rewards.
- He's bullish on neural nets writing software (Software 2.0).
- They're not just an algorithm anymore. They're a programming modality. (Because a neural net works better than an algorithm for classifying images?).
- Elon is a warrior against entropy.

### ON AGI
- Neural nets already "think" in that they process information.
- I'm bullish on our ability to build AGIs. Automated - human-like systems. Text alone not enough. Multimodal stuff.
- "Touching" and interacting with the world.
- We're on the verge of models solving complex problems.
- Intelligence is an emergent phenomenon of large enough or complex enough models.
- A world model / and a model that knows it's place in the world.

> Aging is a disease. But let's not focus on it. Let's focus on AI and then AI will solve this problem. The correct thing to do is ignore these problems. Let AI solve them. I'm betting there's a high chance this will work.
> What if we can generate AI content on demand? And if there's infinite of it?
Friedman: It's humbling because we think we can create art.
> I'm cautiously optimistic.

## Karpathy on Dwarkesh Patel (17/10/2025)
- The decade of agents - not the year of agents. 'I was triggered by that'. I was reacting to the timeline.
- They're not smart enough or multimodal enough (the agents).
- The timeline is based on my intuition and my extrapolation.
- The problems are surmountable, but still hard.
- Seismic shifts in AI come with a surprising regularity.
- AlexNet reoriented everyone.
- ATARI was another shift in the direction of RL (environment), but might have been a misstep.
- Games wouldn't have led to AGI. In the real world you need an accountant. My project at Open AI was something that works with a keyboard and a mouse. It was too early. You're missing the representational power of ANNS + the rewards in RL are too sparse.
- People try to get everything working a few times before they end up with something useful.
- I'm hesitant to take inspiration from brains because we're not running that process (09:23).
- We're training by imitating humans (Brian Christian).
- If agents had less knowledge or less memory; they might be better.
- Patel: If pretraining and in-context learning are both doing variation of gradient descent - why does it feel like one is emergent and the other is not? (17:48).
- Karpathy: Maybe it's analogous to long-term vs working memory.
- Patel: What have we failed to recreate?
- Karpathy: Just a lot of it (20:14).

### LLM Cognitive Defects (30:33)
- Coding models are very limited to me. I still prefer to use autocomplete vs over vibe coding.
- They (models) keep misunderstanding the code because a repo might be doing things differently than the pretraining repos.
- Eg. models kept trying to get him to use a DDP where he had a custom implementation.
- They're overly defensive - too many try/excepts - bloated - deprecated APIs, etc.
- A data center of geniuses. Karpathy is one. See how he talks about coding agents?!.
- It's not amazing; it is slop.

### RL is Terrible (41:00)
- Humans don't learn through RL. Humans do better than RL.
- Partial process supervision is hard.


### AGI - 2% GDP Growth
- I'm tempted to reject the question entirely. This is computing. How do you chart progress in computing?
- OpenAI definition of AGI: economically valuable tasks at $\geq$ human performance.
- Geoff Hinton's prediction about radiologists. By the same token, we don't know if AI has made that big a dent yet.
- Some jobs will always be more amenable to automation than others.
- GDP grows gradually and slowly. Can't chalk down growth to AI; or even phones and computers.
- I think you're presupposing a discrete jump - that has no precedent (1:30:00).
- Self driving cars are nowhere near done (1:44:00). They're not economical either. Making them affordable will be a slog.
- We're not getting magical generalization with LLMs.
- I'm sounding pessimistic because of all that I see on the internet - attention, fundraising, etc. Otherwise I'm pretty bullish on technology.
- Very reputable people keep getting this wrong all the time. (1:56:49). I want
  to be properly calibrated
