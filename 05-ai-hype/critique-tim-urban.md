The seeds of AI hype and misinformation started sprouting in popular discourse
long before Instagram was awash with AI slop and every other post on LinkedIn
preyed on AI FOMO. As we've seen before, there was nothing essentially new about
it, but this time, the hype virus found a new ecosystem to infect: social media.

In 2015, Tim Urban, the writer of the popular blog "Wait But Why", wrote a
two-part series on AI, titled ["The AI Revolution: The Road to
Superintelligence"](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html).
Even if he wasn't the first AI booster[^3] and wouldn't be the last, few people
have been as influential as him in popularizing AI for a layperson. A fantastic
writer and explainer of complex topics, Urban did a great job of collating many
common AI fallacies in one place. What follows might read like a hit piece
against Urban, but it's really a case study. A critique of his posts serves as a
microcosm of much that is wrong with AI hype.

> It's an intimate mixture of rubbish and good ideas, and it's very hard to
> disentangle the two, because these are smart people; they're not stupid.

> —Douglas Hofstadter, [_American
Scientist_](https://web.archive.org/web/20140122012828/https://www.americanscientist.org/bookshelf/pub/douglas-r-hofstadter)

Nearly every bizarre claim about AI can be traced back to the so-called "Law of
Accelerating Returns", proposed by Ray Kurzweil. Kurzweil is a colorful figure
in the history of technology: a pioneer of optical character recognition, speech
synthesis and electronic keyboards, he has long since emerged as a prophet of
technology, complete with his share of acolytes and critics. He introduced the
Law of Accelerating Returns in his 1999 book _The Age of Spiritual Machines_.
Here's an excerpt from a 2001 essay by Kurzweil with the same title:

> ... the history of technology shows that technological change is
> exponential... So we won’t experience 100 years of progress in the 21st
> century — it will be more like 20,000 years of progress (at today’s rate). The
> “returns,” such as chip speed and cost-effectiveness, also increase
> exponentially. There’s even exponential growth in the rate of exponential
> growth. Within a few decades, machine intelligence will surpass human
> intelligence, leading to The Singularity — technological change so rapid and
> profound it represents a rupture in the fabric of human history.

If it sounds like unfalsifiable nonsense, that's because it is. It's not a "law"
of nature, but simply an absurd extrapolation of an observed pattern. Just like
the current AI hype can be traced back to this 'law', the law can in turn be
traced back to another non-law: Moore's Law.

Moore's Law is the observation that the number of transistors (electronic
switches) in an integrated circuit (like a microprocessor in our computers)
doubles every two years. This means that the number of arithmetic and logical
operations a chip can perform in a fixed time doubles every two years - without
growing in size.  Kurzweil's proposal is predicated on the idea (or at least the
bastardization of it) that things—economies, technologies, collective human
knowledge, etc—grow over time. And even if Moore's Law is the cornerstone of his
proposal, he expects everything to show "exponential growth"—something that
cannot be taken for granted. Even if Moore's Law were to continue to hold _ad
infinitum_ (it stopped being true years ago), the assumption that every
desirable component of technological change will continue to grow exponentially
is absurd. The economist Daniel Susskind writes,

> ... our apparent success created the impression that an expanding economy was
> the norm, with any slowdown to be regarded as an unfortunate but temporary
> exception. Today, that assuredness feels misplaced. Almost every country has
> slumped its way into the 21st century, though the timings differ... Most
> economies, battered by two decades of crises—including the dot-com bust, the
> 2007-8 financial crisis and the COVID-19 pandemic—are sluggish shadows of
> former selves. We increasingly realize we cannot take growth for granted.

But even if we grant Kurzweil his premise—that growth is preordained,
unstoppable—the deeper fallacy still remains: the conflation of all growth with
_exponential_ growth.

The adjective "exponential" has a precise mathematical meaning. When we say that
something grows exponentially, we mean that the rate at which it grows is
proportional to its current size. Compound interest is a classic example of
exponential growth. If your wealth grows at 10% today, a year later it would be
growing at 10% of your _then_ wealth—which itself will be higher because of the
_current_ growth of 10%. However, the figure of 10% is a fraction, which means
that while this growth is still exponential, it is relatively slow. In contrast,
if your wealth were to grow at 100% (i.e it doubles every year), the resulting
growth would be: massive, staggering, astronomical—pick an adjective. But that
doesn't mean that everything which grows massively, staggeringly, or
astronomically is exponential. Quite often, the correct adjective is simply
'more'. Every other chart and visual in Urban's post—like stick figures standing
on a time-series curve, seemingly oblivious to the walls next to them, and a GIF
of how fast an empty lake fills up if the water pouring in it doubles every
second—reinforces this idea of the exponential curve.

Even if we concede that something does grow exponentially in the true sense of
the word, we still need to ask whether such growth is _observed_ in specific
time-windows, or whether it is predestined. The exponential decay of a
radioactive element is inevitable. But the growth of an economy or that of human
population are not. They only look exponential if we hold our time-windows to
convenient periods. It is all too easy to confuse the observed effect of some
things with the primal cause of something else.

In a 2005 interview[^1], Gordon Moore himself, with humility and amusement, said
that the semiconductor industry "made it (Moore's Law) a self-fulfilling
prophecy", and that it was, on his part, "lucky extrapolation". But that did
nothing to temper Kurzweil's enthusiasm. Most of his predictions were predicated
almost exclusively on Moore's law—and only a few of them have come true. He has
repeatedly time-shifted and revised his predictions (which hasn't helped), but
never fully retracted them. At best, his predictions can be called inaccurate.
At worst, they are unfalsifiable.

And yet, in as late as 2015, Tim Urban stands squarely on Kurzweil's shoulders
to reiterate the same predictions. Urban's obsequiousness to Kurzweil's ideas
could not be more evident. He quotes Kurzweil more than any other source by far,
and appeals to his authority every step of the way. Nearly half of the footnotes
in the first part of his post are from Kurzweil's book: _The Singularity is
Near_[^2].

There's a sprawling introduction to Kurzweil's law, followed by justifications
of why artificial superintelligence is an inevitable outcome of it. It's
peppered with quotes like

> This isn't science fiction. It's what many scientists smarter and more
> knowledgeable than you or I firmly believe...

By the time Urban comes to talk about the Singularity, or an "intelligence
explosion", he begins by reminding the reader that

> ... every single thing I'm going to say is real—real science and real
> forecasts of the future from a large array of the most respected thinkers and
> scientists. Just keep remembering that.

What in the world are "real forecasts of the future"? Not once does Urban turn a
critical eye towards these ideas—which, even in 2015, would have been doable and
revealing. For instance, he calls Moore's Law a "historically reliable rule"
—something that ought to have been avoided. He takes cheaper and faster
computation for granted, which to him means that we will be able to "reverse
engineer the brain", and then reminds us that "if emulating the brain seems
hopeless, remember the power of exponential growth". This argument, that simply
scaling a computer makes it smarter, has some merit, as we will see in the
following section. But to suggest that scaling alone will make it as smart as
the human brain, and that in turn will
somehow bring forth the rapture, is reductive. It presupposes that the human
brain is nothing more than a giant, parallel computer which can only do
arithmetic and logic operations—and intelligence is the result of enough of
them.

After repeating the same few arguments over and over, with a lot more hubris
("we have a lot of advantages over evolution", and we can build a
superintelligence by trying "to do what evolution did, but this time for us"),
he climaxes with "An Intelligence Explosion—the ultimate example of the Law of
Accelerating Returns." If that is indeed true, then hopefully the explosion will
be as full of gas as the law itself.

In short,

* _if_ Moore's Law continues to hold long enough,
* _and if_ the Law of Accelerating Returns rides well enough on it,
* _and if_ highly scaled computers indeed mimic the human brain,
* _and if_ we actually reverse engineer the human brain,

_then_ we will have superintelligence on our hands.

All of this is supposed to be inevitable, and it will happen before we know it.

It's such an exceptional compounding of fallacies[^4] that I'm tempted to slip up
myself and call this nonsense "exponential". Nearly all of it is based on appeal
to authority and hasty generalization.

But credit where it is due, Tim Urban did write that a "growth spurt might be
brewing right now", and he was right. In 2017, researchers from Google
introduced the transformer architecture—the next big leap in machine learning.
It soon became the workhorse of the large-language-model (LLM) revolution of the
early 2020s.

Before transformers, language models were typically recurrent, meaning they
processed text one word (or token) at a time, each prediction depending on the
one before it. That worked, but it made it hard to handle long passages: the
farther back the context went, the harder it became for the model to remember
and compute efficiently. The transformer solved this bottleneck by replacing
recurrence with attention — a way for the model to look at all words in a
sequence at once and decide which ones matter most to predicting the next. This
made training faster, more parallel, and more scalable.

The implication was that researchers were now ready to test what they had
suspected for ages—the question of whether sheer _scale_ can produce
intelligence. Could we systematically scale training data, computing power and
sizes of models to see if perhaps intelligence emerges? In 2018, OpenAI
published their results from the first of such experiments. The resulting model
was called GPT—Generative Pretrained Transformer. It was this model that would
undergo iteration after iteration of scaling over the next few years. OpenAI's
series of GPT models, Anthropic's Claude, Google's Gemini models are all,
fundamentally, massively scaled transformers.

And that's how we arrived here. Dwarkesh Patel, the host of a popular podcast on
AI calls the period from 2019 to 2025 _the Scaling Era_. Now that we could, it
was time to finally put exponentialism to the test.

---

[^1]: [Excerpts from A Conversation with Gordon
Moore](https://web.archive.org/web/20080218225540/http://download.intel.com/museum/Moores_Law/Video-Transcripts/Excepts_A_Conversation_with_Gordon_Moore.pdf)
[^2]: Kurzweil later wrote another book, called _The Singularity is Nearer_.
Since the Singularity is perpetually just around the corner, it makes sense that
Kurzweil took a serious interest in immortality. At the time Urban wrote his
article, Kurzweil was taking [100 pills every
day](https://www.businessinsider.com/ray-kurzweils-immortality-diet-2015-4).
[^3]: That dubious honor goes, sadly, to Frank Rosenblatt himself.
[^4]: Princeton researchers Arvin Narayanan and Sayash Kapoor came to a similar
    conclusion in their book _AI Snake Oil_. In an entire chapter dedicated to
    such fallacies, they write, "...we've seen in the history of AI research, that once
    one aspect gets automated, other aspects that weren't recognized earlier
    tend to reveal themselves as bottlenecks." Every link in the chain of "ifs"
    above has its own bottlenecks.
