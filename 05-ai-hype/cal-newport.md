## The Hype Machine - AI CEOs Keep Talking

https://www.youtube.com/watch?v=WwI8Q80-73s

*   Euphoria vs dread - only two options?
*   Zuck, Sama, Amodei's hyperbole
*   The layoffs are NOT related to AI
*   GPT-5 was a disappointment.
*   They're not getting better (see benchmarks)
*   Edward Zitron

* The scaling law - a very rational reason to believe in growth
* But now Jensen Huang is redefining the scaling law
* (this is a paper far from OpenAI : Amodei et al, 2020)
* Apparently this paper demonstrated that larger models get better with size,
  and DON'T overfit as we'd expect.
* And apparently (check this); this paper drove GPT-3; it fell on the curve
  exactly where it was supposed to be
* As soon as they validated this "law", Sama wrote his (in)famous Moore's Law
  for Everything ← extremely spicy
* ChatGPT was a shiny layer on all of this, only.
* Microsoft had to invent custom cooling tech for the data centers
* GPT-4 brought with it (reasoning, coding, math); because of the scaling law. →
  and we thought it'll extrapolate.
* According to "The Information": As of spring 2024 (Orion) (GPT-4 came out in
  spring 2023) SAMA was telling employees that GPT-5 was going to be
  "significantly better", but by the fall of 2024; it was disappointing.
* check against Karen Hao**


* Everyone else (Meta, XAI) all faltered
* Techcrunch article about Sutskever - on how scaling's done and we're back to
  the discovery phase (Check this)
* Meta & XAI both faltered on the scaling laws (Meta delayed release of BMF)
* No leap in Grok3
* The Techcrunch article articulates the slowing down of the scaling law.
* So what did big tech do to put a positive spin on this all? They knew the
  music had stopped.
* Jensen Huang on the "NEW" scaling laws (35:00 mins) talking about RLHF —
  clearly faltering and grasping at straws. Very slick way of moving back
* Jensen Huang "Post-training" - Newport catches him on his techno babble.
* Need to figure out what this post-training is. Newport has a Newyorker article
  about it.
* Kaplan's paper on the scaling law?
* Wait, they're just doing finetuning for cool/marketable tasks?!

## What happens now? (NewYorker article again)
*   AI tools make steady / gradual progress
*   35% of us stock market tied up in big tech. (Zitron)
*   They spent half a trillion dollars; revenue from AI only 30 billion.
*   Newport calls out Altman too (talking through his teeth) (Check: 56:34 /
    turning on a dime).
*   Newport admits to being a low-key Luddite. - He was vocal against blockchain
    (Not crypto) see 1:00:00


*   **Listener question on losing job to AI (a software engg Newport calls B&
    directly.**
*   **Aide: how to arrange a bookshelf**
*   **Look towards what Ryan Holiday does.**

## Newport's NewYorker article:

* Post training improvements (examples):
    - use RL for some tasks
    - Longer thinking & refinement
* Satya Nadella: "Emergence of a new scaling law" - didn't Jensen Huang say this
  too?
* o3-mini, o3 pro, o3 high are all simply better post trained models.
* Open AI's announcement of GPT-5 contained a large ward of technobabble.
* A paper by Apple called "The Illusion of Thinking".
* "What AI companies call reasoning is just a brittle mirage." — Arizona State
  univ.
* Nate Silver made fun of Ed Zitron. ← Find this!!

> Certain fields like academia & programming will transform dramatically. A
> minority of professions like, voice acting and social media copy writing may
> essentially disappear. But AI may not massively disrupt the job market.
